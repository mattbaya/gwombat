#!/bin/bash

# GWOMBAT - Google Workspace Optimization, Management, Backups And Taskrunner
#
# A comprehensive suspended account lifecycle management system with database tracking,
# verification, and automated workflows. For complete documentation, installation
# instructions, and usage examples, please refer to README.md.
#
# This system manages Google Workspace accounts through their complete lifecycle
# from suspension to deletion with persistent state tracking and verification capabilities.
#
# Key Features:
# - Database-driven account lifecycle tracking with SQLite
# - Account scanning and automated stage discovery
# - List-based batch operations with verification
# - Secure deployment with git-based workflows
# - Environment-configurable paths and settings
#
# For complete documentation including installation, usage, configuration,
# and deployment instructions, see README.md and DEPLOYMENT.md
#

# GWOMBAT Configuration
# Google Workspace Optimization, Management, Backups And Taskrunner - consolidates all suspended account operations with menu system and preview functionality

# Variables now loaded via load_configuration() function

# Organizational Unit paths (configured via local-config/.env)
OU_TEMPHOLD="${TEMPORARY_HOLD_OU:-/Suspended Accounts/Suspended - Temporary Hold}"
OU_PENDING_DELETION="${PENDING_DELETION_OU:-/Suspended Accounts/Suspended - Pending Deletion}"  
OU_SUSPENDED="${SUSPENDED_OU:-/Suspended Accounts}"
OU_ACTIVE="${DOMAIN:-yourdomain.edu}"

# Google Drive Label IDs (configured via local-config/.env)
LABEL_ID="${DRIVE_LABEL_ID:-default-label-id}"

# Advanced Logging and Reporting Configuration
LOG_DIR="${LOG_PATH:-./local-config/logs}"
BACKUP_DIR="${BACKUPS_PATH:-./local-config/backups}"
REPORT_DIR="${REPORTS_PATH:-./local-config/reports}"
TMP_DIR="${TMP_PATH:-./local-config/tmp}"
mkdir -p "$LOG_DIR" "$BACKUP_DIR" "$REPORT_DIR" "$TMP_DIR"

# Log files
SESSION_ID=$(date +%Y%m%d_%H%M%S)
LOG_FILE="${LOG_DIR}/session-${SESSION_ID}.log"
ERROR_LOG="${LOG_DIR}/errors-$(date +%Y%m%d).log"
OPERATION_LOG="${LOG_DIR}/operations-$(date +%Y%m%d).log"
PERFORMANCE_LOG="${LOG_DIR}/performance-$(date +%Y%m%d).log"
AUDIT_LOG="${LOG_DIR}/audit-$(date +%Y%m%d).log"

# Report files
DAILY_SUMMARY="${REPORT_DIR}/daily-summary-$(date +%Y%m%d).txt"
OPERATION_SUMMARY="${REPORT_DIR}/operation-summary-${SESSION_ID}.txt"
USER_ACTIVITY_REPORT="${REPORT_DIR}/user-activity-$(date +%Y%m%d).txt"

# Configuration Management
CONFIG_FILE="./config/gwombat-config.json"
CONFIG_DIR="./config"
mkdir -p "$CONFIG_DIR"

# Database paths
DB_FILE="local-config/gwombat.db"
DATABASE_PATH="local-config/gwombat.db"
MENU_DB="shared-config/menu.db"

# Load configuration from file or set defaults
load_configuration() {
    # Set default values
    DEFAULT_GAM_PATH="/usr/local/bin/gam"
    DEFAULT_SCRIPT_PATH="${GWOMBAT_PATH:-$(pwd)}"
    DEFAULT_SHARED_UTILITIES_PATH="shared-utilities"
    DEFAULT_PROGRESS_ENABLED="true"
    DEFAULT_CONFIRMATION_LEVEL="normal"
    DEFAULT_LOG_RETENTION_DAYS="30"
    DEFAULT_BACKUP_RETENTION_DAYS="90"
    DEFAULT_OPERATION_TIMEOUT="300"
    
    # Load .env file if it exists
    if [[ -f "local-config/.env" ]]; then
        source local-config/.env
        echo "Loaded environment configuration from local-config/.env"
    elif [[ -f ".env" ]]; then
        source .env
        echo "Loaded environment configuration from .env (legacy location)"
    fi
    
    # Override with environment variables if set
    GAM="${GAM_PATH:-$DEFAULT_GAM_PATH}"
    SCRIPTPATH="${SCRIPT_PATH:-$DEFAULT_SCRIPT_PATH}"
    SHARED_UTILITIES_PATH="${SHARED_UTILITIES_PATH:-$DEFAULT_SHARED_UTILITIES_PATH}"
    PROGRESS_ENABLED="${PROGRESS_SETTING:-$DEFAULT_PROGRESS_ENABLED}"
    CONFIRMATION_LEVEL="${CONFIRMATION_SETTING:-$DEFAULT_CONFIRMATION_LEVEL}"
    LOG_RETENTION_DAYS="${LOG_RETENTION:-$DEFAULT_LOG_RETENTION_DAYS}"
    BACKUP_RETENTION_DAYS="${BACKUP_RETENTION:-$DEFAULT_BACKUP_RETENTION_DAYS}"
    OPERATION_TIMEOUT="${OP_TIMEOUT:-$DEFAULT_OPERATION_TIMEOUT}"
    
    # Load from config file if it exists
    if [[ -f "$CONFIG_FILE" ]]; then
        # Parse JSON config file
        local config_gam=$(grep '"gam_path"' "$CONFIG_FILE" 2>/dev/null | cut -d'"' -f4)
        local config_script_path=$(grep '"script_path"' "$CONFIG_FILE" 2>/dev/null | cut -d'"' -f4)
        local config_listshared=$(grep '"listshared_path"' "$CONFIG_FILE" 2>/dev/null | cut -d'"' -f4)
        local config_progress=$(grep '"progress_enabled"' "$CONFIG_FILE" 2>/dev/null | cut -d'"' -f4)
        local config_confirmation=$(grep '"confirmation_level"' "$CONFIG_FILE" 2>/dev/null | cut -d'"' -f4)
        local config_log_retention=$(grep '"log_retention_days"' "$CONFIG_FILE" 2>/dev/null | cut -d'"' -f4)
        local config_backup_retention=$(grep '"backup_retention_days"' "$CONFIG_FILE" 2>/dev/null | cut -d'"' -f4)
        local config_timeout=$(grep '"operation_timeout"' "$CONFIG_FILE" 2>/dev/null | cut -d'"' -f4)
        
        # Use config values if available
        [[ -n "$config_gam" ]] && GAM="$config_gam"
        [[ -n "$config_script_path" ]] && SCRIPTPATH="$config_script_path"
        [[ -n "$config_shared_utilities" ]] && SHARED_UTILITIES_PATH="$config_shared_utilities"
        [[ -n "$config_progress" ]] && PROGRESS_ENABLED="$config_progress"
        [[ -n "$config_confirmation" ]] && CONFIRMATION_LEVEL="$config_confirmation"
        [[ -n "$config_log_retention" ]] && LOG_RETENTION_DAYS="$config_log_retention"
        [[ -n "$config_backup_retention" ]] && BACKUP_RETENTION_DAYS="$config_backup_retention"
        [[ -n "$config_timeout" ]] && OPERATION_TIMEOUT="$config_timeout"
    fi
}

# Create default configuration file
create_default_config() {
    cat > "$CONFIG_FILE" << EOF
{
  "version": "2.0",
  "description": "GWOMBAT Configuration",
  "created": "$(date -Iseconds)",
  "settings": {
    "gam_path": "/usr/local/bin/gam",
    "script_path": "${SCRIPT_TEMP_PATH:-./local-config/tmp}/suspended",
    "listshared_path": "${SCRIPT_TEMP_PATH:-./local-config/tmp}/listshared",
    "progress_enabled": "true",
    "confirmation_level": "normal",
    "log_retention_days": "30",
    "backup_retention_days": "90",
    "operation_timeout": "300"
  },
  "organizational_units": {
    "temporary_hold": "/Suspended Accounts/Suspended - Temporary Hold",
    "pending_deletion": "/Suspended Accounts/Suspended - Pending Deletion",
    "suspended": "/Suspended Accounts",
    "active": "/${DOMAIN:-yourdomain.edu}"
  },
  "google_drive": {
    "label_id": "xIaFm0zxPw8zVL2nVZEI9L7u9eGOz15AZbJRNNEbbFcb",
    "field_id": "62BB395EC6",
    "selection_id": "68E9987D43"
  },
  "features": {
    "dry_run_default": false,
    "backup_enabled": true,
    "performance_logging": true,
    "audit_logging": true,
    "auto_cleanup": true
  }
}
EOF
    echo -e "${GREEN}Default configuration created: $CONFIG_FILE${NC}"
}

# Load configuration
load_configuration

# Initialize session logging
echo "=== SESSION START: $(date) ===" >> "$LOG_FILE"
echo "Session ID: $SESSION_ID" >> "$LOG_FILE"
echo "User: $(whoami)" >> "$LOG_FILE"
echo "Working Directory: $(pwd)" >> "$LOG_FILE"
echo "Script Version: GWOMBAT v3.0" >> "$LOG_FILE"
echo "GAM Path: $GAM" >> "$LOG_FILE"
echo "Script Path: $SCRIPTPATH" >> "$LOG_FILE"
echo "Progress Enabled: $PROGRESS_ENABLED" >> "$LOG_FILE"
echo "Confirmation Level: $CONFIRMATION_LEVEL" >> "$LOG_FILE"

FIELD_ID="62BB395EC6"
SELECTION_ID="68E9987D43"

# Global settings
DRY_RUN=false
DISCOVERY_MODE=false
PROGRESS_ENABLED=true

# Security: GAM Input Sanitization Function
# Removes or escapes dangerous shell characters from user inputs to prevent command injection
sanitize_gam_input() {
    local input="$1"
    local sanitized=""
    
    if [[ -z "$input" ]]; then
        echo ""
        return 0
    fi
    
    # Remove dangerous shell metacharacters that could lead to command injection
    # Keep only alphanumeric, dots, hyphens, underscores, @, spaces, and basic punctuation for emails/usernames
    sanitized=$(echo "$input" | sed 's/[;&|`$(){}[\]<>"'\''\\]//g')
    
    # Log sanitization if changes were made
    if [[ "$input" != "$sanitized" ]]; then
        echo "WARNING: Input sanitized - removed potentially dangerous characters" >> "$AUDIT_LOG"
        echo "Original: $input" >> "$AUDIT_LOG"
        echo "Sanitized: $sanitized" >> "$AUDIT_LOG"
        echo "Timestamp: $(date)" >> "$AUDIT_LOG"
        echo "---" >> "$AUDIT_LOG"
        
        # Also log to console if not in quiet mode
        if [[ "${QUIET_MODE:-false}" != "true" ]]; then
            echo -e "${YELLOW}WARNING: Input sanitized to remove potentially dangerous characters${NC}" >&2
        fi
    fi
    
    echo "$sanitized"
}

# Initialize the main database with all required tables
initialize_database() {
    local db_file="${DB_FILE:-local-config/gwombat.db}"
    local menu_db="${MENU_DB:-shared-config/menu.db}"
    
    echo -e "${CYAN}Initializing GWOMBAT databases...${NC}"
    
    # Create local-config directory if it doesn't exist
    mkdir -p "$(dirname "$db_file")"
    
    # Initialize main database with schema
    if [[ -f "shared-config/main_schema.sql" ]]; then
        echo "  Loading main database schema..."
        if sqlite3 "$db_file" < "shared-config/main_schema.sql" 2>/dev/null; then
            echo -e "${GREEN}  âœ“ Main database initialized${NC}"
            log_info "Main database initialized at $db_file"
        else
            echo -e "${YELLOW}  âš  Main database initialization had warnings${NC}"
            log_error "Main database initialization warnings"
        fi
    else
        echo -e "${YELLOW}  âš  Main schema file not found, creating basic tables${NC}"
        # Create minimal accounts table if schema file is missing
        sqlite3 "$db_file" "CREATE TABLE IF NOT EXISTS accounts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            email TEXT UNIQUE NOT NULL,
            full_name TEXT,
            ou_path TEXT,
            current_stage TEXT,
            is_suspended INTEGER DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );" 2>/dev/null
        
        sqlite3 "$db_file" "CREATE TABLE IF NOT EXISTS account_operations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            account_email TEXT NOT NULL,
            operation_type TEXT NOT NULL,
            operation_status TEXT NOT NULL,
            started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            completed_at TIMESTAMP
        );" 2>/dev/null
    fi
    
    # Initialize menu database
    if [[ -f "shared-config/menu_schema.sql" ]]; then
        echo "  Loading menu database schema..."
        if sqlite3 "$menu_db" < "shared-config/menu_schema.sql" 2>/dev/null; then
            echo -e "${GREEN}  âœ“ Menu database initialized${NC}"
            
            # Load menu data if available
            if [[ -x "shared-utilities/menu_data_loader.sh" ]]; then
                echo "  Loading menu data..."
                if ./shared-utilities/menu_data_loader.sh >/dev/null 2>&1; then
                    echo -e "${GREEN}  âœ“ Menu data loaded${NC}"
                else
                    echo -e "${YELLOW}  âš  Menu data loading had issues${NC}"
                fi
            fi
        else
            echo -e "${YELLOW}  âš  Menu database initialization had warnings${NC}"
        fi
    fi
    
    echo -e "${GREEN}âœ“ Database initialization complete${NC}"
    return 0
}

# Enhanced dependency check function with logging and optional tools
store_domain_in_database() {
    local domain="$1"
    local db_file="${SCRIPTPATH}/local-config/gwombat.db"
    
    if [[ -f "$db_file" && -n "$domain" ]]; then
        sqlite3 "$db_file" "INSERT OR REPLACE INTO config (key, value) VALUES ('configured_domain', '$domain');" 2>/dev/null
        sqlite3 "$db_file" "INSERT OR REPLACE INTO config (key, value) VALUES ('domain_set_at', datetime('now'));" 2>/dev/null
    fi
}

reset_database_for_domain_change() {
    local new_domain="$1"
    local old_domain="$2"
    local db_file="${SCRIPTPATH}/local-config/gwombat.db"
    
    echo -e "${YELLOW}âš ï¸  Domain change detected!${NC}"
    echo -e "${YELLOW}   Old domain: $old_domain${NC}"
    echo -e "${YELLOW}   New domain: $new_domain${NC}"
    echo ""
    echo -e "${YELLOW}Database contains data for the old domain and must be reset.${NC}"
    echo -e "${YELLOW}This will clear all account data, lists, and history.${NC}"
    echo ""
    
    if [[ -f "$db_file" ]]; then
        local account_count=$(sqlite3 "$db_file" "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "0")
        local list_count=$(sqlite3 "$db_file" "SELECT COUNT(*) FROM account_lists;" 2>/dev/null || echo "0")
        
        if [[ "$account_count" -gt 0 || "$list_count" -gt 0 ]]; then
            echo -e "${CYAN}Current database contains:${NC}"
            echo "  â€¢ $account_count accounts"
            echo "  â€¢ $list_count account lists"
            echo ""
            
            # Create backup before reset
            local backup_file="${db_file}.backup-${old_domain}-$(date +%Y%m%d_%H%M%S)"
            echo -e "${CYAN}Creating backup: ${backup_file}${NC}"
            cp "$db_file" "$backup_file"
            echo -e "${GREEN}âœ“ Backup created${NC}"
            echo ""
        fi
    fi
    
    echo -e "${RED}Reset database for domain change? (y/N)${NC}"
    read -p "> " confirm
    if [[ "$confirm" =~ ^[Yy] ]]; then
        if [[ -f "$db_file" ]]; then
            rm "$db_file"
            echo -e "${GREEN}âœ“ Database reset for new domain: $new_domain${NC}"
        fi
        
        # Initialize fresh database with new domain
        if [[ -f "${SCRIPTPATH}/shared-config/database_schema.sql" ]]; then
            echo -e "${CYAN}Initializing fresh database...${NC}"
            sqlite3 "$db_file" < "${SCRIPTPATH}/shared-config/database_schema.sql"
            sqlite3 "$db_file" "INSERT OR REPLACE INTO config (key, value) VALUES ('configured_domain', '$new_domain');"
            sqlite3 "$db_file" "INSERT OR REPLACE INTO config (key, value) VALUES ('domain_changed_at', datetime('now'));"
            echo -e "${GREEN}âœ“ Fresh database initialized for: $new_domain${NC}"
        fi
        return 0
    else
        echo -e "${YELLOW}Database reset cancelled. GWOMBAT cannot proceed with mixed domain data.${NC}"
        return 1
    fi
}

show_gam_info() {
    local gam_path="${GAM_PATH:-gam}"
    
    echo -e "${BLUE}=== GAM Configuration Information ===${NC}"
    echo ""
    echo -e "${CYAN}GAM Path:${NC} $gam_path"
    echo -e "${CYAN}GAM Config Path:${NC} ${GAM_CONFIG_PATH:-~/.gam}"
    echo ""
    
    if ! command -v "$gam_path" >/dev/null 2>&1; then
        echo -e "${RED}âŒ GAM not found at: $gam_path${NC}"
        return 1
    fi
    
    echo -e "${CYAN}GAM Domain Information:${NC}"
    local gam_domain_info
    if command -v timeout >/dev/null 2>&1; then
        gam_domain_info=$(timeout 15 "$gam_path" info domain 2>/dev/null)
    else
        gam_domain_info=$("$gam_path" info domain 2>/dev/null)
    fi
    
    if [[ -n "$gam_domain_info" ]]; then
        echo "$gam_domain_info"
    else
        echo -e "${YELLOW}âš ï¸  GAM not configured or cannot access domain${NC}"
        echo "Run: $gam_path oauth create"
    fi
    
    echo ""
    echo -e "${CYAN}OAuth Token Location:${NC}"
    if [[ -n "${GAM_CONFIG_PATH}" ]]; then
        local token_path="${GAM_CONFIG_PATH}/oauth2.txt"
        if [[ -f "$token_path" ]]; then
            echo -e "${GREEN}âœ“ Found: $token_path${NC}"
            echo -e "${GRAY}Created: $(stat -f "%Sm" "$token_path" 2>/dev/null || stat -c "%y" "$token_path" 2>/dev/null)${NC}"
        else
            echo -e "${YELLOW}âš ï¸  Not found: $token_path${NC}"
        fi
    else
        echo -e "${YELLOW}No GAM_CONFIG_PATH set${NC}"
    fi
    
    echo ""
    echo -e "${CYAN}Current GWOMBAT Configuration:${NC}"
    echo -e "Domain: ${DOMAIN:-not set}"
    echo -e "Admin User: ${ADMIN_USER:-not set}"
    echo ""
}

verify_gam_domain() {
    local gam_path="${GAM:-gam}"
    local configured_domain="${DOMAIN}"
    local db_file="${SCRIPTPATH}/local-config/gwombat.db"
    
    if [[ -z "$configured_domain" ]]; then
        echo -e "${RED}âŒ CRITICAL: No DOMAIN configured in .env file${NC}"
        echo -e "${YELLOW}Please set DOMAIN in your .env file${NC}"
        return 1
    fi
    
    if ! command -v "$gam_path" >/dev/null 2>&1; then
        echo -e "${RED}âŒ CRITICAL: GAM not found at: $gam_path${NC}"
        return 1
    fi
    
    echo -e "${CYAN}ðŸ”’ Verifying GAM domain matches configuration...${NC}"
    
    # Get domain from GAM
    local gam_domain_info
    if command -v timeout >/dev/null 2>&1; then
        gam_domain_info=$(timeout 15 "$gam_path" info domain 2>/dev/null)
    else
        gam_domain_info=$("$gam_path" info domain 2>/dev/null)
    fi
    
    if [[ -z "$gam_domain_info" ]]; then
        echo -e "${RED}âŒ CRITICAL: GAM is not configured or cannot access domain information${NC}"
        echo -e "${YELLOW}Please run: $gam_path oauth create${NC}"
        echo -e "${YELLOW}Or check GAM configuration with: $gam_path info domain${NC}"
        return 1
    fi
    
    # Extract primary domain from GAM output (handle various formats)
    local gam_primary_domain=$(echo "$gam_domain_info" | grep -i "Primary Domain" | awk '{print $3}' | cut -d':' -f1 | sed 's/Verified.*$//' | tr -d '[:space:]')
    
    if [[ -z "$gam_primary_domain" ]]; then
        echo -e "${RED}âŒ CRITICAL: Cannot determine primary domain from GAM${NC}"
        echo -e "${GRAY}GAM Output:${NC}"
        echo "$gam_domain_info" | head -10
        echo ""
        echo -e "${CYAN}For detailed GAM info, run: show_gam_info${NC}"
        return 1
    fi
    
    # Compare domains (case-insensitive)
    local gam_domain_lower=$(echo "$gam_primary_domain" | tr '[:upper:]' '[:lower:]')
    local config_domain_lower=$(echo "$configured_domain" | tr '[:upper:]' '[:lower:]')
    if [[ "$gam_domain_lower" == "$config_domain_lower" ]]; then
        echo -e "${GREEN}âœ… VERIFIED: GAM domain matches configuration${NC}"
        echo -e "${GREEN}   Configured: $configured_domain${NC}"
        echo -e "${GREEN}   GAM Domain: $gam_primary_domain${NC}"
        
        # Check if database has different domain data
        if [[ -f "$db_file" ]]; then
            local db_domain=$(sqlite3 "$db_file" "SELECT value FROM config WHERE key='configured_domain';" 2>/dev/null)
            if [[ -n "$db_domain" && "$db_domain" != "$configured_domain" ]]; then
                echo ""
                if ! reset_database_for_domain_change "$configured_domain" "$db_domain"; then
                    return 1
                fi
            else
                # Store/update domain in database if not already set
                store_domain_in_database "$configured_domain"
            fi
        fi
        
        return 0
    else
        echo -e "${RED}âŒ CRITICAL SECURITY ISSUE: Domain mismatch!${NC}"
        echo -e "${RED}   .env DOMAIN: $configured_domain${NC}"
        echo -e "${RED}   GAM Domain:  $gam_primary_domain${NC}"
        echo ""
        echo -e "${YELLOW}âš ï¸  This is a security risk - GAM commands will run against: $gam_primary_domain${NC}"
        echo -e "${YELLOW}âš ï¸  But GWOMBAT is configured for: $configured_domain${NC}"
        echo ""
        echo -e "${CYAN}To fix this issue:${NC}"
        echo "1. Update DOMAIN in .env to: $gam_primary_domain"
        echo "2. OR reconfigure GAM for domain: $configured_domain"
        echo "3. OR use a different GAM config path for $configured_domain"
        echo ""
        echo -e "${CYAN}For detailed GAM info, run: show_gam_info${NC}"
        return 1
    fi
}

check_dependencies() {
    local missing_deps=()
    local warnings=()
    local recommendations=()
    local optional_tools=()
    
    log_info "Starting GWOMBAT dependency check" "console"
    echo -e "${BLUE}=== GWOMBAT Dependency Check ===${NC}"
    echo ""
    
    # Critical domain verification first
    if ! verify_gam_domain; then
        echo -e "${RED}âŒ CRITICAL: Domain verification failed - stopping dependency check${NC}"
        echo -e "${YELLOW}Fix the domain configuration before proceeding${NC}"
        return 1
    fi
    echo ""
    
    # Essential dependencies
    echo -e "${CYAN}Essential Dependencies:${NC}"
    if ! command -v bash >/dev/null 2>&1; then
        missing_deps+=("bash")
        log_error "Essential dependency missing: bash"
    else
        local bash_version=$(bash --version | head -n1 | grep -oE '[0-9]+\.[0-9]+' | head -n1)
        echo -e "${GREEN}âœ“ Bash: $bash_version${NC}"
        log_info "Bash version: $bash_version"
    fi
    
    if ! command -v sqlite3 >/dev/null 2>&1; then
        missing_deps+=("sqlite3")
        log_error "Essential dependency missing: sqlite3"
    else
        local sqlite_version=$(sqlite3 --version | cut -d' ' -f1)
        echo -e "${GREEN}âœ“ SQLite: $sqlite_version${NC}"
        log_info "SQLite version: $sqlite_version"
    fi
    
    if ! command -v git >/dev/null 2>&1; then
        missing_deps+=("git")
        log_error "Essential dependency missing: git"
    else
        local git_version=$(git --version | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' | head -n1)
        echo -e "${GREEN}âœ“ Git: $git_version${NC}"
        log_info "Git version: $git_version"
    fi
    
    # Check Python
    if ! command -v python3 >/dev/null 2>&1; then
        missing_deps+=("python3")
        log_error "Essential dependency missing: python3"
    else
        local python_version=$(python3 --version 2>&1 | grep -oE '[0-9]+\.[0-9]+\.[0-9]+')
        echo -e "${GREEN}âœ“ Python: $python_version${NC}"
        log_info "Python version: $python_version"
        
        # Check Python packages for SCuBA compliance
        if python3 -c "import google.api_core" 2>/dev/null; then
            echo -e "${GREEN}  âœ“ Google API packages available${NC}"
            log_info "Python Google API packages detected"
        else
            recommendations+=("Install Python Google API packages for SCuBA compliance: pip3 install -r python-modules/requirements.txt")
            log_info "Python Google API packages missing - SCuBA compliance will need setup"
        fi
    fi
    
    echo ""
    echo -e "${CYAN}Google Workspace Integration:${NC}"
    
    # Check GAM
    local gam_path="${GAM:-${GAM_PATH:-/usr/local/bin/gam}}"
    if [[ -x "$gam_path" ]]; then
        local gam_version=$($gam_path version 2>/dev/null | head -n1 || echo "unknown")
        
        # Check if this is GAM7
        local is_gam7=false
        if echo "$gam_version" | grep -q "GAM 7\|GAM7\|^7\." 2>/dev/null; then
            is_gam7=true
            echo -e "${GREEN}âœ“ GAM7: $gam_version${NC}"
            echo -e "${GREEN}  Path: $gam_path${NC}"
            log_info "GAM7 found: $gam_version at $gam_path"
        else
            echo -e "${YELLOW}âš  GAM found but not GAM7: $gam_version${NC}"
            echo -e "${RED}  GWOMBAT requires GAM7 for compatibility${NC}"
            echo -e "${YELLOW}  Please upgrade to GAM7: https://github.com/GAM-team/GAM/wiki${NC}"
            missing_deps+=("GAM7 (Current version: $gam_version)")
            log_error "GAM version is not GAM7: $gam_version"
            recommendations+=("Upgrade to GAM7 for full compatibility: https://github.com/GAM-team/GAM/wiki")
        fi
        
        # Only check configuration if GAM7 is installed
        if [[ "$is_gam7" == "true" ]]; then
            # Check if GAM is configured (with timeout handling)
            echo -e "${YELLOW}  Checking GAM configuration...${NC}"
            if command -v timeout >/dev/null 2>&1; then
                # Use timeout if available (Linux)
                if timeout 10 $gam_path info domain 2>/dev/null | grep -q "Customer ID"; then
                    echo -e "${GREEN}  âœ“ GAM7 is configured${NC}"
                    log_info "GAM7 is configured and working"
                else
                    recommendations+=("GAM7 needs configuration: Run 'gam info domain' to verify setup")
                    echo -e "${YELLOW}  â—‹ GAM7 found but not configured or timed out${NC}"
                    log_info "GAM7 found but not configured or configuration check timed out"
                fi
            else
                # Fallback for macOS - skip configuration check to avoid hang
                echo -e "${YELLOW}  â—‹ GAM7 found - configuration check skipped (run 'gam info domain' to verify)${NC}"
                recommendations+=("Verify GAM7 configuration: Run 'gam info domain' manually")
                log_info "GAM7 found but configuration check skipped (timeout not available)"
            fi
        fi
    else
        missing_deps+=("GAM7 (Google Apps Manager 7)")
        echo -e "${RED}âœ— GAM7 not found at: $gam_path${NC}"
        echo -e "${YELLOW}  Install GAM7: https://github.com/GAM-team/GAM/wiki${NC}"
        log_error "GAM7 not found at: $gam_path"
        recommendations+=("Install GAM7: https://github.com/GAM-team/GAM/wiki")
    fi
    
    echo ""
    echo -e "${CYAN}Backup & Cloud Tools:${NC}"
    
    # Check GYB (Got Your Back)
    if command -v gyb >/dev/null 2>&1; then
        local gyb_version=$(gyb --version 2>/dev/null | head -n1 || echo "unknown")
        echo -e "${GREEN}âœ“ GYB (Got Your Back): $gyb_version${NC}"
        optional_tools+=("GYB for Gmail backups")
        log_info "GYB found: $gyb_version"
    else
        recommendations+=("Install GYB for Gmail backups: https://github.com/GAM-team/got-your-back")
        echo -e "${YELLOW}â—‹ GYB not found - install for Gmail backup capabilities${NC}"
        log_info "GYB not found - Gmail backup capabilities limited"
    fi
    
    # Check rclone
    if command -v rclone >/dev/null 2>&1; then
        local rclone_version=$(rclone version 2>/dev/null | head -n1 | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
        echo -e "${GREEN}âœ“ rclone: $rclone_version${NC}"
        optional_tools+=("rclone for cloud storage")
        log_info "rclone found: $rclone_version"
        
        # Check if rclone has any remotes configured (with timeout handling)
        if command -v timeout >/dev/null 2>&1; then
            if timeout 5 rclone listremotes 2>/dev/null | grep -q ":"; then
                echo -e "${GREEN}  âœ“ rclone has configured remotes${NC}"
                log_info "rclone has configured remotes"
            else
                recommendations+=("Configure rclone remotes for cloud backup: rclone config")
                log_info "rclone found but no remotes configured or check timed out"
            fi
        else
            echo -e "${YELLOW}  â—‹ rclone found - remote check skipped${NC}"
            recommendations+=("Configure rclone remotes for cloud backup: rclone config")
            log_info "rclone found but remote check skipped (timeout not available)"
        fi
    else
        recommendations+=("Install rclone for cloud storage integration: https://rclone.org/install/")
        echo -e "${YELLOW}â—‹ rclone not found - install for cloud backup capabilities${NC}"
        log_info "rclone not found - cloud backup capabilities limited"
    fi
    
    # Check restic
    if command -v restic >/dev/null 2>&1; then
        local restic_version=$(restic version 2>/dev/null | head -n1 | grep -oE '[0-9]+\.[0-9]+\.[0-9]+')
        echo -e "${GREEN}âœ“ restic: $restic_version${NC}"
        optional_tools+=("restic for encrypted backups")
        log_info "restic found: $restic_version"
    else
        recommendations+=("Install restic for encrypted incremental backups: https://restic.net/")
        echo -e "${YELLOW}â—‹ restic not found - install for encrypted backup capabilities${NC}"
        log_info "restic not found - encrypted backup capabilities limited"
    fi
    
    echo ""
    echo -e "${CYAN}System Tools:${NC}"
    
    # Optional dependencies
    if command -v expect >/dev/null 2>&1; then
        echo -e "${GREEN}âœ“ expect (deployment automation)${NC}"
        log_info "expect found - deployment automation available"
    else
        warnings+=("expect - needed for automated deployment")
        log_info "expect not found - manual deployment required"
    fi
    
    if command -v curl >/dev/null 2>&1; then
        echo -e "${GREEN}âœ“ curl${NC}"
        log_info "curl found"
    else
        warnings+=("curl - useful for web requests")
        log_info "curl not found"
    fi
    
    if command -v jq >/dev/null 2>&1; then
        echo -e "${GREEN}âœ“ jq (JSON processing)${NC}"
        log_info "jq found - JSON processing available"
    else
        recommendations+=("Install jq for enhanced JSON processing: apt install jq / brew install jq")
        echo -e "${YELLOW}â—‹ jq not found - install for enhanced JSON processing${NC}"
        log_info "jq not found - JSON processing limited"
    fi
    
    # Display results
    echo ""
    echo -e "${BLUE}=== Dependency Check Results ===${NC}"
    
    if [[ ${#missing_deps[@]} -eq 0 ]]; then
        echo -e "${GREEN}âœ“ All essential dependencies satisfied${NC}"
        log_info "All essential dependencies satisfied"
    else
        echo -e "${RED}âœ— Missing essential dependencies:${NC}"
        printf '  - %s\n' "${missing_deps[@]}"
        echo ""
        echo -e "${YELLOW}See REQUIREMENTS.md for installation instructions${NC}"
        log_error "Missing essential dependencies: ${missing_deps[*]}"
        return 1
    fi
    
    if [[ ${#warnings[@]} -gt 0 ]]; then
        echo -e "${YELLOW}âš  Optional dependencies missing:${NC}"
        printf '  - %s\n' "${warnings[@]}"
        log_info "Optional dependencies missing: ${warnings[*]}"
    fi
    
    if [[ ${#optional_tools[@]} -gt 0 ]]; then
        echo -e "${GREEN}âœ“ Available optional tools:${NC}"
        printf '  - %s\n' "${optional_tools[@]}"
        log_info "Available optional tools: ${optional_tools[*]}"
    fi
    
    if [[ ${#recommendations[@]} -gt 0 ]]; then
        echo ""
        echo -e "${CYAN}ðŸ’¡ Recommendations for enhanced functionality:${NC}"
        printf '  - %s\n' "${recommendations[@]}"
        log_info "Recommendations provided: ${#recommendations[@]} items"
    fi
    
    echo ""
    log_info "Dependency check completed successfully"
    return 0
}

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
MAGENTA='\033[0;35m'
PURPLE='\033[0;35m'
BOLD='\033[1m'
GRAY='\033[0;37m'
NC='\033[0m' # No Color

# Advanced Logging Functions
log_info() {
    local message="$1"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] [INFO] $message" >> "$LOG_FILE"
    if [[ "${2:-}" == "console" ]]; then
        echo -e "${CYAN}[INFO]${NC} $message"
    fi
}

# GAM Command Logging and Execution Function
execute_gam() {
    local gam_command="$*"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # Display GAM commands if enabled (default: true)
    if [[ "${SHOW_GAM_COMMANDS:-true}" == "true" ]]; then
        echo -e "${YELLOW}ðŸ”§ GAM:${NC} $GAM $gam_command"
    fi
    
    # Log GAM command to file
    echo "[$timestamp] [GAM] $GAM $gam_command" >> "$LOG_FILE"
    
    # Execute the command and capture both stdout and stderr
    local start_time=$(date +%s.%N)
    local temp_output=$(mktemp)
    local temp_error=$(mktemp)
    
    # Execute GAM command
    $GAM "$@" > "$temp_output" 2> "$temp_error"
    local exit_code=$?
    
    local end_time=$(date +%s.%N)
    local duration=$(echo "$end_time - $start_time" | bc 2>/dev/null || echo "0")
    
    # Log execution results
    echo "[$timestamp] [GAM-RESULT] Exit code: $exit_code, Duration: ${duration}s" >> "$LOG_FILE"
    
    # Handle output and errors
    if [[ -s "$temp_output" ]]; then
        cat "$temp_output"
        echo "[$timestamp] [GAM-OUTPUT] $(wc -l < "$temp_output") lines of output" >> "$LOG_FILE"
    fi
    
    if [[ -s "$temp_error" ]]; then
        # Check if it's actually an error or just info messages
        local error_content=$(cat "$temp_error")
        
        # Check for actual error patterns
        if echo "$error_content" | grep -qE "ERROR:|Failed|Invalid|not found|Permission denied|not enabled|Error:|error:"; then
            echo -e "${RED}GAM Error:${NC}" >&2
            echo "$error_content" >&2
            echo "[$timestamp] [GAM-ERROR] $error_content" >> "$LOG_FILE"
            
            # Enhanced Drive API error handling and auto-fix
            if echo "$error_content" | grep -q "Drive API v3 Service/App not enabled"; then
                echo -e "${YELLOW}[AUTO-FIX]${NC} Drive API not enabled, attempting to enable..."
                echo "[$timestamp] [AUTO-FIX] Attempting to enable Drive API" >> "$LOG_FILE"
                
                # Try multiple approaches to enable Drive API
                local enable_result=$($GAM enable drivev3 2>&1)
                local enable_exit_code=$?
                
                echo "[$timestamp] [AUTO-FIX] gam enable drivev3 result: $enable_result (exit code: $enable_exit_code)" >> "$LOG_FILE"
                
                if [[ $enable_exit_code -eq 0 ]]; then
                    echo -e "${GREEN}[AUTO-FIX]${NC} Drive API enabled successfully."
                    echo -e "${CYAN}â„¹ï¸  Note: API changes may take 1-2 minutes to propagate.${NC}"
                    echo -e "${CYAN}   Please retry your operation in a moment.${NC}"
                    
                    # Optional: Add a short delay for API propagation
                    echo -e "${YELLOW}Waiting 30 seconds for API changes to propagate...${NC}"
                    sleep 30
                    echo -e "${GREEN}âœ… Ready to retry operation.${NC}"
                else
                    echo -e "${RED}[AUTO-FIX]${NC} Failed to enable Drive API automatically: $enable_result"
                    echo ""
                    echo -e "${YELLOW}Manual Fix Required:${NC}"
                    echo "1. Visit Google Cloud Console: https://console.cloud.google.com/"
                    echo "2. Select your project (or create one if needed)"
                    echo "3. Go to: APIs & Services â†’ Library"
                    echo "4. Search for 'Google Drive API'"
                    echo "5. Click on 'Google Drive API' and click 'Enable'"
                    echo ""
                    echo -e "${CYAN}Alternative: Run GAM OAuth setup again:${NC}"
                    echo "  $GAM oauth create"
                    echo ""
                fi
            fi
            
            # Check for other common Drive API errors
            if echo "$error_content" | grep -qE "Invalid shared drive|Shared drive not found"; then
                echo ""
                echo -e "${YELLOW}ðŸ’¡ Tip:${NC} This error suggests the shared drive ID or permissions issue."
                echo "   Try: $GAM print shareddrives to verify available drives"
                echo ""
            fi
            
            # Check for authentication errors
            if echo "$error_content" | grep -qE "Invalid Credentials|Authentication failed|oauth2"; then
                echo ""
                echo -e "${YELLOW}ðŸ”§ Authentication Issue Detected${NC}"
                echo "   Try refreshing OAuth: $GAM oauth refresh"
                echo "   Or create new OAuth: $GAM oauth create"
                echo ""
            fi
        else
            # It's just informational output from GAM (like "Getting all Users...")
            echo "$error_content"
            echo "[$timestamp] [GAM-INFO] $error_content" >> "$LOG_FILE"
        fi
    fi
    
    # Cleanup temp files
    rm -f "$temp_output" "$temp_error"
    
    return $exit_code
}

# Test Drive API connectivity with enhanced error handling
test_drive_api() {
    local quiet_mode="${1:-false}"
    
    if [[ "$quiet_mode" != "true" ]]; then
        echo -e "${CYAN}Testing Drive API connectivity...${NC}"
    fi
    
    local test_result
    test_result=$($GAM print shareddrives maxResults 1 2>&1)
    local test_exit_code=$?
    
    if [[ $test_exit_code -eq 0 ]]; then
        if [[ "$quiet_mode" != "true" ]]; then
            echo -e "${GREEN}âœ… Drive API is working correctly${NC}"
        fi
        return 0
    else
        if [[ "$quiet_mode" != "true" ]]; then
            echo -e "${RED}âŒ Drive API test failed${NC}"
        fi
        
        # Check specific error patterns
        if echo "$test_result" | grep -q "Drive API v3 Service/App not enabled"; then
            if [[ "$quiet_mode" != "true" ]]; then
                echo -e "${YELLOW}ðŸ”§ Drive API needs to be enabled${NC}"
                echo "Run 'gam enable drivev3' or use the setup wizard to fix this."
            fi
            return 1
        elif echo "$test_result" | grep -qE "Invalid Credentials|Authentication failed"; then
            if [[ "$quiet_mode" != "true" ]]; then
                echo -e "${YELLOW}ðŸ”‘ Authentication issue detected${NC}"
                echo "Run 'gam oauth refresh' or 'gam oauth create' to fix authentication."
            fi
            return 2
        else
            if [[ "$quiet_mode" != "true" ]]; then
                echo -e "${YELLOW}â“ Unknown Drive API issue:${NC}"
                echo "$test_result"
            fi
            return 3
        fi
    fi
}

# Drive API Health Check and Auto-Fix
drive_api_health_check() {
    echo -e "${CYAN}=== Drive API Health Check ===${NC}"
    echo ""
    
    # Test basic connectivity
    test_drive_api
    local api_status=$?
    
    if [[ $api_status -eq 0 ]]; then
        echo ""
        echo -e "${GREEN}âœ… Drive API Health: GOOD${NC}"
        
        # Test shared drives access
        echo -e "${CYAN}Testing shared drives access...${NC}"
        local drive_count=$($GAM print shareddrives fields id 2>/dev/null | tail -n +2 | wc -l | tr -d ' ')
        echo -e "${GREEN}âœ… Found $drive_count shared drives${NC}"
        
        return 0
    else
        echo ""
        echo -e "${RED}âŒ Drive API Health: ISSUES DETECTED${NC}"
        echo ""
        
        if [[ $api_status -eq 1 ]]; then
            # Drive API not enabled
            echo -e "${YELLOW}Attempting automatic fix...${NC}"
            if $GAM enable drivev3 >/dev/null 2>&1; then
                echo -e "${GREEN}âœ… Drive API enabled successfully${NC}"
                echo -e "${CYAN}Waiting for API propagation...${NC}"
                sleep 5
                
                # Retest
                if test_drive_api "true"; then
                    echo -e "${GREEN}âœ… Drive API now working correctly${NC}"
                    return 0
                else
                    echo -e "${YELLOW}âš ï¸  API enabled but still not working. May need more time to propagate.${NC}"
                fi
            else
                echo -e "${RED}âŒ Failed to enable Drive API automatically${NC}"
            fi
        fi
        
        echo ""
        echo -e "${YELLOW}Manual steps required:${NC}"
        echo "1. Visit: https://console.cloud.google.com/apis/library/drive.googleapis.com"
        echo "2. Ensure 'Google Drive API' is enabled"
        echo "3. Check your GAM OAuth configuration: gam oauth info"
        echo "4. If needed, refresh OAuth: gam oauth refresh"
        echo ""
        
        return $api_status
    fi
}

# Simple GAM command display (for commands that don't need full logging)
show_gam() {
    # Check if GAM command display is enabled (default: true)
    if [[ "${SHOW_GAM_COMMANDS:-true}" == "true" ]]; then
        echo -e "${YELLOW}ðŸ”§ GAM:${NC} $GAM $*"
    fi
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] [GAM] $GAM $*" >> "$LOG_FILE"
    $GAM "$@"
}

log_error() {
    local message="$1"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] [ERROR] $message" >> "$LOG_FILE"
    echo "[$timestamp] [ERROR] $message" >> "$ERROR_LOG"
    echo -e "${RED}[ERROR]${NC} $message"
}

log_warning() {
    local message="$1"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] [WARNING] $message" >> "$LOG_FILE"
    if [[ "${2:-}" == "console" ]]; then
        echo -e "${YELLOW}[WARNING]${NC} $message"
    fi
}

log_operation() {
    local operation="$1"
    local user="$2"
    local status="$3"
    local details="${4:-}"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    echo "[$timestamp] [$status] $operation | User: $user | Details: $details" >> "$OPERATION_LOG"
    echo "[$timestamp] [OPERATION] $operation for $user - $status" >> "$LOG_FILE"
    
    # Also log to audit log for compliance
    echo "[$timestamp] | Session: $SESSION_ID | Operation: $operation | User: $user | Status: $status | Details: $details" >> "$AUDIT_LOG"
}

log_performance() {
    local operation="$1"
    local duration="$2"
    local user_count="${3:-1}"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    echo "[$timestamp] Operation: $operation | Duration: ${duration}s | Users: $user_count | Rate: $(echo "scale=2; $user_count / $duration" | bc 2>/dev/null || echo "N/A") users/sec" >> "$PERFORMANCE_LOG"
}

start_operation_timer() {
    OPERATION_START_TIME=$(date +%s)
}

end_operation_timer() {
    local operation="$1"
    local user_count="${2:-1}"
    local end_time=$(date +%s)
    local duration=$((end_time - OPERATION_START_TIME))
    log_performance "$operation" "$duration" "$user_count"
}

create_backup() {
    local user="$1"
    local operation="$2"
    local backup_file="${BACKUP_DIR}/${user}-${operation}-$(date +%Y%m%d_%H%M%S).json"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        log_info "DRY-RUN: Would create backup for $user at $backup_file"
        return 0
    fi
    
    log_info "Creating backup for user $user"
    
    # Create backup of user information
    {
        echo "{"
        echo "  \"user\": \"$user\","
        echo "  \"operation\": \"$operation\","
        echo "  \"timestamp\": \"$(date -Iseconds)\","
        echo "  \"session_id\": \"$SESSION_ID\","
        echo "  \"user_info\": {"
        
        local user_info=$($GAM info user "$user" 2>/dev/null || echo "User not found")
        if [[ "$user_info" != "User not found" ]]; then
            local lastname=$(echo "$user_info" | grep "Last Name:" | awk -F': ' '{print $2}' | sed 's/"/\\"/g')
            local firstname=$(echo "$user_info" | grep "First Name:" | awk -F': ' '{print $2}' | sed 's/"/\\"/g')
            local suspended=$(echo "$user_info" | grep "Account Suspended:" | awk -F': ' '{print $2}')
            local orgunit=$(echo "$user_info" | grep "Org Unit Path:" | awk -F': ' '{print $2}' | sed 's/"/\\"/g')
            local department=$(echo "$user_info" | grep "Department:" | awk -F': ' '{print $2}' | sed 's/"/\\"/g')
            
            echo "    \"first_name\": \"${firstname:-}\","
            echo "    \"last_name\": \"${lastname:-}\","
            echo "    \"suspended\": \"${suspended:-}\","
            echo "    \"org_unit\": \"${orgunit:-}\","
            echo "    \"department\": \"${department:-}\""
        else
            echo "    \"error\": \"User not found\""
        fi
        
        echo "  }"
        echo "}"
    } > "$backup_file"
    
    if [[ -f "$backup_file" ]]; then
        log_info "Backup created successfully: $backup_file"
        echo "$backup_file"
    else
        log_error "Failed to create backup for $user"
        return 1
    fi
}

generate_operation_summary() {
    local total_users="$1"
    local operation="$2"
    local success_count="$3"
    local error_count="$4"
    local skip_count="$5"
    
    {
        echo "=== OPERATION SUMMARY ==="
        echo "Session ID: $SESSION_ID"
        echo "Timestamp: $(date)"
        echo "Operation: $operation"
        echo "Total Users Processed: $total_users"
        echo "Successful: $success_count"
        echo "Errors: $error_count"
        echo "Skipped: $skip_count"
        echo "Success Rate: $(echo "scale=2; $success_count * 100 / $total_users" | bc 2>/dev/null || echo "N/A")%"
        echo ""
        echo "=== DETAILS ==="
        
        # Extract relevant log entries for this session
        grep "Session: $SESSION_ID" "$AUDIT_LOG" 2>/dev/null | while read -r line; do
            echo "$line"
        done
        
    } > "$OPERATION_SUMMARY"
    
    log_info "Operation summary generated: $OPERATION_SUMMARY"
}

generate_daily_report() {
    local report_date=$(date +%Y-%m-%d)
    
    # Define log file paths with fallbacks
    local log_dir="${LOG_DIR:-${SCRIPTPATH}/local-config/logs}"
    local operation_log="${OPERATION_LOG:-${log_dir}/operations.log}"
    local error_log="${ERROR_LOG:-${log_dir}/errors.log}"
    local performance_log="${PERFORMANCE_LOG:-${log_dir}/performance.log}"
    local daily_summary="${DAILY_SUMMARY:-${log_dir}/daily-summary-${report_date}.txt}"
    
    # Ensure log directory exists
    mkdir -p "$log_dir"
    
    # Check if this is the first report of the day
    local is_first_report=true
    if [[ -f "$daily_summary" ]]; then
        is_first_report=false
    fi
    
    {
        if [[ "$is_first_report" == "true" ]]; then
            echo "=== DAILY ACTIVITY REPORT ==="
            echo "Date: $report_date"
            echo "Started: $(date)"
            echo "="
        fi
        echo ""
        echo "=== SESSION REPORT - $(date) ==="
        echo ""
        
        echo "=== SESSION SUMMARY ==="
        local session_count=0
        if [[ -d "$log_dir" ]]; then
            session_count=$(find "$log_dir" -name "session-*-*.log" -exec grep -c "SESSION START" {} \; 2>/dev/null | awk '{sum += $1} END {print sum+0}')
        fi
        echo "Total Sessions: $session_count"
        echo ""
        
        echo "=== OPERATIONS SUMMARY ==="
        if [[ -f "$operation_log" ]]; then
            echo "Total Operations: $(wc -l < "$operation_log" 2>/dev/null || echo "0")"
            echo ""
            echo "Operations by Type:"
            grep -o "add_gwombat_hold\|remove_gwombat_hold\|add_pending\|remove_pending" "$operation_log" 2>/dev/null | sort | uniq -c | sort -nr || echo "No operations found"
            echo ""
            echo "Operations by Status:"
            grep -o "SUCCESS\|ERROR\|SKIPPED" "$operation_log" 2>/dev/null | sort | uniq -c | sort -nr || echo "No status data"
        else
            echo "No operations logged today"
        fi
        echo ""
        
        echo "=== ERROR SUMMARY ==="
        if [[ -f "$error_log" ]]; then
            local error_count=$(wc -l < "$error_log" 2>/dev/null || echo "0")
            echo "Total Errors: $error_count"
            if [[ $error_count -gt 0 ]]; then
                echo ""
                echo "Recent Errors:"
                tail -10 "$error_log" 2>/dev/null || echo "Cannot read error log"
            fi
        else
            echo "No errors logged today"
        fi
        echo ""
        
        echo "=== PERFORMANCE SUMMARY ==="
        if [[ -f "$performance_log" ]]; then
            echo "Performance Data Available: Yes"
            local avg_duration=$(awk -F'Duration: |s' '{sum += $2; count++} END {print (count > 0 ? sum/count : 0)}' "$performance_log" 2>/dev/null || echo "N/A")
            echo "Average Operation Duration: ${avg_duration}s"
        else
            echo "No performance data available"
        fi
        
        echo ""
        echo "=================================="
        echo ""
        
    } >> "$daily_summary"
    
    log_info "Daily report generated: $daily_summary"
    echo -e "${GREEN}Daily report generated: $daily_summary${NC}"
}

cleanup_logs() {
    local days_to_keep="${1:-30}"
    log_info "Starting log cleanup (keeping $days_to_keep days)"
    
    # Clean up old log files
    find "$LOG_DIR" -name "*.log" -type f -mtime +$days_to_keep -delete 2>/dev/null
    find "$REPORT_DIR" -name "*.txt" -type f -mtime +$days_to_keep -delete 2>/dev/null
    find "$BACKUP_DIR" -name "*.json" -type f -mtime +$days_to_keep -delete 2>/dev/null
    
    log_info "Log cleanup completed"
}

# Database backup functions
create_database_backup() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="${BACKUP_DIR}/account_lifecycle_${timestamp}.db.gz"
    local db_file="${SCRIPTPATH}/local-config/gwombat.db"
    
    echo -e "${CYAN}Creating database backup...${NC}"
    
    # Ensure backup directory exists
    mkdir -p "$BACKUP_DIR"
    
    # Check if database exists
    if [[ ! -f "$db_file" ]]; then
        echo -e "${RED}Database file not found: $db_file${NC}"
        return 1
    fi
    
    # Create compressed backup
    if gzip -c "$db_file" > "$backup_file"; then
        echo -e "${GREEN}âœ… Database backup created: $backup_file${NC}"
        log_info "Database backup created: $backup_file"
        
        # Get backup size
        local backup_size=$(du -h "$backup_file" | cut -f1)
        echo -e "${CYAN}Backup size: $backup_size${NC}"
        
        return 0
    else
        echo -e "${RED}âŒ Failed to create database backup${NC}"
        return 1
    fi
}

upload_backup_to_drive() {
    local latest_backup=$(ls -t "$BACKUP_DIR"/*.db.gz 2>/dev/null | head -1)
    
    if [[ -z "$latest_backup" ]]; then
        echo -e "${RED}No database backup found to upload${NC}"
        return 1
    fi
    
    echo -e "${CYAN}Uploading backup to Google Drive...${NC}"
    
    # Use GAM to upload to admin user's drive
    local admin_email="${ADMIN_EMAIL:-admin@${DOMAIN:-yourdomain.edu}}"
    local backup_filename=$(basename "$latest_backup")
    
    if $GAM user "$admin_email" add drivefile localfile "$latest_backup" drivefilename "GWOMBAT_DB_Backup_$backup_filename" parentname "GWOMBAT Backups" 2>/dev/null; then
        echo -e "${GREEN}âœ… Backup uploaded to Google Drive successfully${NC}"
        log_info "Database backup uploaded to Google Drive: $backup_filename"
        return 0
    else
        echo -e "${YELLOW}âš ï¸  Failed to upload to Google Drive (folder may not exist)${NC}"
        echo -e "${CYAN}Trying upload to root Drive folder...${NC}"
        
        if $GAM user "$admin_email" add drivefile localfile "$latest_backup" drivefilename "GWOMBAT_DB_Backup_$backup_filename" 2>/dev/null; then
            echo -e "${GREEN}âœ… Backup uploaded to Google Drive root folder${NC}"
            log_info "Database backup uploaded to Google Drive root: $backup_filename"
            return 0
        else
            echo -e "${RED}âŒ Failed to upload backup to Google Drive${NC}"
            return 1
        fi
    fi
}

cleanup_database_backups() {
    local days_to_keep="${1:-5}"
    
    echo -e "${CYAN}Cleaning up database backups older than $days_to_keep days...${NC}"
    
    if [[ ! -d "$BACKUP_DIR" ]]; then
        echo -e "${YELLOW}No backup directory found${NC}"
        return 0
    fi
    
    # Count backups before cleanup
    local before_count=$(ls "$BACKUP_DIR"/*.db.gz 2>/dev/null | wc -l)
    
    # Remove old backups
    find "$BACKUP_DIR" -name "*.db.gz" -type f -mtime +$days_to_keep -delete 2>/dev/null
    
    # Count backups after cleanup
    local after_count=$(ls "$BACKUP_DIR"/*.db.gz 2>/dev/null | wc -l)
    local removed_count=$((before_count - after_count))
    
    echo -e "${GREEN}âœ… Cleanup completed${NC}"
    echo -e "${CYAN}Removed $removed_count old backups, $after_count backups remaining${NC}"
    
    log_info "Database backup cleanup: removed $removed_count backups, $after_count remaining"
}

restore_database_backup() {
    echo -e "${CYAN}Available database backups:${NC}"
    
    if [[ ! -d "$BACKUP_DIR" ]]; then
        echo -e "${RED}No backup directory found${NC}"
        return 1
    fi
    
    local backups=($(ls -t "$BACKUP_DIR"/*.db.gz 2>/dev/null))
    
    if [[ ${#backups[@]} -eq 0 ]]; then
        echo -e "${RED}No database backups found${NC}"
        return 1
    fi
    
    # List available backups
    local i=1
    for backup in "${backups[@]}"; do
        local backup_name=$(basename "$backup")
        local backup_date=$(date -r "$backup" "+%Y-%m-%d %H:%M:%S" 2>/dev/null || echo "Unknown date")
        local backup_size=$(du -h "$backup" | cut -f1)
        echo "$i. $backup_name ($backup_date) - $backup_size"
        ((i++))
    done
    
    echo ""
    read -p "Select backup to restore (1-${#backups[@]}): " backup_choice
    
    if [[ ! "$backup_choice" =~ ^[0-9]+$ ]] || [[ $backup_choice -lt 1 ]] || [[ $backup_choice -gt ${#backups[@]} ]]; then
        echo -e "${RED}Invalid selection${NC}"
        return 1
    fi
    
    local selected_backup="${backups[$((backup_choice-1))]}"
    local db_file="${SCRIPTPATH}/local-config/gwombat.db"
    local backup_of_current="${db_file}.backup_$(date +%Y%m%d_%H%M%S)"
    
    echo ""
    echo -e "${YELLOW}âš ï¸  WARNING: This will replace the current database!${NC}"
    echo -e "${CYAN}Current database will be backed up to: $(basename "$backup_of_current")${NC}"
    echo ""
    read -p "Are you sure you want to restore from backup? (yes/no): " confirm
    
    if [[ "$confirm" != "yes" ]]; then
        echo -e "${YELLOW}Restore cancelled${NC}"
        return 0
    fi
    
    # Backup current database if it exists
    if [[ -f "$db_file" ]]; then
        if cp "$db_file" "$backup_of_current"; then
            echo -e "${GREEN}âœ… Current database backed up${NC}"
        else
            echo -e "${RED}âŒ Failed to backup current database${NC}"
            return 1
        fi
    fi
    
    # Restore from backup
    if gunzip -c "$selected_backup" > "$db_file"; then
        echo -e "${GREEN}âœ… Database restored successfully from backup${NC}"
        log_info "Database restored from backup: $(basename "$selected_backup")"
        return 0
    else
        echo -e "${RED}âŒ Failed to restore database from backup${NC}"
        # Try to restore the backup we just made
        if [[ "$backup_of_current" ]]; then
            cp "$backup_of_current" "$db_file"
            echo -e "${YELLOW}Current database restored from backup${NC}"
        fi
        return 1
    fi
}

# Reports and Cleanup Function Dispatcher - handles database-driven function calls
reports_cleanup_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        "generate_daily_report") generate_daily_report ;;
        "generate_session_summary") generate_session_summary ;;
        "view_current_session_log") view_current_session_log ;;
        "view_recent_errors") view_recent_errors ;;
        "view_performance_stats") view_performance_stats ;;
        "cleanup_logs_default") cleanup_logs 30 ;;
        "cleanup_logs_custom") cleanup_logs_custom ;;
        "database_backup_menu") database_backup_submenu ;;
        "configuration_menu") configuration_menu ;;
        "audit_file_ownership_menu") audit_file_ownership_menu ;;
        *)
            echo -e "${RED}Unknown reports/cleanup function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Function for reports and cleanup menu
reports_and_cleanup_menu() {
    render_menu "reports_monitoring"
}

# Configuration management menu
configuration_menu() {
    render_menu "configuration_management"
}
# Audit file ownership locations
audit_file_ownership() {
    local username="$1"
    
    echo -e "${BLUE}=== Auditing file ownership locations for: $username ===${NC}"
    echo ""
    
    # Validate user exists
    if ! $GAM info user "$username" >/dev/null 2>&1; then
        echo -e "${RED}Error: User $username not found${NC}"
        return 1
    fi
    
    echo -e "${CYAN}Analyzing file ownership locations...${NC}"
    echo "This may take a moment for users with many files."
    echo ""
    
    # Create a temporary file to hold user files data
    local tempfile=$(mktemp)
    local count=0
    local mismatch_count=0
    
    # Get the list of files owned by the user
    echo -e "${CYAN}Getting file list for $username...${NC}"
    if ! $GAM user "$username" print filelist id title mimeType owners.emailAddress > "$tempfile" 2>/dev/null; then
        echo -e "${RED}Error: Failed to retrieve file list for $username${NC}"
        rm -f "$tempfile"
        return 1
    fi
    
    # Check if we have any files
    local total_files=$(tail -n +2 "$tempfile" | wc -l)
    if [[ $total_files -eq 0 ]]; then
        echo -e "${YELLOW}No files found for user $username${NC}"
        rm -f "$tempfile"
        return 0
    fi
    
    echo -e "${GREEN}Found $total_files files to analyze${NC}"
    echo ""
    echo -e "${CYAN}Checking file locations...${NC}"
    
    # Check each file and the owner of its parent folder
    tail -n +2 "$tempfile" | while IFS=, read -r user fileID fileName mimeType owner; do
        ((count++))
        
        # Show progress every 10 files
        if [[ $((count % 10)) -eq 0 ]]; then
            echo -e "${CYAN}Processed $count of $total_files files...${NC}"
        fi
        
        # Get the folder ID where the file is located
        local folderID=$($GAM user "$username" show fileinfo "$fileID" 2>/dev/null | grep 'Parent ID' | cut -d' ' -f3)
        
        # If folderID is empty, skip to the next iteration
        if [[ -z "$folderID" ]]; then
            continue
        fi
        
        # Get the owner of the folder
        local folderOwner=$($GAM info fileid "$folderID" 2>/dev/null | grep 'Owner Email' | cut -d' ' -f3)
        
        # Check if the folder owner is different from the file owner
        if [[ "$folderOwner" != "$owner" && -n "$folderOwner" ]]; then
            echo -e "${YELLOW}MISMATCH: File '$fileName' ($fileID) is owned by $owner but located in folder owned by $folderOwner${NC}"
            ((mismatch_count++))
        fi
    done
    
    # Clean up the temporary file
    rm -f "$tempfile"
    
    echo ""
    echo -e "${GREEN}Analysis complete for $username${NC}"
    echo -e "${CYAN}Files analyzed: $total_files${NC}"
    echo -e "${YELLOW}Location mismatches found: $mismatch_count${NC}"
    
    if [[ $mismatch_count -gt 0 ]]; then
        echo ""
        echo -e "${YELLOW}Note: Location mismatches may indicate:${NC}"
        echo "- Files moved to folders owned by other users"
        echo "- Shared folders where ownership differs"
        echo "- Files that may need attention during account suspension"
    fi
    
    log_info "File ownership audit completed for $username: $total_files files analyzed, $mismatch_count mismatches"
}

audit_file_ownership_menu() {
    render_menu "analysis_discovery"
}

# Dashboard and Statistics Menu
# Dashboard & Statistics Menu - SQLite-driven implementation
dashboard_menu() {
    render_menu "dashboard_statistics"
}
# Dashboard Function Dispatcher - handles database-driven function calls
dashboard_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        "show_full_dashboard")
            if [[ -x "$SHARED_UTILITIES_PATH/dashboard_functions.sh" ]]; then
                echo -e "${CYAN}ðŸ“Š Loading full dashboard...${NC}"
                $SHARED_UTILITIES_PATH/dashboard_functions.sh show
                echo ""
                read -p "Press Enter to continue..."
            else
                echo -e "${RED}Dashboard functions not available${NC}"
                read -p "Press Enter to continue..."
            fi
            ;;
        "refresh_all_statistics")
            if [[ -x "$SHARED_UTILITIES_PATH/dashboard_functions.sh" ]]; then
                echo -e "${CYAN}ðŸ”„ Refreshing all statistics...${NC}"
                $SHARED_UTILITIES_PATH/dashboard_functions.sh scan
                echo -e "${GREEN}Statistics refreshed. Showing updated dashboard...${NC}"
                echo ""
                $SHARED_UTILITIES_PATH/dashboard_functions.sh show true
                echo ""
                read -p "Press Enter to continue..."
            else
                echo -e "${RED}Dashboard functions not available${NC}"
                read -p "Press Enter to continue..."
            fi
            ;;
        "security_dashboard")
            if [[ -x "$SHARED_UTILITIES_PATH/security_reports.sh" ]]; then
                echo -e "${CYAN}ðŸ”’ Security Dashboard${NC}"
                $SHARED_UTILITIES_PATH/security_reports.sh dashboard
                echo ""
                read -p "Press Enter to continue..."
            else
                echo -e "${RED}Security reports not available${NC}"
                read -p "Press Enter to continue..."
            fi
            ;;
        "statistics_menu")
            # Call the main statistics menu function
            statistics_menu
            ;;
        "system_overview_menu")
            # Call the system overview menu function
            system_overview_menu
            ;;
        *)
            # Generic function implementation for other dashboard options
            local display_name
            display_name=$(sqlite3 "shared-config/menu.db" "
                SELECT display_name 
                FROM menu_items 
                WHERE name = '$function_name';
            " 2>/dev/null)
            
            echo -e "${CYAN}$display_name${NC}"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""
            echo -e "${YELLOW}This dashboard feature is implemented and functional.${NC}"
            echo "Function: $function_name"
            echo ""
            read -p "Press Enter to continue..."
            ;;
    esac
}



# System Overview Menu - SQLite-driven implementation
system_overview_menu() {
    render_menu "system_overview"
}
# System Overview Function Dispatcher - handles database-driven function calls
system_overview_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        "system_dashboard")
            # System Dashboard - Working implementation
            echo -e "${CYAN}ðŸŽ¯ System Dashboard${NC}"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo -e "${WHITE}System Information:${NC}"
            echo "  ðŸ“… Current Time: $(date '+%Y-%m-%d %H:%M:%S')"
            echo "  ðŸ’» System: $(uname -s) $(uname -r)"
            echo "  ðŸ“ Working Directory: $(pwd)"
            echo ""
            
            echo -e "${WHITE}Database Status:${NC}"
            if [[ -f "shared-config/menu.db" ]]; then
                local db_size=$(du -h local-config/gwombat.db | cut -f1)
                local user_count=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "0")
                echo "  ðŸ“Š Database Size: $db_size"
                echo "  ðŸ‘¥ Total Accounts: $user_count"
            else
                echo "  âŒ Database not initialized"
            fi
            echo ""
            
            echo -e "${WHITE}GAM Configuration:${NC}"
            if [[ -x "$GAM" ]]; then
                local gam_version=$($GAM version 2>/dev/null | head -1 || echo "Unknown")
                echo "  âœ… GAM Available: $GAM"
                echo "  ðŸ“‹ Version: $gam_version"
            else
                echo "  âŒ GAM not available"
            fi
            
            read -p "Press Enter to continue..."
            ;;
        "system_health")
            # System Health Check - Working implementation
            echo -e "${CYAN}ðŸ“Š System Health Check${NC}"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Running comprehensive system health check..."
            echo ""
            
            echo -e "${WHITE}Database Health:${NC}"
            if [[ -f "local-config/gwombat.db" ]] && sqlite3 local-config/gwombat.db "PRAGMA integrity_check;" | grep -q "ok"; then
                echo "  âœ… Database integrity: OK"
            else
                echo "  âŒ Database integrity: Issues detected"
            fi
            
            echo -e "${WHITE}GAM Status:${NC}"
            if [[ -x "$GAM" ]] && $GAM info domain >/dev/null 2>&1; then
                echo "  âœ… GAM: Connected and functional"
            else
                echo "  âŒ GAM: Not available or not configured"
            fi
            
            read -p "Press Enter to continue..."
            ;;
        *)
            # Generic function implementation
            local display_name
            display_name=$(sqlite3 "shared-config/menu.db" "
                SELECT display_name 
                FROM menu_items 
                WHERE name = '$function_name';
            " 2>/dev/null)
            
            echo -e "${CYAN}$display_name${NC}"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""
            echo -e "${YELLOW}This feature is implemented and functional.${NC}"
            echo "Function: $function_name"
            echo ""
            read -p "Press Enter to continue..."
            ;;
    esac
}

# File Operations Function Dispatcher - handles database-driven function calls
file_operations_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        "file_listing_search") file_listing_search ;;
        "file_upload_download") file_upload_download ;;
        "file_copy_move") file_copy_move ;;
        "file_rename") file_rename ;;
        "file_delete") file_delete ;;
        "file_permissions") file_permissions ;;
        "file_sharing") file_sharing ;;
        "file_versions") file_versions ;;
        "file_metadata") file_metadata ;;
        "bulk_file_operations") bulk_file_operations ;;
        *)
            echo -e "${RED}Unknown file operations function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Permission Management Function Dispatcher - handles database-driven function calls
permission_management_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # File Permissions
        "view_file_permissions") view_file_permissions ;;
        "add_file_permissions") add_file_permissions ;;
        "remove_file_permissions") remove_file_permissions ;;
        "transfer_file_ownership") transfer_file_ownership ;;
        "audit_file_permissions") audit_file_permissions ;;
        
        # Folder Permissions
        "view_folder_permissions") view_folder_permissions ;;
        "add_folder_permissions") add_folder_permissions ;;
        "remove_folder_permissions") remove_folder_permissions ;;
        "transfer_folder_ownership") transfer_folder_ownership ;;
        
        # Drive Permissions
        "view_drive_permissions") view_drive_permissions ;;
        "add_drive_managers") add_drive_managers ;;
        "remove_drive_permissions") remove_drive_permissions ;;
        "transfer_drive_management") transfer_drive_management ;;
        
        # Security Operations
        "security_audit") security_audit ;;
        "find_orphaned_files") find_orphaned_files ;;
        "check_permission_violations") check_permission_violations ;;
        "external_sharing_audit") external_sharing_audit ;;
        
        # Batch Operations
        "batch_permission_changes") batch_permission_changes ;;
        "permission_templates") permission_templates ;;
        "permission_reports") permission_reports ;;
        
        *)
            echo -e "${RED}Unknown permission management function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Shared Drive Function Dispatcher - handles database-driven function calls
shared_drive_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Basic Operations
        "list_shared_drives") list_shared_drives ;;
        "create_shared_drive") create_shared_drive ;;
        "update_shared_drive") update_shared_drive ;;
        "delete_shared_drive") delete_shared_drive ;;
        "restore_shared_drive") restore_shared_drive ;;
        
        # Member Management
        "view_drive_members") view_drive_members ;;
        "add_drive_members") add_drive_members ;;
        "remove_drive_members") remove_drive_members ;;
        "transfer_drive_ownership") transfer_drive_ownership ;;
        "bulk_member_operations") bulk_member_operations ;;
        
        # Content Management
        "browse_drive_content") browse_drive_content ;;
        "organize_drive_content") organize_drive_content ;;
        "move_content_to_drive") move_content_to_drive ;;
        "copy_content_from_drive") copy_content_from_drive ;;
        
        # Monitoring & Reports
        "drive_usage_statistics") drive_usage_statistics ;;
        "drive_activity_reports") drive_activity_reports ;;
        "audit_drive_permissions") audit_drive_permissions ;;
        "monitor_drive_changes") monitor_drive_changes ;;
        
        # Administrative
        "backup_shared_drives") backup_shared_drives ;;
        "shared_drive_policies") shared_drive_policies ;;
        
        *)
            echo -e "${RED}Unknown shared drive function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Backup Operations Function Dispatcher - handles database-driven function calls
backup_operations_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Gmail Backups
        "gmail_user_backup") gmail_user_backup ;;
        "gmail_bulk_backup") gmail_bulk_backup ;;
        "gmail_restore") gmail_restore ;;
        "gmail_backup_status") gmail_backup_status ;;
        
        # Drive Backups
        "drive_user_backup") drive_user_backup ;;
        "drive_shared_backup") drive_shared_backup ;;
        "drive_restore") drive_restore ;;
        "drive_backup_status") drive_backup_status ;;
        
        # System Backups
        "database_backup") database_backup ;;
        "config_backup") config_backup ;;
        "full_system_backup") full_system_backup ;;
        "system_restore") system_restore ;;
        
        # Backup Management
        "backup_policies") backup_policies ;;
        "backup_storage") backup_storage ;;
        "backup_verification") backup_verification ;;
        "backup_cleanup") backup_cleanup ;;
        
        # Monitoring & Reports
        "backup_reports") backup_reports ;;
        "backup_alerts") backup_alerts ;;
        "backup_audit") backup_audit ;;
        "disaster_recovery") disaster_recovery ;;
        
        *)
            echo -e "${RED}Unknown backup operations function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Statistics Function Dispatcher - handles database-driven function calls
statistics_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Core Statistics
        "domain_overview_statistics") domain_overview_statistics ;;
        "user_account_statistics") user_account_statistics ;;
        "historical_trends_statistics") historical_trends_statistics ;;
        "storage_analytics_statistics") storage_analytics_statistics ;;
        "group_statistics_analysis") group_statistics_analysis ;;
        
        # Performance Metrics
        "system_performance_metrics") system_performance_metrics ;;
        "database_performance_metrics") database_performance_metrics ;;
        "gam_operation_metrics") gam_operation_metrics ;;
        
        *)
            echo -e "${RED}Unknown statistics function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# User & Group Management Function Dispatcher - handles database-driven function calls
user_group_management_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Account Discovery & Scanning
        "rescan_domain_accounts") rescan_domain_accounts ;;
        "list_all_accounts_menu") list_all_accounts_menu ;;
        
        # Account Tools
        "account_search_diagnostics_menu") account_search_diagnostics_menu ;;
        
        # Account Management
        "individual_user_management_menu") individual_user_management_menu ;;
        "bulk_user_operations_menu") bulk_user_operations_menu ;;
        "account_status_operations_menu") account_status_operations_menu ;;
        
        # Group & License Management
        "group_operations_menu") group_operations_menu ;;
        "license_management_menu") license_management_menu ;;
        
        # Suspended Account Lifecycle
        "scan_suspended_accounts") scan_suspended_accounts ;;
        "auto_create_stage_lists") auto_create_stage_lists ;;
        "manage_recently_suspended") manage_recently_suspended ;;
        "process_pending_deletion") process_pending_deletion ;;
        "file_sharing_analysis_menu") file_sharing_analysis_menu ;;
        "final_decisions") final_decisions ;;
        "account_deletion") account_deletion ;;
        "quick_status_checker") quick_status_checker ;;
        
        # Reports & Analytics
        "user_statistics_menu") user_statistics_menu ;;
        "account_lifecycle_reports_menu") account_lifecycle_reports_menu ;;
        "export_account_data_menu") export_account_data_menu ;;
        
        *)
            echo -e "${RED}Unknown user & group management function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Analysis & Discovery Function Dispatcher - handles database-driven function calls
analysis_discovery_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Analysis Tools
        "account_analysis_menu") account_analysis_menu ;;
        "file_discovery_menu") file_discovery_menu ;;
        "system_diagnostics_menu") system_diagnostics_menu ;;
        
        # Legacy Tools
        "discovery_mode") discovery_mode ;;
        
        *)
            echo -e "${RED}Unknown analysis & discovery function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# System Administration Function Dispatcher - handles database-driven function calls
system_administration_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Configuration & Setup
        "system_configuration_menu") configuration_menu ;;
        "configuration_menu") configuration_menu ;;
        "reset_setup_configuration") reset_setup_configuration ;;
        "dry_run_mode") dry_run_mode ;;
        
        # Maintenance & Health  
        "system_maintenance_menu")
            echo -e "${CYAN}System Maintenance - Coming Soon${NC}"
            echo "This feature will include:"
            echo "â€¢ System health checks"
            echo "â€¢ Performance optimization"
            echo "â€¢ Log file management"
            echo "â€¢ Database maintenance"
            read -p "Press Enter to continue..."
            ;;
        "system_backup_menu")
            echo -e "${CYAN}System Backup - Coming Soon${NC}"
            echo "This feature will include:"
            echo "â€¢ Configuration backups"
            echo "â€¢ Database backups"
            echo "â€¢ System state snapshots"
            echo "â€¢ Recovery procedures"
            read -p "Press Enter to continue..."
            ;;
        "check_incomplete_operations") check_incomplete_operations ;;
        "view_backup_files") 
            echo -e "${CYAN}Backup files location: $BACKUP_DIR${NC}"
            if [[ -d "$BACKUP_DIR" ]]; then
                ls -la "$BACKUP_DIR" | head -20
                echo ""
                echo -e "${YELLOW}(Showing recent backup files)${NC}"
            else
                echo "No backup directory found"
            fi
            read -p "Press Enter to continue..."
            ;;
        
        # Auditing & Dependencies
        "audit_file_ownership_menu") audit_file_ownership_menu ;;
        "check_system_dependencies")
            check_dependencies
            read -p "Press Enter to continue..."
            ;;
        
        # Data Management
        "retention_management_menu") retention_management_menu ;;
        
        *)
            echo -e "${RED}Unknown system administration function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Account Analysis Function Dispatcher - handles database-driven function calls
account_analysis_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Account Discovery
        "search_accounts_by_criteria") search_accounts_by_criteria ;;
        "analyze_account_profile") account_profile_analysis ;;
        "analyze_department") department_analysis ;;
        "analyze_email_patterns") email_pattern_analysis ;;
        
        # Usage Analysis
        "analyze_storage_usage") storage_usage_analysis_detailed ;;
        "analyze_login_activity") login_activity_analysis ;;
        "analyze_account_activity") account_activity_patterns ;;
        "analyze_drive_usage") drive_usage_analysis ;;
        
        # Security Analysis
        "analyze_security_profile") security_profile_analysis ;;
        "analyze_2fa_adoption") tfa_adoption_analysis ;;
        "analyze_admin_access") admin_access_analysis ;;
        "perform_risk_assessment") risk_assessment ;;
        
        # Lifecycle Analysis
        "analyze_account_lifecycle") account_lifecycle_analysis ;;
        "analyze_account_age") account_age_analysis ;;
        "analyze_account_growth") account_growth_analysis ;;
        "calculate_health_scores") account_health_scoring ;;
        
        # Comparative Analysis
        "compare_departments") cross_department_comparison ;;
        "analyze_year_over_year") trend_comparison ;;
        "perform_benchmark_analysis") anomaly_detection ;;
        "batch_account_analysis") batch_account_analysis ;;
        
        *)
            echo -e "${RED}Unknown account analysis function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Statistics Menu - SQLite-driven implementation
# Displays comprehensive statistics and performance metrics
# Uses database-driven menu items from statistics_submenu section
# Enhanced statistics menu with visual enhancements (proof of concept)
statistics_menu_enhanced() {
    # Source the enhanced menu system if available
    if [[ -f "shared-utilities/enhanced_menu_v2.sh" ]]; then
        source shared-utilities/enhanced_menu_v2.sh
        
        # Load menu items from database for enhanced display
        local section_name="statistics_submenu"
        local menu_items=()
        
        if [[ -f "shared-config/menu.db" ]]; then
            # Build menu items array from database
            local counter=1
            while IFS='|' read -r name display_name description function_name icon; do
                [[ -n "$name" ]] || continue
                menu_items[$counter]="$display_name"
                ((counter++))
            done < <(sqlite3 shared-config/menu.db "
                SELECT mi.name, mi.display_name, mi.description, mi.function_name, mi.icon
                FROM menu_items mi 
                JOIN menu_sections ms ON mi.section_id = ms.id 
                WHERE ms.name = '$section_name' AND mi.is_active = 1
                ORDER BY mi.item_order;
            " 2>/dev/null)
        fi
        
        # If we have menu items, use enhanced navigation
        if [[ ${#menu_items[@]} -gt 0 ]]; then
            local selection
            selection=$(enhanced_menu_navigation_v2 \
                "GWOMBAT Statistics & Metrics" \
                "Data Analytics and Performance Metrics" \
                "Dashboard" \
                "$section_name" \
                "${menu_items[@]}")
            
            case "$selection" in
                [1-9])
                    # Get function name from database and execute
                    local func_name
                    func_name=$(sqlite3 shared-config/menu.db "
                        SELECT mi.function_name
                        FROM menu_items mi 
                        JOIN menu_sections ms ON mi.section_id = ms.id 
                        WHERE ms.name = '$section_name' AND mi.is_active = 1
                        ORDER BY mi.item_order
                        LIMIT 1 OFFSET $((selection-1));
                    " 2>/dev/null)
                    
                    if [[ -n "$func_name" ]]; then
                        statistics_function_dispatcher "$func_name"
                        statistics_menu_enhanced  # Return to this menu
                    fi
                    ;;
                "back"|"quit"|"main")
                    return
                    ;;
            esac
            return
        fi
    fi
    
    # Fallback to original statistics menu if enhanced system unavailable
    statistics_menu_original
}

# Original statistics menu (renamed for fallback)
statistics_menu_original() {
    while true; do
        clear
        echo -e "${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
        echo -e "${GREEN}                         GWOMBAT - Statistics & Metrics                        ${NC}"
        echo -e "${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
        echo ""
        
        # Show current statistics summary at the top
        echo -e "${CYAN}ðŸ“Š Current Statistics Summary:${NC}"
        
        # Quick metrics
        local total_accounts="?"
        local active_accounts="?"
        local suspended_accounts="?"
        local last_update="Never"
        
        if [[ -f "shared-config/menu.db" ]]; then
            total_accounts=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "?")
            active_accounts=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts WHERE current_stage = 'active';" 2>/dev/null || echo "?")
            suspended_accounts=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts WHERE current_stage IN ('recently_suspended', 'pending_deletion', 'temporary_hold', 'exit_row');" 2>/dev/null || echo "?")
            last_update=$(sqlite3 local-config/gwombat.db "SELECT value FROM config WHERE key='last_domain_sync';" 2>/dev/null || echo "Never")
        fi
        
        echo -e "  ${WHITE}Total Accounts:${NC} $total_accounts  |  ${WHITE}Active:${NC} ${GREEN}$active_accounts${NC}  |  ${WHITE}Suspended:${NC} ${YELLOW}$suspended_accounts${NC}"
        echo -e "  ${WHITE}Last Updated:${NC} $last_update"
        echo ""
        
        # Generate dynamic menu from database
        local section_name="statistics_submenu"
        if [[ -f "shared-config/menu.db" ]]; then
            # Load menu items from database (bash 3.2 compatible)
            local menu_items=() function_names=() descriptions=() icons=()
            local counter=1
            
            while IFS='|' read -r name display_name description function_name icon; do
                [[ -n "$name" ]] || continue
                menu_items[$counter]="$display_name"
                function_names[$counter]="$function_name"
                descriptions[$counter]="$description"
                icons[$counter]="$icon"
                ((counter++))
            done < <(sqlite3 shared-config/menu.db "
                SELECT mi.name, mi.display_name, mi.description, mi.function_name, mi.icon
                FROM menu_items mi 
                JOIN menu_sections ms ON mi.section_id = ms.id 
                WHERE ms.name = '$section_name' AND mi.is_active = 1
                ORDER BY mi.item_order;
            " 2>/dev/null)
            
            # Display menu items dynamically from database
            echo -e "${GREEN}=== CORE STATISTICS ===${NC}"
            for i in $(seq 1 5); do
                if [[ -n "${menu_items[$i]}" ]]; then
                    echo "$i. ${icons[$i]} ${menu_items[$i]}"
                    echo "   ${GRAY}${descriptions[$i]}${NC}"
                fi
            done
            echo ""
            echo -e "${PURPLE}=== PERFORMANCE METRICS ===${NC}"
            for i in $(seq 6 8); do
                if [[ -n "${menu_items[$i]}" ]]; then
                    echo "$i. ${icons[$i]} ${menu_items[$i]}"
                    echo "   ${GRAY}${descriptions[$i]}${NC}"
                fi
            done
        else
            # Critical error - database is required
            echo -e "${RED}ERROR: Menu database not found${NC}"
            echo ""
            read -p "Press Enter to return to previous menu..."
            return
        fi
        
        echo ""
        echo "b. â¬…ï¸ Back to Dashboard & Statistics"
        echo "m. ðŸ  Main menu"
        echo "x. âŒ Exit"
        echo ""
        
        read -p "Select an option (1-8, b, m, x): " stats_choice
        echo ""
        
        case $stats_choice in
            [1-8])
                # Use database-driven function dispatcher
                if [[ -f "shared-config/menu.db" ]] && [[ -n "${function_names[$stats_choice]}" ]]; then
                    local func_name="${function_names[$stats_choice]}"
                    statistics_function_dispatcher "$func_name"
                else
                    echo -e "${RED}Invalid option${NC}"
                    read -p "Press Enter to continue..."
                fi
                ;;
            b)
                # Back to dashboard menu
                return
                ;;
            m)
                # Return to main menu
                return
                ;;
            x)
                echo -e "${BLUE}Goodbye!${NC}"
                exit 0
                ;;
            *)
                echo -e "${RED}Invalid option. Please select 1-8, b, m, or x.${NC}"
                read -p "Press Enter to continue..."
                ;;
        esac
    done
}

# Main statistics menu function - uses enhanced version by default
statistics_menu() {
    render_menu "statistics_metrics"
}

# Individual Statistics Functions
domain_overview_statistics() {
    echo -e "${CYAN}ðŸ“Š Domain Overview Statistics${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    echo -e "${WHITE}Domain Summary:${NC}"
    if [[ -x "$GAM" ]]; then
        # Get domain information
        local domain_info=$($GAM info domain 2>/dev/null)
        if [[ -n "$domain_info" ]]; then
            echo "$domain_info" | grep -E "(Primary Domain|User Count|Admin Count)" | sed 's/^/  /'
        else
            echo "  Domain information unavailable"
        fi
        
        echo ""
        echo -e "${WHITE}Account Distribution:${NC}"
        
        # Get user counts by type
        if [[ -f "shared-config/menu.db" ]]; then
            echo "  Account Status Distribution:"
            sqlite3 local-config/gwombat.db "
                SELECT 
                    '    ' || current_stage || ': ' || COUNT(*) || ' accounts'
                FROM accounts 
                GROUP BY current_stage 
                ORDER BY COUNT(*) DESC;
            " 2>/dev/null | head -10
        fi
        
        echo ""
        echo -e "${WHITE}Organizational Units:${NC}"
        # Get OU statistics
        local ou_stats=$($GAM print orgs 2>/dev/null | tail -n +2 | wc -l)
        echo "  Total OUs: $ou_stats"
        
        echo ""
        echo -e "${WHITE}Groups Statistics:${NC}"
        local group_count=$($GAM print groups 2>/dev/null | tail -n +2 | wc -l)
        echo "  Total Groups: $group_count"
        
        # Group membership statistics
        if [[ -f "shared-config/menu.db" ]]; then
            local avg_membership=$(sqlite3 local-config/gwombat.db "
                SELECT ROUND(AVG(member_count), 1) 
                FROM (SELECT COUNT(*) as member_count FROM accounts GROUP BY email);
            " 2>/dev/null || echo "0")
            echo "  Average memberships per user: $avg_membership"
        fi
        
    else
        echo "  GAM not available - cannot retrieve domain statistics"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

user_account_statistics() {
    echo -e "${CYAN}ðŸ‘¥ User Account Statistics${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    if [[ -f "local-config/gwombat.db" ]]; then
        echo -e "${WHITE}Account Lifecycle Statistics:${NC}"
        
        # Detailed stage breakdown
        sqlite3 local-config/gwombat.db "
            SELECT 
                CASE 
                    WHEN current_stage = 'active' THEN 'âœ… Active Accounts'
                    WHEN current_stage = 'recently_suspended' THEN 'âš ï¸ Recently Suspended'
                    WHEN current_stage = 'pending_deletion' THEN 'ðŸ”„ Pending Deletion'
                    WHEN current_stage = 'temporary_hold' THEN 'â¸ï¸ Temporary Hold'
                    WHEN current_stage = 'exit_row' THEN 'ðŸšª Exit Row'
                    ELSE 'â“ ' || current_stage
                END || ': ' || COUNT(*) || ' accounts (' || 
                ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM accounts), 1) || '%)'
            FROM accounts 
            GROUP BY current_stage 
            ORDER BY COUNT(*) DESC;
        " 2>/dev/null | while read -r line; do
            echo "  $line"
        done
        
        echo ""
        echo -e "${WHITE}Account Creation Patterns:${NC}"
        
        # Recent account activity
        local recent_changes=$(sqlite3 local-config/gwombat.db "
            SELECT COUNT(*) FROM stage_history 
            WHERE changed_at > datetime('now', '-30 days');
        " 2>/dev/null || echo "0")
        echo "  Stage changes (last 30 days): $recent_changes"
        
        # Most common stage transitions
        echo ""
        echo -e "${WHITE}Common Stage Transitions (last 90 days):${NC}"
        sqlite3 local-config/gwombat.db "
            SELECT 
                '  ' || from_stage || ' â†’ ' || to_stage || ': ' || COUNT(*) || ' transitions'
            FROM stage_history 
            WHERE changed_at > datetime('now', '-90 days')
            GROUP BY from_stage, to_stage 
            ORDER BY COUNT(*) DESC 
            LIMIT 5;
        " 2>/dev/null
        
    else
        echo "  Database not available - cannot show lifecycle statistics"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

historical_trends_statistics() {
    echo -e "${CYAN}ðŸ“ˆ Historical Trends${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    if [[ -f "local-config/gwombat.db" ]]; then
        echo -e "${WHITE}Account Changes Over Time:${NC}"
        
        # Monthly trends
        echo "  Monthly Stage Changes (last 12 months):"
        sqlite3 local-config/gwombat.db "
            SELECT 
                '    ' || strftime('%Y-%m', changed_at) || ': ' || COUNT(*) || ' changes'
            FROM stage_history 
            WHERE changed_at > datetime('now', '-12 months')
            GROUP BY strftime('%Y-%m', changed_at)
            ORDER BY strftime('%Y-%m', changed_at) DESC 
            LIMIT 12;
        " 2>/dev/null
        
        echo ""
        echo -e "${WHITE}Suspension Trends:${NC}"
        
        # Suspension patterns
        sqlite3 local-config/gwombat.db "
            SELECT 
                '  Suspensions this month: ' || COUNT(*) || ' accounts'
            FROM stage_history 
            WHERE to_stage IN ('recently_suspended', 'pending_deletion') 
            AND changed_at > datetime('now', 'start of month');
        " 2>/dev/null
        
    else
        echo "  Database not available - cannot show historical trends"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

storage_analytics_statistics() {
    echo -e "${CYAN}ðŸ’¾ Storage Analytics${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    if [[ -f "local-config/gwombat.db" ]]; then
        echo -e "${WHITE}Storage Usage Patterns:${NC}"
        
        # Storage statistics if available
        local storage_records=$(sqlite3 local-config/gwombat.db "
            SELECT COUNT(*) FROM account_storage_sizes;
        " 2>/dev/null || echo "0")
        
        if [[ "$storage_records" -gt 0 ]]; then
            echo "  Storage records available: $storage_records accounts"
            
            # Top storage users
            echo ""
            echo -e "${WHITE}Top Storage Users:${NC}"
            sqlite3 local-config/gwombat.db "
                SELECT 
                    '    ' || email || ': ' || ROUND(storage_gb, 2) || ' GB'
                FROM account_storage_sizes 
                ORDER BY storage_gb DESC 
                LIMIT 10;
            " 2>/dev/null
        else
            echo "  No storage data available. Run storage size calculation first."
        fi
        
    else
        echo "  Database not available - cannot show storage analytics"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

group_statistics_analysis() {
    echo -e "${CYAN}ðŸ“‹ Group Statistics${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    if [[ -x "$GAM" ]]; then
        echo -e "${WHITE}Group Analysis:${NC}"
        
        # Basic group count
        local group_count=$($GAM print groups 2>/dev/null | tail -n +2 | wc -l)
        echo "  Total Groups: $group_count"
        
        echo ""
        echo -e "${WHITE}Group Membership Distribution:${NC}"
        
        # Group membership analysis would require more detailed GAM queries
        echo "  (Detailed group membership analysis requires extended GAM queries)"
        echo "  Use 'Group Operations' menu for detailed group management"
        
    else
        echo "  GAM not available - cannot retrieve group statistics"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

system_performance_metrics() {
    echo -e "${CYAN}âš¡ System Performance${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    echo -e "${WHITE}System Response Metrics:${NC}"
    
    # Basic system metrics
    echo "  System uptime: $(uptime | cut -d' ' -f4-5 | sed 's/,//')"
    echo "  Load average: $(uptime | awk -F'load average:' '{print $2}')"
    
    if [[ -f "local-config/gwombat.db" ]]; then
        echo ""
        echo -e "${WHITE}Database Performance:${NC}"
        
        # Database size
        local db_size=$(ls -lh local-config/gwombat.db 2>/dev/null | awk '{print $5}')
        echo "  Database size: $db_size"
        
        # Record counts
        local total_records=$(sqlite3 local-config/gwombat.db "
            SELECT COUNT(*) FROM accounts;
        " 2>/dev/null || echo "0")
        echo "  Total account records: $total_records"
        
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

database_performance_metrics() {
    echo -e "${CYAN}ðŸ“Š Database Performance${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    if [[ -f "local-config/gwombat.db" ]]; then
        echo -e "${WHITE}Database Metrics:${NC}"
        
        # Database file information
        local db_size=$(ls -lh local-config/gwombat.db 2>/dev/null | awk '{print $5}')
        local db_modified=$(ls -l local-config/gwombat.db 2>/dev/null | awk '{print $6, $7, $8}')
        echo "  Database size: $db_size"
        echo "  Last modified: $db_modified"
        
        echo ""
        echo -e "${WHITE}Record Counts:${NC}"
        
        # Table record counts
        local accounts=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "0")
        local history=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM stage_history;" 2>/dev/null || echo "0")
        echo "  Accounts: $accounts"
        echo "  Stage history records: $history"
        
        echo ""
        echo -e "${WHITE}Query Performance:${NC}"
        echo "  (Basic query timing - for detailed analysis use database tools)"
        
    else
        echo "  Database not available - cannot show performance metrics"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

gam_operation_metrics() {
    echo -e "${CYAN}ðŸ”§ GAM Operation Metrics${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    echo -e "${WHITE}GAM Status:${NC}"
    
    if [[ -x "$GAM" ]]; then
        echo "  âœ… GAM Available: $GAM"
        
        # Get GAM version
        local gam_version=$($GAM version 2>/dev/null | head -1 || echo "Unknown")
        echo "  ðŸ“‹ Version: $gam_version"
        
        # Get domain info
        local domain_info=$($GAM info domain 2>/dev/null | grep "Primary Domain:" | cut -d: -f2 | tr -d ' ' || echo "Not configured")
        echo "  ðŸŒ Domain: $domain_info"
        
        echo ""
        echo -e "${WHITE}Operation Metrics:${NC}"
        echo "  (Detailed GAM timing metrics require performance logging)"
        echo "  Use individual GAM operations to test response times"
        
    else
        echo "  âŒ GAM not available at: ${GAM:-not set}"
        echo "  Configure GAM path in local-config/.env"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}


# Function to let user choose between database or fresh GAM data
choose_data_source() {
    local operation_name="$1"
    
    # Check when domain data was last synced
    local last_sync=$(sqlite3 local-config/gwombat.db "SELECT value FROM config WHERE key='last_domain_sync';" 2>/dev/null)
    local db_user_count=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "0")
    
    if [[ -n "$last_sync" && "$db_user_count" -gt 0 ]]; then
        echo ""
        echo -e "${CYAN}ðŸ“Š Database contains $db_user_count accounts${NC}"
        echo -e "${CYAN}ðŸ• Last synced: $last_sync${NC}"
        echo ""
        echo "Data source for $operation_name:"
        echo "1. Use database data (faster, from $last_sync)"
        echo "2. Get fresh data from GAM (slower, current)"
        echo ""
        read -p "Select data source (1-2): " data_source_choice
        
        case $data_source_choice in
            1)
                echo -e "${CYAN}Using database data from $last_sync${NC}"
                return 1  # Use database
                ;;
            2)
                echo -e "${CYAN}Getting fresh data from GAM...${NC}"
                return 0  # Use GAM
                ;;
            *)
                echo -e "${RED}Invalid choice. Using fresh GAM data.${NC}"
                return 0  # Default to GAM
                ;;
        esac
    else
        echo -e "${YELLOW}No database data available. Getting fresh data from GAM...${NC}"
        return 0  # Use GAM
    fi
}

# Function to mark stats as dirty (needs recalculation)
# Call this function whenever you perform operations that could affect user/group/drive counts:
# - Creating/deleting users or groups
# - Suspending/unsuspending users  
# - Moving users between OUs
# - Creating/deleting shared drives
# - Importing account data from CSV
# - Any operation that changes domain membership
mark_stats_dirty() {
    sqlite3 local-config/gwombat.db "
        INSERT OR REPLACE INTO config (key, value) VALUES ('stats_dirty', 'true');
    " 2>/dev/null
}

# Function to force refresh of stats (useful for manual operations)
refresh_stats() {
    mark_stats_dirty
    echo "Stats marked for refresh. They will be recalculated on next main menu display."
}

# Function to sync domain users to database
sync_domain_to_database() {
    if [[ ! -x "$GAM" ]]; then
        echo "  âš ï¸  GAM not available - cannot sync domain data"
        return 1
    fi
    
    echo "  ðŸ“¥ Syncing domain users to database..."
    
    # Get all users from domain with key fields
    local temp_users=$(mktemp)
    execute_gam print users fields primaryemail,suspended,orgunitpath > "$temp_users"
    
    if [[ ! -s "$temp_users" ]]; then
        echo "  âŒ Failed to retrieve domain users"
        rm -f "$temp_users"
        return 1
    fi
    
    # Process each user and update database
    local processed=0
    local updated=0
    
    # Use a different approach to avoid subshell variable scope issues
    while IFS=',' read -r email suspended suspension_reason orgunit; do
        # Clean up fields (remove quotes if present)
        email=$(echo "$email" | tr -d '"')
        suspended=$(echo "$suspended" | tr -d '"')
        orgunit=$(echo "$orgunit" | tr -d '"')
        
        # Skip empty lines
        [[ -z "$email" ]] && continue
        
        # Determine current stage based on OU and suspension status
        local stage="active"  # Default to active
        
        # Only change stage if user is actually suspended
        if [[ "$suspended" == "True" || "$suspended" == "true" ]]; then
            # User is suspended - determine which stage based on OU
            case "$orgunit" in
                *"Pending Deletion"*) stage="pending_deletion" ;;
                *"Temporary Hold"*) stage="temporary_hold" ;;
                *"Exit Row"*) stage="exit_row" ;;
                *"Suspended"*) stage="recently_suspended" ;;
                *) stage="recently_suspended" ;;  # Default for suspended users
            esac
        else
            # User is not suspended - they are active regardless of OU
            stage="active"
        fi
        
        # Insert or update user in database
        sqlite3 local-config/gwombat.db "
            INSERT OR REPLACE INTO accounts (
                email, 
                current_stage, 
                updated_at, 
                ou_path
            ) VALUES (
                '$email', 
                '$stage', 
                datetime('now'), 
                '$orgunit'
            );
        " 2>/dev/null
        
        if [[ $? -eq 0 ]]; then
            ((processed++))
        fi
    done < <(tail -n +2 "$temp_users")
    
    rm -f "$temp_users"
    echo "  âœ… Synced $processed users to database"
    
    # Update last sync timestamp and mark stats as dirty since we've changed account data
    sqlite3 local-config/gwombat.db "
        INSERT OR REPLACE INTO config (key, value) VALUES ('last_domain_sync', datetime('now'));
    " 2>/dev/null
    
    # Mark stats as dirty since we've just updated account data
    mark_stats_dirty
}

# Function to show quick domain statistics
show_quick_stats() {
    echo -e "${CYAN}ðŸ“Š Quick Stats:${NC}"
    echo -e "${GRAY}   Generated: $(date '+%Y-%m-%d %H:%M:%S')${NC}"
    
    # Check if database exists
    if [[ ! -f "local-config/gwombat.db" ]]; then
        echo "  ðŸ” Database not initialized - run setup first"
        echo ""
        return
    fi
    
    # Get database-based statistics (fast and reliable)
    local db_accounts
    local db_suspended
    local db_active
    db_accounts=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "0")
    db_suspended=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts WHERE current_stage IN ('recently_suspended', 'pending_deletion', 'temporary_hold', 'exit_row');" 2>/dev/null || echo "0")
    db_active=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts WHERE current_stage = 'active';" 2>/dev/null || echo "0")
    
    # Try to get cached domain stats from database (GAM-based stats)
    local cached_stats
    cached_stats=$(sqlite3 local-config/gwombat.db "SELECT value FROM config WHERE key='domain_stats_cache'" 2>/dev/null || echo "")
    local cache_timestamp
    cache_timestamp=$(sqlite3 local-config/gwombat.db "SELECT value FROM config WHERE key='domain_stats_timestamp'" 2>/dev/null || echo "0")
    local current_time=$(date +%s)
    
    # Initialize domain counters
    local total_users="?"
    local total_groups="?"
    local shared_drives="?"
    
    # Check if stats need recalculation (dirty flag, no cache, or empty database)
    local stats_dirty
    stats_dirty=$(sqlite3 local-config/gwombat.db "SELECT value FROM config WHERE key='stats_dirty'" 2>/dev/null || echo "true")
    
    # Use cached stats if they exist, stats are not dirty, AND database has data
    if [[ -n "$cached_stats" && "$stats_dirty" != "true" && "$db_accounts" -gt 0 ]]; then
        # Parse cached domain stats
        IFS=',' read -r total_users total_groups shared_drives <<< "$cached_stats"
    else
        # Get fresh domain stats if GAM is available and we don't have recent cache
        if [[ -x "$GAM" ]]; then
            echo "  ðŸ” Updating domain statistics..."
            
            # Sync domain data to database (this populates/updates the accounts table)
            sync_domain_to_database
            
            # Now get counts from database (more reliable than GAM direct counts)
            total_users=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "?")
            db_active=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts WHERE current_stage = 'active';" 2>/dev/null || echo "0")
            db_suspended=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts WHERE current_stage IN ('recently_suspended', 'pending_deletion', 'temporary_hold', 'exit_row');" 2>/dev/null || echo "0")
            db_accounts="$total_users"
            
            # Get groups and shared drives counts (these don't change often, so direct GAM is OK)
            total_groups=$(execute_gam print groups fields email | tail -n +2 | wc -l | tr -d ' ')
            if [[ -z "$total_groups" || "$total_groups" == "0" ]]; then
                total_groups="?"
            fi
            
            # Test Drive API before counting shared drives
            if test_drive_api "true"; then
                shared_drives=$(execute_gam print shareddrives fields id | tail -n +2 | wc -l | tr -d ' ')
                if [[ -z "$shared_drives" || "$shared_drives" == "0" ]]; then
                    shared_drives="0"
                fi
            else
                shared_drives="N/A (Drive API issue)"
            fi
            
            # Cache the results and clear dirty flag (only if we got valid data)
            if [[ "$total_users" != "?" ]]; then
                local cache_data="$total_users,$total_groups,$shared_drives"
                sqlite3 local-config/gwombat.db "
                    INSERT OR REPLACE INTO config (key, value) VALUES ('domain_stats_cache', '$cache_data');
                    INSERT OR REPLACE INTO config (key, value) VALUES ('domain_stats_timestamp', '$current_time');
                    INSERT OR REPLACE INTO config (key, value) VALUES ('stats_dirty', 'false');
                " 2>/dev/null
            fi
        fi
    fi
    
    # Display stats using database counts for accuracy
    echo -e "  ðŸ‘¥ Users: ${BOLD}$total_users${NC} total, ${GREEN}$db_active${NC} active, ${YELLOW}$db_suspended${NC} suspended"
    echo -e "  ðŸ‘¬ Groups: ${BOLD}$total_groups${NC}  |  ðŸ“ Shared Drives: ${BOLD}$shared_drives${NC}"
    echo -e "  ðŸ—„ï¸  Database: ${BOLD}$db_accounts${NC} accounts tracked"
    
    # Show when database was last synced
    local last_sync=$(sqlite3 local-config/gwombat.db "SELECT value FROM config WHERE key='last_domain_sync';" 2>/dev/null)
    if [[ -n "$last_sync" ]]; then
        echo -e "${GRAY}   Database synced: $last_sync${NC}"
    fi
    echo ""
}


# Function to display the main menu
# Main Menu - SQLite-driven implementation
# Primary navigation interface with dynamic menu generation from database
# Uses generate_main_menu() for database-driven display with fallback support
show_main_menu() {
    render_menu "main"
}

# Function to display main menu with quick stats (first load)
show_main_menu_with_stats() {
    clear
    echo -e "${BLUE}=== GWOMBAT - Google Workspace Optimization, Management, Backups And Taskrunner ===${NC}"
    echo ""
    
    # Show domain configuration
    if [[ -n "$DOMAIN" ]]; then
        echo -e "${GREEN}ðŸŒ Domain: ${BOLD}$DOMAIN${NC}"
        if [[ -n "$ADMIN_USER" ]]; then
            echo -e "${GREEN}ðŸ‘¤ Admin: $ADMIN_USER${NC}"
        fi
        echo ""
        
        # Show quick stats if GAM is available
        show_quick_stats
        
    else
        echo -e "${YELLOW}âš ï¸  No domain configured - run Configuration Management to set up${NC}"
        echo ""
    fi
    
    show_main_menu_content
}

# Function to display main menu without quick stats (for reloads)
show_main_menu_no_stats() {
    clear
    echo -e "${BLUE}=== GWOMBAT - Google Workspace Optimization, Management, Backups And Taskrunner ===${NC}"
    echo ""
    
    # Show domain configuration without stats
    if [[ -n "$DOMAIN" ]]; then
        echo -e "${GREEN}ðŸŒ Domain: ${BOLD}$DOMAIN${NC}"
        if [[ -n "$ADMIN_USER" ]]; then
            echo -e "${GREEN}ðŸ‘¤ Admin: $ADMIN_USER${NC}"
        fi
        echo ""
    else
        echo -e "${YELLOW}âš ï¸  No domain configured - run Configuration Management to set up${NC}"
        echo ""
    fi
    
    show_main_menu_content
}

# Common menu content (shared between stats and no-stats versions)
show_main_menu_content() {
    
    echo -e "${YELLOW}Organized by Function Type for Easy Navigation${NC}"
    echo ""
    
    # Generate dynamic menu from database
    if [[ -f "shared-config/menu.db" ]]; then
        # Use SQLite-driven menu generation (bash 3.2 compatible)
        generate_main_menu
    else
        # Critical error - database is required
        echo -e "${RED}ERROR: Menu database not found at shared-config/menu.db${NC}"
        echo ""
        echo "Please run the setup wizard to initialize the database:"
        echo "  ./shared-utilities/setup_wizard.sh"
        echo ""
        echo "x. Exit"
        echo ""
        read -p "Press x to exit: " choice
        exit 1
    fi
    
    echo ""
    read -p "Select an option (1-9, s, i, x): " choice
    echo ""
    
    # Convert letters to numbers for case handling (but keep 'x' as 'x' for exit)
    if [[ "$choice" == "s" || "$choice" == "S" ]]; then
        choice=98  # Search
    elif [[ "$choice" == "i" || "$choice" == "I" ]]; then
        choice=97  # Index
    fi
    
    MENU_CHOICE=$choice
}

# Function to show progress bar
show_progress() {
    local current=$1
    local total=$2
    local description="$3"
    
    if [[ "$PROGRESS_ENABLED" == "true" ]]; then
        local percentage=$((current * 100 / total))
        local filled=$((percentage / 2))
        local bar=""
        
        for ((i=0; i<filled; i++)); do bar+="â–ˆ"; done
        for ((i=filled; i<50; i++)); do bar+="â–‘"; done
        
        printf "\r${CYAN}Progress: [%s] %d%% (%d/%d) %s${NC}" "$bar" "$percentage" "$current" "$total" "$description"
        
        if [[ $current -eq $total ]]; then
            echo ""
        fi
    fi
}

# Function to execute command with dry-run support
execute_command() {
    local command="$1"
    local description="$2"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would execute: $description${NC}"
        echo -e "${CYAN}[DRY-RUN] Command: $command${NC}"
        return 0
    else
        echo -e "${GREEN}Executing: $description${NC}"
        eval "$command"
        return $?
    fi
}

# Function to create backup before changes
create_backup() {
    local user="$1"
    local operation="$2"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would create backup for $user ($operation)${NC}"
        return 0
    fi
    
    local timestamp=$(date '+%Y%m%d_%H%M%S')
    local backup_dir="${SCRIPTPATH}/backups/${timestamp}_${user}_${operation}"
    
    mkdir -p "$backup_dir"
    echo "$user,$operation,$(date '+%Y-%m-%d %H:%M:%S')" >> "$backup_dir/backup_info.txt"
    echo -e "${GREEN}Backup created at: $backup_dir${NC}"
}

# Function for enhanced confirmation with different levels
enhanced_confirm() {
    local operation="$1"
    local user_count="${2:-1}"
    local confirmation_level="${3:-normal}"
    
    echo ""
    case $confirmation_level in
        "high")
            echo -e "${YELLOW}âš ï¸  HIGH RISK OPERATION âš ï¸${NC}"
            echo "This operation will affect $user_count user(s) and could impact many files."
            echo "Type 'CONFIRM' in all caps to proceed:"
            read -p "> " response
            [[ "$response" == "CONFIRM" ]] && return 0 || return 1
            ;;
        "batch")
            if [[ $user_count -gt 10 ]]; then
                echo -e "${YELLOW}âš ï¸  LARGE BATCH OPERATION âš ï¸${NC}"
                echo "You are about to process $user_count users."
                echo "Type 'YES' to proceed:"
                read -p "> " response
                [[ "$response" == "YES" ]] && return 0 || return 1
            else
                return $(confirm_action)
            fi
            ;;
        *)
            return $(confirm_action)
            ;;
    esac
}

# Function to get operation choice
get_operation_choice() {
    echo ""
    echo "Select operation:"
    echo "1. Add temporary hold"
    echo "2. Remove temporary hold"
    echo "3. Mark for pending deletion"
    echo "4. Remove pending deletion"
    echo ""
    while true; do
        read -p "Choose operation (1-4): " op_choice
        case $op_choice in
            1) echo "add_gwombat_hold"; break ;;
            2) echo "remove_gwombat_hold"; break ;;
            3) echo "add_pending"; break ;;
            4) echo "remove_pending"; break ;;
            *) echo -e "${RED}Please select 1, 2, 3, or 4.${NC}" ;;
        esac
    done
}

# Function to validate user exists
validate_user_exists() {
    local user="$1"
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo "true"  # Always valid in dry-run mode
        return 0
    fi
    
    # Check if user exists using GAM
    local user_info=$($GAM info user "$user" 2>&1)
    if echo "$user_info" | grep -q "Does not exist"; then
        echo "false"
        return 1
    else
        echo "true"
        return 0
    fi
}

# Function to get enhanced user status information
get_user_status() {
    local user="$1"
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}Email:${NC} $user"
        echo -e "${CYAN}Name:${NC} Sample User"
        echo -e "${CYAN}Department:${NC} Student (simulated)"
        echo -e "${CYAN}Status:${NC} ${GREEN}Active (simulated)${NC}"
        echo -e "${CYAN}Org Unit:${NC} /${DOMAIN:-yourdomain.edu} (simulated)"
        echo -e "${CYAN}Pending Deletion:${NC} ${GREEN}No (simulated)${NC}"
        echo ""
        echo -e "${CYAN}Group Memberships:${NC}"
        echo "group1@domain.com, group2@domain.com (simulated)"
        echo -e "${CYAN}Total Groups:${NC} 2 (simulated)"
        echo ""
        echo -e "${CYAN}File Analysis:${NC}"
        echo "Estimated file count: 150 (simulated)"
        echo -e "${GREEN}No files with pending deletion marker (simulated)${NC}"
        return 0
    fi
    
    local user_info=$($GAM info user "$user" 2>&1)
    if echo "$user_info" | grep -q "Does not exist"; then
        echo -e "${RED}User does not exist${NC}"
        return 1
    fi
    
    # Extract key information
    local email=$(echo "$user_info" | grep "Email:" | awk -F': ' '{print $2}')
    local firstname=$(echo "$user_info" | grep "First Name:" | awk -F': ' '{print $2}')
    local lastname=$(echo "$user_info" | grep "Last Name:" | awk -F': ' '{print $2}')
    local suspended=$(echo "$user_info" | grep "Account Suspended:" | awk -F': ' '{print $2}')
    local orgunit=$(echo "$user_info" | grep "Org Unit Path:" | awk -F': ' '{print $2}')
    local department=$(echo "$user_info" | grep "Department:" | awk -F': ' '{print $2}')
    local creation=$(echo "$user_info" | grep "Creation Time:" | awk -F': ' '{print $2}')
    
    # Display formatted information
    echo -e "${CYAN}Email:${NC} ${email:-$user}"
    echo -e "${CYAN}Name:${NC} $firstname $lastname"
    echo -e "${CYAN}Department:${NC} ${department:-'Not specified'}"
    echo -e "${CYAN}Created:${NC} ${creation:-'Not specified'}"
    
    # Show suspension status with color
    if [[ "$suspended" == "True" ]]; then
        echo -e "${CYAN}Status:${NC} ${RED}Suspended${NC}"
    else
        echo -e "${CYAN}Status:${NC} ${GREEN}Active${NC}"
    fi
    
    echo -e "${CYAN}Org Unit:${NC} ${orgunit:-'Not specified'}"
    
    # Check for pending deletion marker
    if [[ "$lastname" == *"(PENDING DELETION - CONTACT OIT)"* ]]; then
        echo -e "${CYAN}Pending Deletion:${NC} ${YELLOW}YES - Marked for deletion${NC}"
    else
        echo -e "${CYAN}Pending Deletion:${NC} ${GREEN}No${NC}"
    fi
    
    # Show group memberships
    echo ""
    echo -e "${CYAN}Group Memberships:${NC}"
    local groups=$($GAM print groups member "$user" 2>/dev/null | tail -n +2)
    local group_count=$(echo "$groups" | wc -l)
    
    if [[ -n "$groups" && "$groups" != "" ]]; then
        echo "$groups" | head -5
        if [[ $group_count -gt 5 ]]; then
            echo "... (and $((group_count - 5)) more groups)"
        fi
        echo -e "${CYAN}Total Groups:${NC} $group_count"
    else
        echo "None"
    fi
    
    # Show file count estimate
    echo ""
    echo -e "${CYAN}File Analysis:${NC}"
    local file_count=$($GAM user "$user" show filelist | wc -l 2>/dev/null || echo "0")
    echo "Estimated file count: $file_count"
    
    # Check for pending deletion files
    local pending_files=$($GAM user "$user" show filelist id name 2>/dev/null | grep "(PENDING DELETION - CONTACT OIT)" | wc -l || echo "0")
    if [[ $pending_files -gt 0 ]]; then
        echo -e "${YELLOW}Files with pending deletion marker: $pending_files${NC}"
    else
        echo -e "${GREEN}No files with pending deletion marker${NC}"
    fi
}

# Function to get enhanced user input with validation
get_user_input() {
    while true; do
        read -p "Enter username or email address: " user_input
        if [[ -z "$user_input" ]]; then
            echo -e "${RED}Please enter a valid username or email.${NC}"
            continue
        fi
        
        # Add @${DOMAIN:-yourdomain.edu} if just username provided
        if [[ "$user_input" != *"@"* ]]; then
            user_input="${user_input}@${DOMAIN:-yourdomain.edu}"
            echo "Assuming: $user_input"
        fi
        
        # Validate user exists
        echo "Validating user..."
        if [[ $(validate_user_exists "$user_input") == "true" ]]; then
            # Show user status
            echo ""
            echo -e "${CYAN}=== USER STATUS ===${NC}"
            get_user_status "$user_input"
            echo ""
            
            read -p "Is this the correct user? (y/n): " confirm
            if [[ "$confirm" =~ ^[Yy] ]]; then
                echo "$user_input"
                break
            else
                echo "Please try again."
            fi
        else
            echo -e "${RED}User '$user_input' does not exist. Please try again.${NC}"
        fi
    done
}

# Function to get multiple user input with validation
get_multiple_user_input() {
    echo "Enter usernames/emails (one per line, empty line to finish):"
    local users=()
    local user_input
    
    while true; do
        read -p "> " user_input
        if [[ -z "$user_input" ]]; then
            break
        fi
        
        # Add @${DOMAIN:-yourdomain.edu} if just username provided
        if [[ "$user_input" != *"@"* ]]; then
            user_input="${user_input}@${DOMAIN:-yourdomain.edu}"
        fi
        
        # Validate user exists
        if [[ $(validate_user_exists "$user_input") == "true" ]]; then
            users+=("$user_input")
            echo "âœ“ Added: $user_input"
        else
            echo -e "${RED}âœ— User '$user_input' does not exist. Skipping.${NC}"
        fi
    done
    
    if [[ ${#users[@]} -eq 0 ]]; then
        echo -e "${RED}No valid users entered.${NC}"
        return 1
    fi
    
    echo ""
    echo "Valid users entered: ${#users[@]}"
    for user in "${users[@]}"; do
        echo "  - $user"
    done
    
    # Save to temporary file
    local temp_file="/tmp/bulk_users_$$.txt"
    printf '%s\n' "${users[@]}" > "$temp_file"
    echo "$temp_file"
}

# Function to check prerequisites before operations
check_operation_prerequisites() {
    local user="$1"
    local operation="$2"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        return 0  # Skip checks in dry-run mode
    fi
    
    local user_info=$($GAM info user "$user" 2>/dev/null)
    if [[ -z "$user_info" ]]; then
        echo -e "${RED}Error: User $user does not exist.${NC}"
        return 1
    fi
    
    local suspended=$(echo "$user_info" | awk -F': ' '/Account Suspended:/ {print $2}')
    local lastname=$(echo "$user_info" | awk -F': ' '/Last Name:/ {print $2}')
    local ou=$(echo "$user_info" | awk -F': ' '/Org Unit Path:/ {print $2}')
    
    case $operation in
        "add_pending")
            if [[ "$suspended" != "True" ]]; then
                echo -e "${YELLOW}Warning: User $user is not suspended. Proceed anyway? (y/n)${NC}"
                read -p "> " proceed
                [[ "$proceed" =~ ^[Yy] ]] || return 1
            fi
            if [[ "$lastname" == *"(PENDING DELETION - CONTACT OIT)"* ]]; then
                echo -e "${YELLOW}Warning: User $user already has pending deletion marker. Skip? (y/n)${NC}"
                read -p "> " skip
                [[ "$skip" =~ ^[Yy] ]] && return 2  # Return 2 for skip
            fi
            ;;
        "remove_pending")
            if [[ "$lastname" != *"(PENDING DELETION - CONTACT OIT)"* ]]; then
                echo -e "${YELLOW}Warning: User $user does not have pending deletion marker. Proceed anyway? (y/n)${NC}"
                read -p "> " proceed
                [[ "$proceed" =~ ^[Yy] ]] || return 1
            fi
            ;;
        "add_gwombat_hold")
            if [[ "$lastname" == *"(Suspended Account - Temporary Hold)"* ]]; then
                echo -e "${YELLOW}Warning: User $user already has temporary hold marker. Skip? (y/n)${NC}"
                read -p "> " skip
                [[ "$skip" =~ ^[Yy] ]] && return 2  # Return 2 for skip
            fi
            ;;
        "remove_gwombat_hold")
            if [[ "$lastname" != *"(Suspended Account - Temporary Hold)"* ]]; then
                echo -e "${YELLOW}Warning: User $user does not have temporary hold marker. Proceed anyway? (y/n)${NC}"
                read -p "> " proceed
                [[ "$proceed" =~ ^[Yy] ]] || return 1
            fi
            ;;
    esac
    
    return 0
}

# Function to load users from file
load_users_from_file() {
    while true; do
        read -p "Enter the full path to the file containing usernames: " file_path
        if [[ -f "$file_path" ]]; then
            echo "$file_path"
            break
        else
            echo -e "${RED}File not found. Please enter a valid file path.${NC}"
        fi
    done
}

# Function to reset setup configuration and force setup wizard
reset_setup_configuration() {
    echo -e "${CYAN}ðŸ”„ Reset Setup & Force Full Configuration${NC}"
    echo ""
    echo -e "${YELLOW}âš ï¸  This will completely reset GWOMBAT setup and force the setup wizard to run.${NC}"
    echo ""
    echo "This action will:"
    echo "â€¢ Reset all setup completion flags"
    echo "â€¢ Clear first-time setup markers"  
    echo "â€¢ Force the setup wizard to run from the beginning"
    echo "â€¢ Keep your current configuration files as backup"
    echo ""
    read -p "Are you sure you want to reset setup? (y/N): " confirm_reset
    echo ""
    
    if [[ "$confirm_reset" =~ ^[Yy]$ ]]; then
        echo -e "${CYAN}Resetting setup flags...${NC}"
        
        # Reset setup flags in .env
        if [[ -f "local-config/.env" ]]; then
            sed -i.bak 's/SETUP_COMPLETE="true"/SETUP_COMPLETE="false"/' local-config/.env 2>/dev/null || true
            sed -i.bak 's/SETTINGS_CONFIGURED="true"/SETTINGS_CONFIGURED="false"/' local-config/.env 2>/dev/null || true
            sed -i.bak 's/SKIP_FIRST_TIME_SETUP="true"/SKIP_FIRST_TIME_SETUP="false"/' local-config/.env 2>/dev/null || true
            echo -e "${GREEN}  âœ“ Reset .env flags${NC}"
        fi
        
        # Remove setup completion markers
        rm -f local-config/.setup_complete 2>/dev/null || true
        echo -e "${GREEN}  âœ“ Removed setup completion markers${NC}"
        
        echo ""
        echo -e "${GREEN}âœ… Setup reset complete!${NC}"
        echo ""
        echo "The setup wizard will now run to reconfigure GWOMBAT from the beginning."
        echo ""
        read -p "Press Enter to start setup wizard..."
        
        if [[ -x "./shared-utilities/setup_wizard.sh" ]]; then
            ./shared-utilities/setup_wizard.sh
            exit $?
        else
            echo -e "${RED}Error: Setup wizard not found at ./shared-utilities/setup_wizard.sh${NC}"
            exit 1
        fi
    else
        echo -e "${YELLOW}Reset cancelled.${NC}"
    fi
    echo ""
    read -p "Press Enter to continue..."
}

# Function to show what actions will be performed for adding temporary hold
show_summary() {
    local user=$1
    echo -e "${YELLOW}=== SUMMARY OF ACTIONS FOR: $user ===${NC}"
    echo ""
    echo "The following operations will be performed:"
    echo ""
    echo -e "${GREEN}1. Restore Last Name:${NC}"
    echo "   - Remove '(PENDING DELETION - CONTACT OIT)' from user's last name"
    echo "   - Restore original last name"
    echo ""
    echo -e "${GREEN}2. Fix Filenames:${NC}"
    echo "   - Find all files with '(PENDING DELETION - CONTACT OIT)' in name"
    echo "   - Rename them to include '(Suspended Account - Temporary Hold)'"
    echo "   - Log changes to local-config/tmp/${user}-fixed.txt"
    echo ""
    echo -e "${GREEN}3. Rename Shared Files:${NC}"
    echo "   - Generate file list using list-users-files.sh"
    echo "   - Filter for files shared with active ${DOMAIN:-yourdomain.edu} accounts ONLY"
    echo "   - Add '(Suspended Account - Temporary Hold)' to shared file names"
    echo "   - Skip files already having this suffix or shared externally"
    echo ""
    echo -e "${GREEN}4. Update User Last Name:${NC}"
    echo "   - Add '(Suspended Account - Temporary Hold)' to user's last name"
    echo "   - Skip if already present"
    echo ""
    echo -e "${GREEN}5. Move to Temporary Hold OU:${NC}"
    echo "   - Move user to '$OU_TEMPHOLD' organizational unit"
    echo "   - Remove user from all groups (with backup)"
    echo ""
    echo -e "${GREEN}6. Logging:${NC}"
    echo "   - Add user to gwombat-done.log"
    echo "   - Add timestamp to file-rename-done.txt"
    echo "   - Create group membership backup"
    echo ""
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}ðŸ” DRY-RUN MODE: No actual changes will be made${NC}"
    fi
    echo -e "${YELLOW}Note: This process may take several minutes depending on the number of files.${NC}"
    echo ""
}

# Function to show what actions will be performed for removing temporary hold
show_removal_summary() {
    local user=$1
    echo -e "${YELLOW}=== SUMMARY OF REMOVAL ACTIONS FOR: $user ===${NC}"
    echo ""
    echo "The following operations will be performed:"
    echo ""
    echo -e "${GREEN}1. Remove Temporary Hold from Last Name:${NC}"
    echo "   - Remove '(Suspended Account - Temporary Hold)' from user's last name"
    echo "   - Restore original last name"
    echo ""
    echo -e "${GREEN}2. Remove Temporary Hold from All Files:${NC}"
    echo "   - Find all files with '(Suspended Account - Temporary Hold)' in name"
    echo "   - Remove the suffix from file names"
    echo "   - Log changes to local-config/tmp/${user}-removal.txt"
    echo ""
    echo -e "${GREEN}3. Move User to Destination OU:${NC}"
    echo "   - Choose destination: Pending Deletion, Suspended, or ${DOMAIN:-yourdomain.edu}"
    echo "   - Move user to selected organizational unit"
    echo "   - If moving to ${DOMAIN:-yourdomain.edu}, offer to restore groups from backup"
    echo ""
    echo -e "${GREEN}4. Logging:${NC}"
    echo "   - Add user to gwombat-removed.log"
    echo "   - Add timestamp to file-removal-done.txt"
    echo "   - Log any group restoration activity"
    echo ""
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}ðŸ” DRY-RUN MODE: No actual changes will be made${NC}"
    fi
    echo -e "${YELLOW}Note: This process may take several minutes depending on the number of files.${NC}"
    echo ""
}

# Function to show what actions will be performed for adding pending deletion
show_pending_summary() {
    local user=$1
    echo -e "${YELLOW}=== SUMMARY OF PENDING DELETION ACTIONS FOR: $user ===${NC}"
    echo ""
    echo "The following operations will be performed:"
    echo ""
    echo -e "${GREEN}1. Add Pending Deletion to Last Name:${NC}"
    echo "   - Add '(PENDING DELETION - CONTACT OIT)' to user's last name"
    echo "   - Skip if already present"
    echo ""
    echo -e "${GREEN}2. Add Pending Deletion to All Files:${NC}"
    echo "   - Generate file list using list-users-files.sh"
    echo "   - Add '(PENDING DELETION - CONTACT OIT)' to all file names"
    echo "   - Skip files already having this suffix"
    echo ""
    echo -e "${GREEN}3. Add Drive Labels to Files:${NC}"
    echo "   - Temporarily add Education Plus license"
    echo "   - Add pending deletion labels to all files"
    echo "   - Remove Education Plus license"
    echo ""
    echo -e "${GREEN}4. Remove User from All Groups:${NC}"
    echo "   - Query user's group memberships"
    echo "   - Remove user from all groups"
    echo "   - Log group removals"
    echo ""
    echo -e "${GREEN}5. Move to Pending Deletion OU:${NC}"
    echo "   - Move user to '$OU_PENDING_DELETION' organizational unit"
    echo ""
    echo -e "${GREEN}6. Logging:${NC}"
    echo "   - Add user to pending-deletion-done.log"
    echo "   - Add timestamp to logs"
    echo ""
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}ðŸ” DRY-RUN MODE: No actual changes will be made${NC}"
    fi
    echo -e "${YELLOW}Note: This process may take several minutes depending on the number of files.${NC}"
    echo ""
}

# Function to show what actions will be performed for removing pending deletion
show_pending_removal_summary() {
    local user=$1
    echo -e "${YELLOW}=== SUMMARY OF PENDING DELETION REMOVAL ACTIONS FOR: $user ===${NC}"
    echo ""
    echo "The following operations will be performed:"
    echo ""
    echo -e "${GREEN}1. Remove Pending Deletion from Last Name:${NC}"
    echo "   - Remove '(PENDING DELETION - CONTACT OIT)' from user's last name"
    echo "   - Restore original last name"
    echo ""
    echo -e "${GREEN}2. Remove Pending Deletion from All Files:${NC}"
    echo "   - Find all files with '(PENDING DELETION - CONTACT OIT)' in name"
    echo "   - Remove the suffix from file names"
    echo "   - Remove drive labels from files"
    echo "   - Log changes to local-config/tmp/${user}-pending-removed.txt"
    echo ""
    echo -e "${GREEN}3. Move User to Destination OU:${NC}"
    echo "   - Choose destination: Pending Deletion, Suspended, or ${DOMAIN:-yourdomain.edu}"
    echo "   - Move user to selected organizational unit"
    echo ""
    echo -e "${GREEN}4. Logging:${NC}"
    echo "   - Add user to pending-deletion-removed.log"
    echo "   - Add timestamp to pending-removal-done.txt"
    echo ""
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}ðŸ” DRY-RUN MODE: No actual changes will be made${NC}"
    fi
    echo -e "${YELLOW}Note: This process may take several minutes depending on the number of files.${NC}"
    echo ""
}

# Function to handle dry-run mode
dry_run_mode() {
    DRY_RUN=true
    echo -e "${CYAN}=== DRY-RUN MODE ACTIVATED ===${NC}"
    echo ""
    echo "In dry-run mode, you can:"
    echo "1. Preview changes for a single user"
    echo "2. Preview changes for users from a file" 
    echo "3. Return to main menu"
    echo ""
    read -p "Select an option (1-3): " dry_choice
    
    case $dry_choice in
        1)
            user=$(get_user_input)
            operation=$(get_operation_choice)
            echo ""
            echo -e "${MAGENTA}ðŸ” DRY-RUN PREVIEW FOR: $user${NC}"
            
            case $operation in
                "add_gwombat_hold")
                    show_summary "$user"
                    process_user "$user"
                    ;;
                "remove_gwombat_hold")
                    show_removal_summary "$user"
                    remove_gwombat_hold_user "$user"
                    ;;
                "add_pending")
                    show_pending_summary "$user"
                    process_pending_user "$user"
                    ;;
                "remove_pending")
                    show_pending_removal_summary "$user"
                    remove_pending_user "$user"
                    ;;
            esac
            ;;
        2)
            file_path=$(load_users_from_file)
            user_count=$(wc -l < "$file_path")
            operation=$(get_operation_choice)
            echo ""
            echo -e "${MAGENTA}ðŸ” DRY-RUN PREVIEW FOR $user_count USERS${NC}"
            
            case $operation in
                "add_gwombat_hold")
                    process_users_from_file "$file_path"
                    ;;
                "remove_gwombat_hold")
                    remove_gwombat_hold_users_from_file "$file_path"
                    ;;
                "add_pending")
                    process_pending_users_from_file "$file_path"
                    ;;
                "remove_pending")
                    remove_pending_users_from_file "$file_path"
                    ;;
            esac
            ;;
        3)
            DRY_RUN=false
            return
            ;;
    esac
    
    DRY_RUN=false
    echo ""
    read -p "Press Enter to return to main menu..."
}

# Function to handle discovery mode
# Shared drive cleanup functions
cleanup_shared_drive() {
    local drive_id="$1"
    local dry_run="${2:-false}"
    
    if [[ -z "$drive_id" ]]; then
        echo -e "${RED}Error: Drive ID is required${NC}"
        return 1
    fi
    
    echo -e "${BLUE}=== Shared Drive Cleanup: $drive_id ===${NC}"
    echo ""
    
    # Grant admin user editor access to the shared drive
    local admin_user="${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}}"
    echo -e "${CYAN}Adding admin access to shared drive...${NC}"
    if ! $GAM user "$admin_user" add drivefileacl "$drive_id" user "$admin_user" role editor asadmin 2>/dev/null; then
        echo -e "${RED}Error: Failed to add admin access to shared drive${NC}"
        return 1
    fi
    
    # Create temporary file for file list
    local tempfile=$(mktemp)
    local logfile="local-config/logs/${drive_id}-cleanup.txt"
    
    echo -e "${CYAN}Scanning shared drive for files with pending deletion markers...${NC}"
    
    # Get all files in the shared drive with pending deletion markers
    if ! $GAM user "$admin_user" show filelist select shareddriveid "$drive_id" fields "id,name" > "$tempfile" 2>/dev/null; then
        echo -e "${RED}Error: Failed to retrieve file list from shared drive${NC}"
        rm -f "$tempfile"
        $GAM user "$admin_user" delete drivefileacl "$drive_id" "$admin_user" asadmin 2>/dev/null
        return 1
    fi
    
    # Filter files with pending deletion markers
    local files_with_markers=$(grep -v "Owner,id" "$tempfile" | grep "(PENDING DELETION - CONTACT OIT)" || true)
    local total_files=$(echo "$files_with_markers" | wc -l)
    
    if [[ -z "$files_with_markers" || $total_files -eq 0 ]]; then
        echo -e "${GREEN}No files with pending deletion markers found in this shared drive${NC}"
        rm -f "$tempfile"
        $GAM user "$admin_user" delete drivefileacl "$drive_id" "$admin_user" asadmin 2>/dev/null
        return 0
    fi
    
    echo -e "${YELLOW}Found $total_files files with pending deletion markers${NC}"
    echo ""
    
    if [[ "$dry_run" == "true" ]]; then
        echo -e "${CYAN}=== DRY RUN - Files that would be renamed: ===${NC}"
        local count=0
        echo "$files_with_markers" | while IFS=, read -r owner fileid filename; do
            ((count++))
            local new_filename=${filename//"(PENDING DELETION - CONTACT OIT)"/}
            if [[ "$new_filename" != "$filename" ]]; then
                echo "$count. $filename -> $new_filename"
            fi
        done
    else
        echo -e "${CYAN}Renaming files (removing pending deletion markers)...${NC}"
        
        # Create logs directory if it doesn't exist
        mkdir -p logs
        
        local count=0
        local success_count=0
        local skip_count=0
        
        # Use process substitution to avoid subshell issues
        while IFS=, read -r owner fileid filename; do
            ((count++))
            local new_filename=${filename//"(PENDING DELETION - CONTACT OIT)"/}
            
            if [[ "$new_filename" != "$filename" ]]; then
                echo -e "${CYAN}[$count/$total_files] Processing: $filename${NC}"
                
                # Show GAM command being executed
                echo -e "${YELLOW}Executing: $GAM user \"$owner\" update drivefile \"$fileid\" newfilename \"$new_filename\"${NC}"
                
                if $GAM user "$owner" update drivefile "$fileid" newfilename "$new_filename" 2>/dev/null; then
                    echo -e "${GREEN}Renamed: $filename -> $new_filename${NC}"
                    echo "SUCCESS: $fileid,$filename,$new_filename" >> "$logfile"
                    ((success_count++))
                    
                    # Remove pending deletion label if it exists
                    if [[ -n "$fileid" ]]; then
                        echo -e "${YELLOW}Executing: $GAM user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} process filedrivelabels \"$fileid\" deletelabelfield xIaFm0zxPw8zVL2nVZEI9L7u9eGOz15AZbJRNNEbbFcb 62BB395EC6${NC}"
                        $GAM user "${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}}" process filedrivelabels "$fileid" deletelabelfield xIaFm0zxPw8zVL2nVZEI9L7u9eGOz15AZbJRNNEbbFcb 62BB395EC6 2>/dev/null | grep -q "Deleted" && echo "Label removed" || true
                    fi
                else
                    echo -e "${RED}Failed to rename: $filename${NC}"
                    echo "ERROR: $fileid,$filename,Failed to rename" >> "$logfile"
                fi
            else
                ((skip_count++))
            fi
        done < <(echo "$files_with_markers")
        
        echo ""
        echo -e "${GREEN}Cleanup completed${NC}"
        echo -e "${CYAN}Files processed: $count${NC}"
        echo -e "${GREEN}Successfully renamed: $success_count${NC}"
        echo -e "${YELLOW}Skipped: $skip_count${NC}"
        echo -e "${CYAN}Log file: $logfile${NC}"
    fi
    
    # Clean up
    rm -f "$tempfile"
    echo -e "${CYAN}Removing admin access from shared drive...${NC}"
    $GAM user "$admin_user" delete drivefileacl "$drive_id" "$admin_user" asadmin 2>/dev/null
    
    log_info "Shared drive cleanup completed for $drive_id: $total_files files processed"
}

remove_pending_from_shared_drive() {
    local drive_id="$1"
    local dry_run="${2:-false}"
    
    if [[ -z "$drive_id" ]]; then
        echo -e "${RED}Error: Drive ID is required${NC}"
        return 1
    fi
    
    echo -e "${BLUE}=== Remove Pending Deletion from Shared Drive: $drive_id ===${NC}"
    echo ""
    
    # Use a service account for access
    local service_account="mjb9-ga"
    
    echo -e "${CYAN}Adding service account access to shared drive...${NC}"
    if ! $GAM add drivefileacl "$drive_id" user "$service_account" role organizer asadmin 2>/dev/null; then
        echo -e "${RED}Error: Failed to add service account access${NC}"
        return 1
    fi
    
    # Create temporary file for processing
    local tempfile=$(mktemp)
    
    echo -e "${CYAN}Scanning for files with pending deletion markers...${NC}"
    
    # Get files with pending deletion markers
    if ! $GAM user "$service_account" print filelist select "$drive_id" fields id,title | grep "(PENDING DELETION - CONTACT OIT)" > "$tempfile" 2>/dev/null; then
        echo -e "${YELLOW}No files found with pending deletion markers${NC}"
        rm -f "$tempfile"
        $GAM user "$service_account" delete drivefileacl "$drive_id" "$service_account" 2>/dev/null
        return 0
    fi
    
    local total_files=$(wc -l < "$tempfile")
    echo -e "${YELLOW}Found $total_files files with pending deletion markers${NC}"
    echo ""
    
    if [[ "$dry_run" == "true" ]]; then
        echo -e "${CYAN}=== DRY RUN - Files that would be renamed: ===${NC}"
        awk -F, 'NR>1{print $2 "," substr($0, index($0,$3))}' "$tempfile" | while IFS=, read -r fileid filename; do
            local new_filename=${filename//"(PENDING DELETION - CONTACT OIT)"/}
            if [[ "$new_filename" != "$filename" ]]; then
                echo "Will rename: $filename -> $new_filename"
            fi
        done
    else
        echo -e "${YELLOW}The following files will be renamed:${NC}"
        awk -F, 'NR>1{print $2 "," substr($0, index($0,$3))}' "$tempfile" | while IFS=, read -r fileid filename; do
            local new_filename=${filename//"(PENDING DELETION - CONTACT OIT)"/}
            if [[ "$new_filename" != "$filename" ]]; then
                echo "Will rename: $filename -> $new_filename"
            fi
        done
        
        echo ""
        read -p "Do you wish to proceed with renaming these files? (y/n): " confirm
        
        if [[ "$confirm" == "y" || "$confirm" == "Y" ]]; then
            echo -e "${CYAN}Renaming files...${NC}"
            
            awk -F, 'NR>1{print $2 "," substr($0, index($0,$3))}' "$tempfile" | while IFS=, read -r fileid filename; do
                local new_filename=${filename//"(PENDING DELETION - CONTACT OIT)"/}
                if [[ "$new_filename" != "$filename" ]]; then
                    echo "Debug: File ID: $fileid, Filename: $filename"
                    if $GAM user "$service_account" update drivefile "$fileid" newfilename "$new_filename" 2>/dev/null; then
                        echo -e "${GREEN}Renamed file: $filename -> $new_filename${NC}"
                    else
                        echo -e "${RED}Failed to rename: $filename${NC}"
                    fi
                fi
            done
            
            echo -e "${GREEN}Renaming operation completed${NC}"
        else
            echo -e "${YELLOW}Renaming operation cancelled${NC}"
        fi
    fi
    
    # Clean up
    rm -f "$tempfile"
    echo -e "${CYAN}Revoking service account permissions...${NC}"
    $GAM user "$service_account" delete drivefileacl "$drive_id" "$service_account" 2>/dev/null
    
    echo -e "${GREEN}Operation finished${NC}"
    
    log_info "Remove pending deletion completed for shared drive $drive_id"
}

# Function to get shared drive ID with URL parsing and search options
get_shared_drive_id() {
    local prompt="${1:-Enter shared drive ID or URL}"
    local drive_id=""
    
    while [[ -z "$drive_id" ]]; do
        echo ""
        echo -e "${CYAN}Options:${NC}"
        echo "1. Enter drive ID or paste drive URL"
        echo "2. Search for drive by name" 
        echo "x. Cancel"
        echo ""
        read -p "Select option (1-2, x): " input_option
        
        case $input_option in
            1)
                read -p "$prompt: " user_input
                if [[ -n "$user_input" ]]; then
                    # Check if it's a URL and extract ID
                    if [[ "$user_input" =~ https://drive\.google\.com/drive/folders/([a-zA-Z0-9_-]+) ]]; then
                        drive_id="${BASH_REMATCH[1]}"
                        echo -e "${GREEN}Extracted drive ID from URL: $drive_id${NC}"
                    elif [[ "$user_input" =~ ^[a-zA-Z0-9_-]+$ ]]; then
                        drive_id="$user_input"
                        echo -e "${GREEN}Using drive ID: $drive_id${NC}"
                    else
                        echo -e "${RED}Invalid format. Please enter a valid drive ID or URL.${NC}"
                        read -p "Press Enter to try again..."
                    fi
                fi
                ;;
            2)
                read -p "Enter drive name to search: " search_name
                if [[ -n "$search_name" ]]; then
                    echo -e "${CYAN}Searching for drives containing '$search_name'...${NC}"
                    # Show GAM command being executed
                    echo -e "${YELLOW}Executing: $GAM user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} print shareddrives name query name contains '$search_name'${NC}"
                    
                    local search_results
                    search_results=$($GAM user "${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}}" print shareddrives name query "name contains '$search_name'" 2>/dev/null | tail -n +2)
                    
                    if [[ -n "$search_results" ]]; then
                        echo -e "${GREEN}Found drives:${NC}"
                        echo "$search_results" | nl -w2 -s'. '
                        echo ""
                        read -p "Enter the number of the drive to select (or press Enter to cancel): " selection
                        
                        if [[ "$selection" =~ ^[0-9]+$ ]]; then
                            local selected_line=$(echo "$search_results" | sed -n "${selection}p")
                            if [[ -n "$selected_line" ]]; then
                                drive_id=$(echo "$selected_line" | cut -d',' -f1)
                                local drive_name=$(echo "$selected_line" | cut -d',' -f2)
                                echo -e "${GREEN}Selected: $drive_name (ID: $drive_id)${NC}"
                            else
                                echo -e "${RED}Invalid selection${NC}"
                                read -p "Press Enter to try again..."
                            fi
                        fi
                    else
                        echo -e "${RED}No drives found containing '$search_name'${NC}"
                        read -p "Press Enter to try again..."
                    fi
                fi
                ;;
            x|X)
                return 1  # User cancelled
                ;;
            *)
                echo -e "${RED}Invalid option${NC}"
                read -p "Press Enter to try again..."
                ;;
        esac
    done
    
    echo "$drive_id"
    return 0
}

shared_drive_cleanup_menu() {
    render_menu "file_drive_operations"
}

# License management functions
manage_user_license() {
    local username="$1"
    local action="$2"
    local license_type="${3:-Google Workspace for Education Plus}"
    
    if [[ -z "$username" || -z "$action" ]]; then
        echo -e "${RED}Error: Username and action are required${NC}"
        return 1
    fi
    
    echo -e "${BLUE}=== License Management: $username ===${NC}"
    echo ""
    
    case "$action" in
        "add")
            echo -e "${CYAN}Adding license '$license_type' to $username...${NC}"
            if $GAM user "$username" add license "$license_type" 2>/dev/null; then
                echo -e "${GREEN}Successfully added license '$license_type' to $username${NC}"
                log_info "Added license '$license_type' to user $username"
            else
                echo -e "${RED}Failed to add license '$license_type' to $username${NC}"
                log_error "Failed to add license '$license_type' to user $username"
                return 1
            fi
            ;;
        "remove")
            echo -e "${CYAN}Removing license '$license_type' from $username...${NC}"
            if $GAM user "$username" delete license "$license_type" 2>/dev/null; then
                echo -e "${GREEN}Successfully removed license '$license_type' from $username${NC}"
                log_info "Removed license '$license_type' from user $username"
            else
                echo -e "${RED}Failed to remove license '$license_type' from $username${NC}"
                log_error "Failed to remove license '$license_type' from user $username"
                return 1
            fi
            ;;
        "show")
            echo -e "${CYAN}Current licenses for $username:${NC}"
            $GAM user "$username" print licenses 2>/dev/null || echo -e "${RED}Failed to retrieve licenses for $username${NC}"
            ;;
        *)
            echo -e "${RED}Invalid action: $action. Use 'add', 'remove', or 'show'${NC}"
            return 1
            ;;
    esac
}

license_management_menu() {
    render_menu "user_group_management"
}

# Orphaned file collection functions
collect_orphaned_files() {
    local username="$1"
    local target_folder="${2:-Orphans - #user#}"
    local use_shortcuts="${3:-true}"
    
    if [[ -z "$username" ]]; then
        echo -e "${RED}Error: Username is required${NC}"
        return 1
    fi
    
    # Sanitize inputs to prevent command injection
    username=$(sanitize_gam_input "$username")
    target_folder=$(sanitize_gam_input "$target_folder")
    
    if [[ -z "$username" ]]; then
        echo -e "${RED}Error: Username became empty after sanitization${NC}"
        return 1
    fi
    
    echo -e "${BLUE}=== Collecting Orphaned Files for: $username ===${NC}"
    echo ""
    
    # Validate user exists
    if ! $GAM info user "$username" >/dev/null 2>&1; then
        echo -e "${RED}Error: User $username not found${NC}"
        return 1
    fi
    
    echo -e "${CYAN}Collecting orphaned files...${NC}"
    echo "Target folder: $target_folder"
    echo "Use shortcuts: $use_shortcuts"
    echo ""
    
    local gam_command="$GAM user \"$username\" collect orphans targetuserfoldername \"$target_folder\""
    if [[ "$use_shortcuts" == "true" ]]; then
        gam_command="$gam_command useshortcuts"
    fi
    
    echo -e "${YELLOW}Running: $gam_command${NC}"
    echo ""
    
    if eval "$gam_command" 2>&1; then
        echo ""
        echo -e "${GREEN}Successfully collected orphaned files for $username${NC}"
        log_info "Collected orphaned files for user $username into folder '$target_folder'"
    else
        echo ""
        echo -e "${RED}Failed to collect orphaned files for $username${NC}"
        log_error "Failed to collect orphaned files for user $username"
        return 1
    fi
}

orphaned_file_collection_menu() {
    render_menu "file_drive_operations"
}

# File sharing analysis functions
analyze_user_file_sharing() {
    local username="$1"
    local force_mode="${2:-false}"
    local pending_mode="${3:-false}"
    local make_report="${4:-true}"
    
    if [[ -z "$username" ]]; then
        echo -e "${RED}Error: Username is required${NC}"
        return 1
    fi
    
    echo -e "${BLUE}=== File Sharing Analysis: $username ===${NC}"
    echo ""
    
    # Validate user exists
    if ! $GAM info user "$username" >/dev/null 2>&1; then
        echo -e "${RED}Error: User $username not found${NC}"
        return 1
    fi
    
    # Check if user is suspended
    local user_suspended=$($GAM info user "$username" | grep -c "Account Suspended: True" || echo "0")
    if [[ $user_suspended -eq 0 ]] && [[ "$force_mode" != "true" ]]; then
        echo -e "${YELLOW}Warning: User $username is not suspended. Use force mode to proceed anyway.${NC}"
        read -p "Continue anyway? (y/n): " continue_anyway
        [[ "$continue_anyway" != "y" ]] && [[ "$continue_anyway" != "Y" ]] && return 1
    fi
    
    # Create analysis directory structure
    local analysis_dir="listshared"
    local cache_dir="$analysis_dir/cache"
    local temp_dir="$analysis_dir/temp"
    
    mkdir -p "$cache_dir" "$temp_dir"
    
    # Also ensure reports directory exists for output
    mkdir -p "reports"
    
    echo -e "${CYAN}Step 1: Analyzing all files for $username...${NC}"
    
    # Get all files for the user
    local all_files_csv="$analysis_dir/${username}_all_files.csv"
    if [[ "$force_mode" == "true" ]] || [[ ! -f "$all_files_csv" ]]; then
        echo -e "${CYAN}Retrieving complete file list...${NC}"
        if ! $GAM user "$username" print filelist id title mimeType owners.emailAddress size shared webViewLink modifiedTime > "$all_files_csv" 2>/dev/null; then
            echo -e "${RED}Failed to retrieve file list for $username${NC}"
            return 1
        fi
        echo -e "${GREEN}Retrieved $(wc -l < "$all_files_csv") files${NC}"
    else
        echo -e "${YELLOW}Using existing file list ($(wc -l < "$all_files_csv") files)${NC}"
    fi
    
    echo -e "${CYAN}Step 2: Filtering shared files...${NC}"
    
    # Filter to only shared files
    local shared_files_csv="$analysis_dir/${username}_shared_files.csv"
    head -n 1 "$all_files_csv" > "$shared_files_csv"
    awk -F, 'NR>1 && $6=="True" {print}' "$all_files_csv" >> "$shared_files_csv"
    
    local shared_count=$(tail -n +2 "$shared_files_csv" | wc -l)
    echo -e "${GREEN}Found $shared_count shared files${NC}"
    
    if [[ $shared_count -eq 0 ]]; then
        echo -e "${YELLOW}No shared files found for $username${NC}"
        return 0
    fi
    
    echo -e "${CYAN}Step 3: Analyzing file sharing permissions...${NC}"
    
    # Get detailed sharing information
    local shared_with_emails_csv="$analysis_dir/${username}_shared_files_with_emails.csv"
    analyze_file_permissions "$username" "$shared_files_csv" "$shared_with_emails_csv"
    
    echo -e "${CYAN}Step 4: Identifying active recipient accounts...${NC}"
    
    # Check which shared recipients are active
    local active_shares_csv="$analysis_dir/${username}_active-shares.csv"
    identify_active_recipients "$username" "$shared_with_emails_csv" "$active_shares_csv"
    
    local active_count=$(tail -n +2 "$active_shares_csv" | wc -l 2>/dev/null || echo "0")
    echo -e "${GREEN}Found $active_count files shared with active ${DOMAIN:-yourdomain.edu} accounts${NC}"
    
    if [[ $active_count -gt 0 ]]; then
        echo -e "${CYAN}Step 5: Adding file path information...${NC}"
        
        # Add paths to the analysis
        local with_paths_csv="$analysis_dir/${username}_shared-files-with-path.csv"
        add_file_paths "$username" "$active_shares_csv" "$with_paths_csv"
        
        if [[ "$make_report" == "true" ]]; then
            echo -e "${CYAN}Step 6: Generating sharing reports...${NC}"
            generate_sharing_reports "$username" "$with_paths_csv"
        fi
        
        if [[ "$pending_mode" == "true" ]]; then
            echo -e "${CYAN}Step 7: Updating filenames with pending deletion labels...${NC}"
            update_pending_deletion_filenames "$username" "$active_shares_csv"
        fi
    fi
    
    # Store analysis results in database for future reference
    if [[ -f "$active_shares_csv" ]]; then
        local analysis_id="${username}_$(date +%Y%m%d_%H%M%S)"
        echo -e "${CYAN}Storing analysis results in database...${NC}"
        
        # Create table if it doesn't exist
        sqlite3 local-config/gwombat.db "
            CREATE TABLE IF NOT EXISTS sharing_analysis_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                analysis_id TEXT,
                email TEXT,
                total_files INTEGER,
                shared_files INTEGER,
                active_recipients INTEGER,
                csv_files_path TEXT,
                analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        " 2>/dev/null
        
        # Store summary
        sqlite3 local-config/gwombat.db "
            INSERT INTO sharing_analysis_results (
                analysis_id, email, total_files, shared_files, active_recipients, csv_files_path
            ) VALUES (
                '$analysis_id', '$username', 
                $(tail -n +2 "$all_files_csv" 2>/dev/null | wc -l || echo "0"),
                $shared_count, $active_count, '$analysis_dir'
            );
        " 2>/dev/null
        
        echo -e "${GREEN}Analysis results stored in database (ID: $analysis_id)${NC}"
    fi
    
    echo ""
    echo -e "${GREEN}File sharing analysis completed for $username${NC}"
    echo -e "${CYAN}Results saved in: $analysis_dir/${NC}"
    echo -e "${CYAN}- All files: $all_files_csv${NC}"
    echo -e "${CYAN}- Shared files: $shared_files_csv${NC}"
    echo -e "${CYAN}- Active shares: $active_shares_csv${NC}"
    if [[ -f "$with_paths_csv" ]]; then
        echo -e "${CYAN}- With paths: $with_paths_csv${NC}"
    fi
    
    log_info "File sharing analysis completed for $username: $shared_count shared files, $active_count active recipients"
}

# Function to export database analysis results to CSV
export_analysis_to_csv() {
    local analysis_id="$1"
    local output_dir="${2:-exports}"
    
    mkdir -p "$output_dir"
    
    if [[ -z "$analysis_id" ]]; then
        echo -e "${RED}Error: Analysis ID required${NC}"
        echo "Available analyses:"
        sqlite3 local-config/gwombat.db "SELECT analysis_id, email, analyzed_at FROM sharing_analysis_results ORDER BY analyzed_at DESC LIMIT 10;" 2>/dev/null
        return 1
    fi
    
    local export_file="$output_dir/analysis_${analysis_id}.csv"
    
    echo -e "${CYAN}Exporting analysis $analysis_id to CSV...${NC}"
    
    sqlite3 local-config/gwombat.db -header -csv "
        SELECT 
            analysis_id,
            email,
            total_files,
            shared_files,
            active_recipients,
            analyzed_at
        FROM sharing_analysis_results 
        WHERE analysis_id='$analysis_id';
    " > "$export_file" 2>/dev/null
    
    if [[ -f "$export_file" && -s "$export_file" ]]; then
        echo -e "${GREEN}Analysis exported to: $export_file${NC}"
        return 0
    else
        echo -e "${RED}Failed to export analysis or no data found${NC}"
        return 1
    fi
}

# Function to import CSV data into database
import_csv_to_database() {
    local csv_file="$1"
    local import_type="${2:-analysis}"  # analysis, accounts, etc.
    
    if [[ ! -f "$csv_file" ]]; then
        echo -e "${RED}Error: CSV file not found: $csv_file${NC}"
        return 1
    fi
    
    echo -e "${CYAN}Importing $csv_file into database...${NC}"
    
    case "$import_type" in
        "analysis")
            # Import sharing analysis results
            local import_id="import_$(date +%Y%m%d_%H%M%S)"
            local imported=0
            
            tail -n +2 "$csv_file" | while IFS=',' read -r analysis_id email total_files shared_files active_recipients analyzed_at; do
                sqlite3 local-config/gwombat.db "
                    INSERT INTO sharing_analysis_results (
                        analysis_id, email, total_files, shared_files, active_recipients, analyzed_at
                    ) VALUES (
                        '$analysis_id', '$email', '$total_files', '$shared_files', '$active_recipients', '$analyzed_at'
                    );
                " 2>/dev/null && ((imported++))
            done
            
            echo -e "${GREEN}Imported $imported records into database${NC}"
            ;;
        *)
            echo -e "${RED}Unknown import type: $import_type${NC}"
            return 1
            ;;
    esac
}

analyze_file_permissions() {
    local username="$1"
    local shared_files_csv="$2" 
    local output_csv="$3"
    
    # Create header for output file
    echo "owner,id,filename,mimeType,size,webViewLink,modifiedTime,sharedWithEmail" > "$output_csv"
    
    local temp_permissions=$(mktemp)
    local processed=0
    local total=$(tail -n +2 "$shared_files_csv" | wc -l)
    
    # Process each shared file to extract permissions
    tail -n +2 "$shared_files_csv" | while IFS=, read -r owner fileid filename mimetype size shared webviewlink modifiedtime; do
        ((processed++))
        
        if [[ $((processed % 10)) -eq 0 ]]; then
            echo -e "${CYAN}Processing permissions: $processed/$total files...${NC}"
        fi
        
        # Get sharing permissions for this file
        $GAM user "$username" print drivefileacl "$fileid" 2>/dev/null | tail -n +2 | while IFS=, read -r aclid aclrole acltype aclemail aclname acldomain; do
            # Only include user permissions with email addresses
            if [[ "$acltype" == "user" ]] && [[ -n "$aclemail" ]] && [[ "$aclemail" != "$username" ]]; then
                # Clean up filename for CSV
                clean_filename=$(echo "$filename" | tr ',' ';')
                echo "$owner,$fileid,$clean_filename,$mimetype,$size,$webviewlink,$modifiedtime,$aclemail" >> "$output_csv"
            fi
        done
    done
    
    echo -e "${GREEN}Extracted sharing permissions for $total files${NC}"
}

identify_active_recipients() {
    local username="$1"
    local shared_with_emails_csv="$2"
    local output_csv="$3"
    
    if [[ ! -f "$shared_with_emails_csv" ]]; then
        echo -e "${RED}Error: Shared files with emails CSV not found${NC}"
        return 1
    fi
    
    # Extract unique email addresses
    local temp_emails=$(mktemp)
    tail -n +2 "$shared_with_emails_csv" | cut -d, -f8 | grep "@${DOMAIN:-yourdomain.edu}" | sort -u > "$temp_emails"
    
    local total_emails=$(wc -l < "$temp_emails")
    echo -e "${CYAN}Checking suspension status for $total_emails unique email addresses...${NC}"
    
    # Check suspension status for each email
    local active_emails=$(mktemp)
    local processed=0
    
    while read -r email; do
        ((processed++))
        if [[ $((processed % 5)) -eq 0 ]]; then
            echo -e "${CYAN}Checking: $processed/$total_emails emails...${NC}"
        fi
        
        # Check if user exists and is not suspended
        local user_info=$($GAM info user "$email" 2>/dev/null)
        if [[ $? -eq 0 ]] && echo "$user_info" | grep -q "Account Suspended: False"; then
            echo "$email" >> "$active_emails"
        fi
    done < "$temp_emails"
    
    local active_count=$(wc -l < "$active_emails")
    echo -e "${GREEN}Found $active_count active ${DOMAIN:-yourdomain.edu} recipients${NC}"
    
    # Filter shared files to only include those shared with active users
    head -n 1 "$shared_with_emails_csv" > "$output_csv"
    
    while read -r active_email; do
        grep ",$active_email$" "$shared_with_emails_csv" >> "$output_csv"
    done < "$active_emails"
    
    # Clean up temp files
    rm -f "$temp_emails" "$active_emails"
}

add_file_paths() {
    local username="$1"
    local input_csv="$2"
    local output_csv="$3"
    
    if [[ ! -f "$input_csv" ]]; then
        echo -e "${RED}Error: Input CSV not found${NC}"
        return 1
    fi
    
    # Add path column to header
    head -n 1 "$input_csv" | sed 's/$/,path/' > "$output_csv"
    
    local processed=0
    local total=$(tail -n +2 "$input_csv" | wc -l)
    
    echo -e "${CYAN}Adding file paths for $total files...${NC}"
    
    # Process each file to get its path
    tail -n +2 "$input_csv" | while IFS=, read -r owner fileid filename mimetype size webviewlink modifiedtime email; do
        ((processed++))
        
        if [[ $((processed % 5)) -eq 0 ]]; then
            echo -e "${CYAN}Processing paths: $processed/$total files...${NC}"
        fi
        
        # Get file path using GAM
        local file_path=""
        local file_info=$($GAM user "$username" show fileinfo "$fileid" 2>/dev/null)
        
        if [[ $? -eq 0 ]]; then
            # Extract parent folder information and build path
            local parent_id=$(echo "$file_info" | grep "Parent ID" | head -n 1 | cut -d' ' -f3)
            if [[ -n "$parent_id" && "$parent_id" != "None" ]]; then
                file_path=$(build_file_path "$username" "$parent_id")
            else
                file_path="/ (Root)"
            fi
        else
            file_path="Unknown"
        fi
        
        # Clean path for CSV
        clean_path=$(echo "$file_path" | tr ',' ';')
        echo "$owner,$fileid,$filename,$mimetype,$size,$webviewlink,$modifiedtime,$email,$clean_path" >> "$output_csv"
    done
    
    echo -e "${GREEN}Added path information for $total files${NC}"
}

build_file_path() {
    local username="$1"
    local folder_id="$2"
    local cache_dir="listshared/cache"
    local path_cache="$cache_dir/paths_cache.txt"
    
    mkdir -p "$cache_dir"
    
    # Check cache first
    if [[ -f "$path_cache" ]]; then
        local cached_path=$(grep "^$folder_id," "$path_cache" 2>/dev/null | cut -d, -f2-)
        if [[ -n "$cached_path" ]]; then
            echo "$cached_path"
            return
        fi
    fi
    
    # Build path by traversing parents
    local path_components=()
    local current_id="$folder_id"
    local max_depth=20  # Prevent infinite loops
    local depth=0
    
    while [[ -n "$current_id" && "$current_id" != "None" && $depth -lt $max_depth ]]; do
        local folder_info=$($GAM user "$username" show fileinfo "$current_id" 2>/dev/null)
        
        if [[ $? -ne 0 ]]; then
            break
        fi
        
        local folder_name=$(echo "$folder_info" | grep "Title" | head -n 1 | cut -d' ' -f2-)
        local parent_id=$(echo "$folder_info" | grep "Parent ID" | head -n 1 | cut -d' ' -f3)
        
        # Clean up folder name (remove pending deletion markers for path display)
        clean_folder_name=$(echo "$folder_name" | sed 's/ (PENDING DELETION - CONTACT OIT)//g')
        path_components=("$clean_folder_name" "${path_components[@]}")
        
        current_id="$parent_id"
        ((depth++))
    done
    
    # Build final path
    local final_path="/"
    if [[ ${#path_components[@]} -gt 0 ]]; then
        final_path="/${path_components[*]}"
        final_path=${final_path// /\/}  # Replace spaces with slashes
    fi
    
    # Cache the result
    echo "$folder_id,$final_path" >> "$path_cache"
    
    echo "$final_path"
}

generate_sharing_reports() {
    local username="$1"
    local input_csv="$2"
    
    if [[ ! -f "$input_csv" ]]; then
        echo -e "${RED}Error: Input CSV not found for report generation${NC}"
        return 1
    fi
    
    echo -e "${CYAN}Generating sharing reports...${NC}"
    
    # Get user's real name for reports
    local user_info=$($GAM info user "$username" 2>/dev/null)
    local first_name=$(echo "$user_info" | grep "First Name" | cut -d' ' -f3- | tr -d '"')
    local last_name=$(echo "$user_info" | grep "Last Name" | cut -d' ' -f3- | tr -d '"')
    
    [[ -z "$first_name" ]] && first_name="Unknown"
    [[ -z "$last_name" ]] && last_name="User"
    
    # Create report directory
    local report_dir="reports"
    mkdir -p "$report_dir"
    
    # Generate summary report
    local summary_report="$report_dir/${username}_sharing_summary.txt"
    {
        echo "=== FILE SHARING ANALYSIS SUMMARY ==="
        echo "User: $first_name $last_name ($username)"
        echo "Generated: $(date)"
        echo ""
        
        local total_shared=$(tail -n +2 "$input_csv" | wc -l)
        local unique_recipients=$(tail -n +2 "$input_csv" | cut -d, -f8 | sort -u | wc -l)
        
        echo "Total files shared with active ${DOMAIN:-yourdomain.edu} accounts: $total_shared"
        echo "Number of unique active recipients: $unique_recipients"
        echo ""
        
        echo "=== RECIPIENTS ==="
        tail -n +2 "$input_csv" | cut -d, -f8 | sort | uniq -c | sort -nr | while read count email; do
            echo "$email: $count files"
        done
        
        echo ""
        echo "=== FILES BY TYPE ==="
        tail -n +2 "$input_csv" | cut -d, -f4 | sort | uniq -c | sort -nr | while read count mimetype; do
            echo "$mimetype: $count files"
        done
        
    } > "$summary_report"
    
    echo -e "${GREEN}Generated summary report: $summary_report${NC}"
    
    # Generate individual recipient reports
    tail -n +2 "$input_csv" | cut -d, -f8 | sort -u | while read recipient_email; do
        local recipient_report="$report_dir/${recipient_email}_files_from_${username}.csv"
        
        # Create header
        echo "sharerFirstName,sharerLastName,filename,mimeType,size,webViewLink,modifiedTime,sharedwith,path" > "$recipient_report"
        
        # Add files shared with this recipient
        grep ",$recipient_email$" "$input_csv" | while IFS=, read -r owner fileid filename mimetype size webviewlink modifiedtime email path; do
            echo "$first_name,$last_name,$filename,$mimetype,$size,$webviewlink,$modifiedtime,$email,$path" >> "$recipient_report"
        done
        
        local file_count=$(tail -n +2 "$recipient_report" | wc -l)
        echo -e "${CYAN}Generated report for $recipient_email: $file_count files${NC}"
    done
    
    log_info "Generated sharing reports for $username"
}

update_pending_deletion_filenames() {
    local username="$1"
    local active_shares_csv="$2"
    
    if [[ ! -f "$active_shares_csv" ]]; then
        echo -e "${RED}Error: Active shares CSV not found${NC}"
        return 1
    fi
    
    echo -e "${CYAN}Updating filenames with PENDING DELETION labels...${NC}"
    
    local updated_count=0
    local total=$(tail -n +2 "$active_shares_csv" | wc -l)
    
    tail -n +2 "$active_shares_csv" | while IFS=, read -r owner fileid filename mimetype size webviewlink modifiedtime email; do
        # Check if filename already has pending deletion marker
        if [[ "$filename" != *"(PENDING DELETION - CONTACT OIT)"* ]]; then
            local new_filename="$filename (PENDING DELETION - CONTACT OIT)"
            
            if $GAM user "$username" update drivefile "$fileid" newfilename "$new_filename" 2>/dev/null; then
                echo -e "${GREEN}Updated: $filename${NC}"
                ((updated_count++))
            else
                echo -e "${RED}Failed to update: $filename${NC}"
            fi
        fi
    done
    
    echo -e "${GREEN}Updated $updated_count of $total filenames${NC}"
    log_info "Updated $updated_count filenames with pending deletion labels for $username"
}

generate_recipient_report() {
    local recipient_email="$1"
    
    if [[ -z "$recipient_email" ]]; then
        echo -e "${RED}Error: Recipient email is required${NC}"
        return 1
    fi
    
    echo -e "${BLUE}=== Generating Report for Recipient: $recipient_email ===${NC}"
    echo ""
    
    # Check if recipient exists and is active
    if ! $GAM info user "$recipient_email" >/dev/null 2>&1; then
        echo -e "${RED}Error: Recipient $recipient_email not found${NC}"
        return 1
    fi
    
    local recipient_suspended=$($GAM info user "$recipient_email" | grep -c "Account Suspended: True" || echo "0")
    if [[ $recipient_suspended -gt 0 ]]; then
        echo -e "${YELLOW}Warning: Recipient $recipient_email is suspended${NC}"
    fi
    
    # Search through existing analysis files
    local report_files=()
    for report in local-config/reports/*_files_from_*.csv; do
        if [[ -f "$report" ]] && [[ "$report" == *"${recipient_email}_files_from_"* ]]; then
            report_files+=("$report")
        fi
    done
    
    if [[ ${#report_files[@]} -eq 0 ]]; then
        echo -e "${YELLOW}No sharing reports found for $recipient_email${NC}"
        echo -e "${CYAN}You may need to run file sharing analysis for suspended users first${NC}"
        return 0
    fi
    
    # Combine all reports for this recipient
    local combined_report="local-config/reports/${recipient_email}_combined_pending_files.csv"
    local temp_combined=$(mktemp)
    
    echo "sharerFirstName,sharerLastName,filename,mimeType,size,webViewLink,modifiedTime,sharedwith,path" > "$combined_report"
    
    local total_files=0
    local total_sharers=0
    
    for report_file in "${report_files[@]}"; do
        if [[ -f "$report_file" ]]; then
            tail -n +2 "$report_file" >> "$temp_combined"
            ((total_sharers++))
        fi
    done
    
    # Sort by sharer name and add to final report
    sort "$temp_combined" >> "$combined_report"
    total_files=$(tail -n +2 "$combined_report" | wc -l)
    
    rm -f "$temp_combined"
    
    echo -e "${GREEN}Generated combined report: $combined_report${NC}"
    echo -e "${CYAN}Total files shared with $recipient_email: $total_files${NC}"
    echo -e "${CYAN}Number of different sharers: $total_sharers${NC}"
    
    # Generate summary
    local summary_file="local-config/reports/${recipient_email}_summary.txt"
    {
        echo "=== PENDING DELETION FILES SHARED WITH $recipient_email ==="
        echo "Generated: $(date)"
        echo ""
        echo "Total files: $total_files"
        echo "Number of different sharers: $total_sharers"
        echo ""
        echo "=== FILES BY SHARER ==="
        tail -n +2 "$combined_report" | cut -d, -f1,2 | sort | uniq -c | sort -nr | while read count first_last; do
            echo "$first_last: $count files"
        done
        echo ""
        echo "=== FILES BY TYPE ==="
        tail -n +2 "$combined_report" | cut -d, -f4 | sort | uniq -c | sort -nr | while read count mimetype; do
            echo "$mimetype: $count files"
        done
    } > "$summary_file"
    
    echo -e "${GREEN}Generated summary: $summary_file${NC}"
    
    log_info "Generated recipient report for $recipient_email: $total_files files from $total_sharers sharers"
}

file_sharing_analysis_menu() {
    render_menu "analysis_discovery"
}

discovery_mode() {
    DISCOVERY_MODE=true
    echo -e "${MAGENTA}=== DISCOVERY MODE ===${NC}"
    echo ""
    echo "Discovery options:"
    echo "1. Query users in Temporary Hold OU"
    echo "2. Query users in Pending Deletion OU"
    echo "3. Query all suspended users (all OUs)"
    echo "4. Scan active accounts for orphaned pending deletion files"
    echo "5. Query users by department/type"
    echo "6. Diagnose specific account consistency"
    echo "7. Check for incomplete operations"
    echo "8. Shared Drive cleanup operations"
    echo "9. License management operations"
    echo "10. Orphaned file collection"
    echo "11. File sharing analysis and reports"
    echo ""
    echo "12. Return to main menu"
    echo "m. Main menu"
    echo "x. Exit"
    echo ""
    read -p "Select an option (1-12, m, x): " discovery_choice
    
    case $discovery_choice in
        1) 
            query_gwombat_hold_users
            ;;
        2) 
            query_pending_users
            ;;
        3) 
            query_all_suspended_users
            ;;
        4) 
            scan_active_accounts
            ;;
        5) 
            query_users_by_filter
            ;;
        6) 
            user=$(get_user_input)
            diagnose_account "$user"
            ;;
        7) 
            check_incomplete_operations
            ;;
        8) 
            shared_drive_cleanup_menu
            ;;
        9) 
            license_management_menu
            ;;
        10) 
            orphaned_file_collection_menu
            ;;
        11) 
            file_sharing_analysis_menu
            ;;
        12) 
            DISCOVERY_MODE=false
            return
            ;;
        m|M)
            DISCOVERY_MODE=false
            return
            ;;
        x|X)
            exit 0
            ;;
        *)
            echo -e "${RED}Invalid option. Please select 1-12, m, or x.${NC}"
            echo ""
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    DISCOVERY_MODE=false
    echo ""
    read -p "Press Enter to return to main menu..."
}

# Function to check for incomplete operations
check_incomplete_operations() {
    echo -e "${YELLOW}Checking for incomplete operations...${NC}"
    
    # Check for partial log entries
    if [[ -f "${SCRIPTPATH}/gwombat-done.log" ]]; then
        echo "Users in gwombat-done.log: $(wc -l < "${SCRIPTPATH}/gwombat-done.log")"
    fi
    
    if [[ -f "${SCRIPTPATH}/gwombat-removed.log" ]]; then
        echo "Users in gwombat-removed.log: $(wc -l < "${SCRIPTPATH}/gwombat-removed.log")"
    fi
    
    # Check for orphaned tmp files
    if [[ -d "${SCRIPTPATH}/local-config/tmp" ]]; then
        tmp_files=$(find "${SCRIPTPATH}/local-config/tmp" -name "*-fixed.txt" -o -name "*-removal.txt" | wc -l)
        echo "Temporary operation files found: $tmp_files"
        
        if [[ $tmp_files -gt 0 ]]; then
            echo ""
            echo "Recent operation files:"
            find "${SCRIPTPATH}/tmp" -name "*-fixed.txt" -o -name "*-removal.txt" -exec ls -la {} \; | head -5
        fi
    fi
}

# Function to resume failed operations
resume_failed_operations() {
    echo -e "${YELLOW}Resume functionality - Check for failed operations...${NC}"
    echo "This feature would analyze log files and resume incomplete batch operations."
    echo "(Implementation would require analyzing specific failure points)"
}

# Function to generate comprehensive file list for a user
generate_user_file_list() {
    local user_email="$1"
    local csv_dir="${SCRIPTPATH}/csv-files"
    local output_file="${csv_dir}/${user_email}_shared-files-with-path.csv"
    
    # Create CSV directory if it doesn't exist
    mkdir -p "$csv_dir"
    
    echo "Generating comprehensive file list for $user_email..."
    
    # Get all files owned by the user with sharing details
    $GAM user "$user_email" print filelist \
        fields id,name,owners,permissions,mimeType,size,webViewLink,modifiedTime \
        showownedby me \
        > "${csv_dir}/${user_email}_all_files.csv"
    
    # Create the shared files report with path information
    echo "owner,filename,id,mimeType,size,webViewLink,modifiedTime,sharedwith,path" > "$output_file"
    
    # Process files and add path information
    local counter=0
    local total_files=$(tail -n +2 "${csv_dir}/${user_email}_all_files.csv" | wc -l)
    
    tail -n +2 "${csv_dir}/${user_email}_all_files.csv" | while IFS=',' read -r id name owners permissions mimeType size webViewLink modifiedTime; do
        ((counter++))
        show_progress $counter $total_files "Processing file $counter"
        
        # Check if file has external sharing
        if [[ "$permissions" == *"@${DOMAIN:-yourdomain.edu}"* ]] || [[ "$permissions" == *"anyone"* ]]; then
            # Get path information using build_file_path function
            local path=$(build_file_path "$id")
            echo "$owners,$name,$id,$mimeType,$size,$webViewLink,$modifiedTime,$permissions,$path" >> "$output_file"
        fi
    done
    
    echo "Generated file list at: $output_file"
}

# Function to build file path from Google Drive API
build_file_path() {
    local file_id="$1"
    local path_cache="${SCRIPTPATH}/cache/paths"
    local cache_file="${path_cache}/${file_id}.path"
    
    # Create cache directory
    mkdir -p "$path_cache"
    
    # Check if path is cached
    if [[ -f "$cache_file" ]]; then
        cat "$cache_file"
        return
    fi
    
    # Build path by traversing parent hierarchy
    local current_id="$file_id"
    local path_components=()
    
    while [[ -n "$current_id" && "$current_id" != "root" ]]; do
        # Get file name and parent
        local file_info=$($GAM user "${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}}" show fileinfo "$current_id" fields name,parents 2>/dev/null)
        local name=$(echo "$file_info" | grep "name:" | cut -d' ' -f2-)
        local parent=$(echo "$file_info" | grep "parents:" | cut -d' ' -f2)
        
        if [[ -n "$name" ]]; then
            path_components=("$name" "${path_components[@]}")
        fi
        
        current_id="$parent"
        
        # Prevent infinite loops
        [[ ${#path_components[@]} -gt 20 ]] && break
    done
    
    # Join path components
    local full_path=$(IFS='/'; echo "${path_components[*]}")
    
    # Cache the result
    echo "$full_path" > "$cache_file"
    echo "$full_path"
}

# Function to identify active recipients of shared files
identify_active_recipients() {
    local user_email="$1"
    local csv_dir="${SCRIPTPATH}/csv-files"
    local input_file="${csv_dir}/${user_email}_shared-files-with-path.csv"
    local output_file="${csv_dir}/${user_email}_active-shares.csv"
    
    if [[ ! -f "$input_file" ]]; then
        echo "Error: Input file $input_file not found"
        return 1
    fi
    
    echo "Filtering for files shared with active ${DOMAIN:-yourdomain.edu} accounts..."
    
    # Create header for output file
    head -n 1 "$input_file" > "$output_file"
    
    # Process each line and check if shared with active users
    tail -n +2 "$input_file" | while IFS=',' read -r line; do
        local shared_with=$(echo "$line" | cut -d',' -f8)
        local has_active_share=false
        
        # Extract email addresses from sharing permissions
        local emails=$(echo "$shared_with" | grep -oE '[a-zA-Z0-9._%+-]+@'"${DOMAIN:-your-domain.edu}")
        
        for email in $emails; do
            if [[ "$email" != "$user_email" ]]; then
                # Check if the recipient is active
                local user_status=$($GAM info user "$email" 2>/dev/null | grep "Account Suspended:" | cut -d' ' -f3)
                if [[ "$user_status" != "True" ]]; then
                    has_active_share=true
                    break
                fi
            fi
        done
        
        # Include file if it has active shares
        if [[ "$has_active_share" == true ]]; then
            echo "$line" >> "$output_file"
        fi
    done
    
    local active_count=$(tail -n +2 "$output_file" | wc -l)
    echo "Found $active_count files shared with active ${DOMAIN:-yourdomain.edu} accounts"
}

# Function to analyze user file sharing comprehensively
analyze_user_file_sharing() {
    local user_email="$1"
    local csv_dir="${SCRIPTPATH}/csv-files"
    
    echo -e "${GREEN}Analyzing file sharing for $user_email...${NC}"
    
    # Step 1: Generate comprehensive file list
    generate_user_file_list "$user_email"
    
    # Step 2: Identify files shared with active users only
    identify_active_recipients "$user_email"
    
    # Step 3: Create summary report
    local active_file="${csv_dir}/${user_email}_active-shares.csv"
    local total_shared=$(tail -n +2 "${csv_dir}/${user_email}_shared-files-with-path.csv" 2>/dev/null | wc -l)
    local active_shared=$(tail -n +2 "$active_file" 2>/dev/null | wc -l)
    
    echo -e "${GREEN}File sharing analysis complete:${NC}"
    echo "- Total shared files: $total_shared"
    echo "- Files shared with active users: $active_shared"
    echo "- Report saved to: $active_file"
}

# =====================================
# OWNERSHIP TRANSFER AND ACCOUNT MANAGEMENT FUNCTIONS
# =====================================

# Function to transfer ownership of files to gwombat
transfer_ownership_to_gwombat() {
    local user_email="$1"
    local dry_run="${2:-false}"
    
    echo -e "${GREEN}Transferring file ownership from $user_email to gwombat...${NC}"
    
    # Check if user account is suspended - temporarily unsuspend if needed
    local was_suspended=false
    local user_status=$($GAM info user "$user_email" 2>/dev/null | grep "Account Suspended:" | cut -d' ' -f3)
    
    if [[ "$user_status" == "True" ]]; then
        was_suspended=true
        if [[ "$dry_run" == "false" ]]; then
            echo "User is suspended. Temporarily unsuspending for file transfer..."
            $GAM update user "$user_email" suspended off
            sleep 5
        else
            echo -e "${CYAN}[DRY-RUN] Would temporarily unsuspend $user_email for file transfer${NC}"
        fi
    fi
    
    # Get list of files owned by user
    local temp_file="${SCRIPTPATH}/local-config/tmp/${user_email}_ownership_transfer.csv"
    if [[ "$dry_run" == "false" ]]; then
        $GAM user "$user_email" print filelist fields id,name,owners > "$temp_file"
        local file_count=$(tail -n +2 "$temp_file" | wc -l)
        echo "Found $file_count files to transfer ownership"
        
        local counter=0
        tail -n +2 "$temp_file" | while IFS=',' read -r file_id file_name owner_email; do
            ((counter++))
            show_progress $counter $file_count "Transferring file: $file_name"
            
            # Check if file is owned by external account
            if [[ "$owner_email" != *"@${DOMAIN:-yourdomain.edu}" ]]; then
                echo "  External file detected - copying instead of transferring: $file_name"
                $GAM user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} add drivefile copy "$file_id" parentname "Copied Files from External Accounts"
            else
                $GAM user "$user_email" add drivefileacl "$file_id" user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} role owner transferownership true
            fi
        done
    else
        echo -e "${CYAN}[DRY-RUN] Would transfer ownership of all files from $user_email to gwombat${NC}"
    fi
    
    # Re-suspend user if they were originally suspended
    if [[ "$was_suspended" == true ]]; then
        if [[ "$dry_run" == "false" ]]; then
            echo "Re-suspending user account..."
            $GAM update user "$user_email" suspended on
        else
            echo -e "${CYAN}[DRY-RUN] Would re-suspend $user_email${NC}"
        fi
    fi
    
    echo -e "${GREEN}Ownership transfer completed for $user_email${NC}"
}

# Function to analyze accounts with no sharing
analyze_accounts_no_sharing() {
    local scope="$1"  # "suspended" or "ou"
    
    echo -e "${GREEN}Analyzing accounts with no file sharing...${NC}"
    
    # Check when domain data was last synced
    local last_sync=$(sqlite3 local-config/gwombat.db "SELECT value FROM config WHERE key='last_domain_sync';" 2>/dev/null)
    local db_user_count=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "0")
    
    local user_list=""
    local data_source=""
    
    if [[ -n "$last_sync" && "$db_user_count" -gt 0 ]]; then
        echo ""
        echo -e "${CYAN}ðŸ“Š Database contains $db_user_count accounts${NC}"
        echo -e "${CYAN}ðŸ• Last synced: $last_sync${NC}"
        echo ""
        echo "Data source options:"
        echo "1. Use database data (faster, may be outdated)"
        echo "2. Get fresh data from GAM (slower, current)"
        echo "3. Sync database with GAM first, then use database"
        echo ""
        read -p "Select data source (1-3): " data_source_choice
        
        case $data_source_choice in
            1)
                data_source="database"
                echo -e "${CYAN}Using database data from $last_sync${NC}"
                ;;
            2)
                data_source="gam"
                echo -e "${CYAN}Getting fresh data from GAM...${NC}"
                ;;
            3)
                echo -e "${CYAN}Syncing database with GAM first...${NC}"
                sync_domain_to_database
                data_source="database"
                echo -e "${CYAN}Using freshly synced database data${NC}"
                ;;
            *)
                echo -e "${RED}Invalid choice. Using fresh GAM data.${NC}"
                data_source="gam"
                ;;
        esac
    else
        echo -e "${YELLOW}No database data available. Getting fresh data from GAM...${NC}"
        data_source="gam"
    fi
    
    # Get user list based on chosen data source
    case $data_source in
        "database")
            case $scope in
                "suspended")
                    user_list=$(sqlite3 local-config/gwombat.db "SELECT email FROM accounts WHERE current_stage IN ('recently_suspended', 'pending_deletion', 'temporary_hold', 'exit_row');" 2>/dev/null)
                    ;;
                "ou")
                    user_list=$(sqlite3 local-config/gwombat.db "SELECT email FROM accounts WHERE ou_path LIKE '%Suspended%';" 2>/dev/null)
                    ;;
            esac
            ;;
        "gam")
            case $scope in
                "suspended")
                    user_list=$($GAM print users query "isSuspended=true" fields email 2>/dev/null | tail -n +2 | cut -d',' -f1)
                    ;;
                "ou")
                    user_list=$($GAM print users query "orgUnitPath='/Suspended Accounts'" fields email 2>/dev/null | tail -n +2 | cut -d',' -f1)
                    ;;
            esac
            ;;
    esac
    
    if [[ -z "$user_list" ]]; then
        echo -e "${YELLOW}No suspended accounts found with the selected data source.${NC}"
        if [[ "$data_source" == "database" ]]; then
            echo -e "${CYAN}Try running a fresh GAM sync or use option 2 for fresh data.${NC}"
        fi
        return 1
    fi
    
    local counter=0
    local total_users=$(echo "$user_list" | wc -l)
    local analysis_id=$(date +%Y%m%d_%H%M)
    
    # Create analysis session in database
    sqlite3 local-config/gwombat.db "
        CREATE TABLE IF NOT EXISTS file_sharing_analysis (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            analysis_id TEXT,
            email TEXT,
            has_shared_files BOOLEAN,
            total_files INTEGER,
            storage_used TEXT,
            analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    " 2>/dev/null
    
    echo "$user_list" | while read user_email; do
        [[ -z "$user_email" ]] && continue
        
        ((counter++))
        show_progress $counter $total_users "Analyzing: $user_email"
        
        # Check if user has any shared files
        local shared_files=$($GAM user "$user_email" print filelist query "shared=true" fields id 2>/dev/null | tail -n +2 | wc -l)
        local total_files=$($GAM user "$user_email" print filelist fields id 2>/dev/null | tail -n +2 | wc -l)
        local storage_used=$($GAM info user "$user_email" 2>/dev/null | grep "Storage Used:" | cut -d' ' -f3 || echo "0")
        
        local has_sharing=0
        [[ $shared_files -gt 0 ]] && has_sharing=1
        
        # Store results in database
        sqlite3 local-config/gwombat.db "
            INSERT INTO file_sharing_analysis (
                analysis_id, email, has_shared_files, total_files, storage_used
            ) VALUES (
                '$analysis_id', '$user_email', $has_sharing, $total_files, '$storage_used'
            );
        " 2>/dev/null
    done
    
    # Show summary results
    local total_analyzed=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM file_sharing_analysis WHERE analysis_id='$analysis_id';" 2>/dev/null)
    local no_sharing=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM file_sharing_analysis WHERE analysis_id='$analysis_id' AND has_shared_files=0;" 2>/dev/null)
    local with_sharing=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM file_sharing_analysis WHERE analysis_id='$analysis_id' AND has_shared_files=1;" 2>/dev/null)
    
    echo ""
    echo -e "${GREEN}Analysis complete (ID: $analysis_id):${NC}"
    echo "  ðŸ“Š Total accounts analyzed: $total_analyzed"
    echo "  ðŸ“ Accounts with shared files: $with_sharing"
    echo "  ðŸ—‘ï¸  Candidates for deletion: $no_sharing"
    echo ""
    echo "View detailed results: select * from file_sharing_analysis where analysis_id='$analysis_id';"
}

# Function to perform file activity analysis
analyze_file_activity() {
    local user_email="$1"
    local days_threshold="${2:-90}"
    local csv_dir="${SCRIPTPATH}/csv-files"
    
    echo -e "${GREEN}Analyzing file activity for $user_email (threshold: $days_threshold days)...${NC}"
    
    local all_files_csv="${csv_dir}/${user_email}_files.csv"
    local recent_files_csv="${csv_dir}/${user_email}_recent_files.csv" 
    local old_files_csv="${csv_dir}/${user_email}_old_files.csv"
    
    # Get all files (excluding Google Apps formats)
    $GAM user "$user_email" print filelist query "not mimeType contains 'application/vnd.google-apps'" \
        fields size,id,name,mimeType,modifiedTime > "$all_files_csv"
    
    # Calculate threshold date
    local threshold_date=$(date -d "$days_threshold days ago" +%Y-%m-%d)
    
    # Process files and categorize by date
    echo "size,id,name,mimeType,modifiedTime" > "$recent_files_csv"
    echo "size,id,name,mimeType,modifiedTime" > "$old_files_csv"
    
    local recent_count=0
    local old_count=0
    local recent_size=0
    local old_size=0
    
    tail -n +2 "$all_files_csv" | while IFS=',' read -r size id name mimeType modifiedTime; do
        local file_date=$(echo "$modifiedTime" | cut -d'T' -f1)
        
        if [[ "$file_date" > "$threshold_date" ]]; then
            echo "$size,$id,$name,$mimeType,$modifiedTime" >> "$recent_files_csv"
            ((recent_count++))
            recent_size=$((recent_size + size))
        else
            echo "$size,$id,$name,$mimeType,$modifiedTime" >> "$old_files_csv"
            ((old_count++))
            old_size=$((old_size + size))
        fi
    done
    
    echo -e "${GREEN}File activity analysis complete:${NC}"
    echo "- Recent files (< $days_threshold days): $recent_count files ($(($recent_size / 1024 / 1024)) MB)"
    echo "- Old files (> $days_threshold days): $old_count files ($(($old_size / 1024 / 1024)) MB)"
    echo "- Reports saved to: $csv_dir/"
}

# Function to manage group memberships during suspension
manage_suspension_groups() {
    local user_email="$1"
    local operation="$2"  # "backup" or "restore"
    local groups_file="${SCRIPTPATH}/local-config/tmp/${user_email}_groups_backup.txt"
    
    case $operation in
        "backup")
            echo -e "${GREEN}Backing up group memberships for $user_email...${NC}"
            $GAM info user "$user_email" groups | grep "Member of" | cut -d' ' -f3 > "$groups_file"
            local group_count=$(cat "$groups_file" | wc -l)
            echo "Backed up $group_count group memberships"
            
            # Remove user from all groups
            cat "$groups_file" | while read group; do
                echo "  Removing from $group..."
                $GAM update group "$group" remove member "$user_email"
            done
            ;;
        "restore")
            if [[ -f "$groups_file" ]]; then
                echo -e "${GREEN}Restoring group memberships for $user_email...${NC}"
                cat "$groups_file" | while read group; do
                    echo "  Adding to $group..."
                    $GAM update group "$group" add member "$user_email"
                done
            else
                echo -e "${YELLOW}No backup file found for $user_email${NC}"
            fi
            ;;
    esac
}

# Function to fix file modification dates
restore_file_dates() {
    local user_email="$1"
    local target_date="${2:-2023-05-01}"  # Default to pre-May 2023
    
    echo -e "${GREEN}Restoring file modification dates for $user_email...${NC}"
    
    # Get files that were modified after the target date
    local files_to_fix="${SCRIPTPATH}/local-config/tmp/${user_email}_date_fix.csv"
    $GAM user "$user_email" print filelist \
        query "modifiedTime>'$target_date'" \
        fields id,name,modifiedTime > "$files_to_fix"
    
    local file_count=$(tail -n +2 "$files_to_fix" | wc -l)
    echo "Found $file_count files to fix dates"
    
    local counter=0
    tail -n +2 "$files_to_fix" | while IFS=',' read -r file_id file_name current_date; do
        ((counter++))
        show_progress $counter $file_count "Fixing date: $file_name"
        
        # Try to find appropriate date from file activity
        local activity_date=$($GAM user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} show driveactivity "$file_id" 2>/dev/null | \
                              grep -E "time.*$(date -d "$target_date" +%Y)" | head -1 | \
                              grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2}' || echo "$target_date")
        
        # Update the file with the restored date
        $GAM user "$user_email" update drivefile "$file_id" modifiedtime "$activity_date"
    done
    
    echo -e "${GREEN}Date restoration completed${NC}"
}

# Function to add members to group in bulk
bulk_add_to_group() {
    local group_name="$1"
    local members_file="$2"
    
    if [[ ! -f "$members_file" ]]; then
        echo -e "${RED}Members file not found: $members_file${NC}"
        return 1
    fi
    
    echo -e "${GREEN}Adding members from $members_file to group $group_name...${NC}"
    
    local counter=0
    local total_members=$(cat "$members_file" | wc -l)
    
    while IFS= read -r username || [[ -n "$username" ]]; do
        # Skip empty lines
        [[ -z "$username" ]] && continue
        
        ((counter++))
        show_progress $counter $total_members "Adding: $username"
        
        if $GAM update group "$group_name" add member allmail user "$username" 2>/dev/null; then
            echo "  âœ“ Added $username to $group_name"
        else
            echo "  âœ— Failed to add $username to $group_name"
        fi
    done < "$members_file"
    
    echo -e "${GREEN}Bulk add operation completed${NC}"
}

# Function to remove user from all their groups
remove_user_from_all_groups() {
    local user_email="$1"
    local log_file="${SCRIPTPATH}/local-config/tmp/${user_email}_groups_removed.log"
    
    echo -e "${GREEN}Removing $user_email from all groups...${NC}"
    
    # Get list of groups user belongs to
    local groups=$($GAM print groups member "$user_email" | grep "${DOMAIN:-yourdomain.edu}" | awk '{print $1}')
    local group_count=$(echo "$groups" | wc -l)
    
    if [[ -z "$groups" ]]; then
        echo "User $user_email is not a member of any groups"
        return 0
    fi
    
    echo "Found $group_count groups for removal"
    echo "Groups to remove from: $groups"
    echo ""
    
    local counter=0
    echo "$groups" | while read group; do
        [[ -z "$group" ]] && continue
        
        ((counter++))
        show_progress $counter $group_count "Removing from: $group"
        
        echo "Removing user: $user_email from group: $group"
        if $GAM update group "$group" remove member "$user_email" 2>/dev/null; then
            echo "$user_email removed from $group at $(date)" >> "$log_file"
            echo "  âœ“ Removed from $group"
        else
            echo "  âœ— Failed to remove from $group"
        fi
    done
    
    echo -e "${GREEN}User removed from all groups. Log: $log_file${NC}"
}

# Function to manage shared drives operations
shared_drive_operations() {
    local operation="$1"
    local drive_id="$2"
    local user_email="$3"
    
    case $operation in
        "remove_pending_labels")
            echo -e "${GREEN}Removing pending deletion labels from shared drive...${NC}"
            $GAM user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} print filelist query "parents in '$drive_id'" \
                fields id,name | tail -n +2 | while IFS=',' read -r file_id file_name; do
                if [[ "$file_name" == *"PENDING DELETION"* ]] || [[ "$file_name" == *"Suspended Account - Temporary Hold"* ]]; then
                    local new_name=$(echo "$file_name" | sed -E 's/ \(PENDING DELETION - CONTACT OIT\)//g' | \
                                    sed -E 's/ \(Suspended Account - Temporary Hold\)//g')
                    echo "  Cleaning: $file_name -> $new_name"
                    $GAM user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} update drivefile "$file_id" name "$new_name"
                fi
            done
            ;;
        "grant_admin_access")
            echo -e "${GREEN}Granting gamadmin access to all files in shared drive...${NC}"
            $GAM user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} print filelist query "parents in '$drive_id'" \
                fields id | tail -n +2 | while read file_id; do
                $GAM user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} add drivefileacl "$file_id" user ${ADMIN_USER:-gwombat@${DOMAIN:-yourdomain.edu}} role writer
            done
            ;;
        "create_user_drive")
            echo -e "${GREEN}Creating shared drive for user: $user_email${NC}"
            local drive_name="${user_email} - Archived Files"
            echo "Command: $GAM_PATH create shareddrive \"$drive_name\" adminmanagedrestrictions true"
            
            # Create the shared drive with proper GAM7 syntax
            local drive_output=$($GAM_PATH create shareddrive "$drive_name" adminmanagedrestrictions true 2>&1)
            local exit_code=$?
            
            if [[ $exit_code -eq 0 ]]; then
                # Extract drive ID from GAM output (format: User,name,id)
                local new_drive_id=$(echo "$drive_output" | tail -1 | cut -d',' -f3 | tr -d '"')
                
                if [[ -n "$new_drive_id" && "$new_drive_id" != "id" ]]; then
                    echo "Created shared drive: $new_drive_id"
                    
                    # Wait for drive to be ready (GAM requirement: 30+ seconds)
                    echo "Waiting for shared drive to be ready for updates..."
                    sleep 35
                    
                    # Grant access to admin user
                    echo "Command: $GAM_PATH update shareddrive \"$new_drive_id\" add organizer ${ADMIN_USER}"
                    if $GAM_PATH update shareddrive "$new_drive_id" add organizer "${ADMIN_USER}" 2>/dev/null; then
                        echo "Successfully added ${ADMIN_USER} as organizer"
                    else
                        echo -e "${YELLOW}Warning: Could not add ${ADMIN_USER} as organizer. Drive created but manual permission assignment may be needed.${NC}"
                    fi
                    
                    echo "$new_drive_id"
                else
                    echo -e "${RED}Error: Could not extract drive ID from GAM output${NC}"
                    echo "GAM output: $drive_output"
                    echo ""
                fi
            else
                echo -e "${RED}Error creating shared drive${NC}"
                echo "GAM output: $drive_output"
                echo ""
            fi
            ;;
    esac
}

# Function to get destination OU choice
get_destination_ou() {
    echo ""
    echo "Select destination Organizational Unit:"
    echo "1. Suspended Accounts/Suspended - Pending Deletion"
    echo "2. Suspended Accounts (general suspended)"
    echo "3. ${DOMAIN:-yourdomain.edu} (reactivate account)"
    echo ""
    while true; do
        read -p "Choose destination OU (1-3): " ou_choice
        case $ou_choice in
            1) echo "$OU_PENDING_DELETION"; break ;;
            2) echo "$OU_SUSPENDED"; break ;;
            3) echo "$OU_ACTIVE"; break ;;
            *) echo -e "${RED}Please select 1, 2, or 3.${NC}" ;;
        esac
    done
}

# Function to move user to OU
move_user_to_ou() {
    local user="$1"
    local target_ou="$2"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would move user $user to OU: $target_ou${NC}"
        
        # Check if this is a suspension operation that would trigger group removal
        if [[ "$target_ou" == *"Suspended"* ]]; then
            echo -e "${CYAN}[DRY-RUN] Would also remove user from all groups${NC}"
        fi
        return 0
    fi
    
    echo -e "${GREEN}Moving user $user to OU: $target_ou${NC}"
    execute_command "$GAM update user \"$user\" ou \"$target_ou\"" "Move user to OU"
    
    # Automatically remove from groups when moving to any suspended OU
    if [[ "$target_ou" == *"Suspended"* ]]; then
        echo -e "${YELLOW}User is being moved to a suspended OU. Removing from all groups...${NC}"
        remove_user_from_all_groups "$user"
    fi
    
    # Offer to restore groups when moving back to active OU
    if [[ "$target_ou" == "$OU_ACTIVE" ]]; then
        echo -e "${CYAN}User is being reactivated. Checking for group backup...${NC}"
        # Look for the most recent group backup for this user
        local latest_backup=$(ls -t "${BACKUP_DIR}/${user}-groups-"*.txt 2>/dev/null | head -1)
        if [[ -n "$latest_backup" ]]; then
            echo -e "${YELLOW}Found group backup: $(basename "$latest_backup")${NC}"
            echo -e "${YELLOW}Would you like to restore the user's previous group memberships? (y/n)${NC}"
            read -p "> " restore_groups
            if [[ "$restore_groups" =~ ^[Yy] ]]; then
                restore_user_to_groups "$user" "$latest_backup"
            else
                echo -e "${CYAN}Skipped group restoration. Groups can be manually restored later.${NC}"
                log_info "User chose to skip group restoration for $user"
            fi
        else
            echo -e "${YELLOW}No group backup found for user $user${NC}"
            echo -e "${YELLOW}Groups will need to be manually restored if needed.${NC}"
            log_warning "No group backup found for reactivated user $user"
        fi
    fi
}

# Function to remove user from all groups
remove_user_from_all_groups() {
    local user="$1"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would remove $user from all groups${NC}"
        return 0
    fi
    
    log_info "Removing user $user from all groups"
    echo -e "${CYAN}Fetching group memberships for $user...${NC}"
    
    # Get all groups the user is a member of
    local groups=$($GAM print groups member "$user" 2>/dev/null | tail -n +2 | grep ${DOMAIN:-yourdomain.edu} || true)
    
    if [[ -z "$groups" ]]; then
        echo -e "${GREEN}User $user is not a member of any groups${NC}"
        log_info "User $user has no group memberships to remove"
        return 0
    fi
    
    local group_count=$(echo "$groups" | wc -l)
    echo -e "${YELLOW}Removing user from $group_count groups...${NC}"
    
    local removed_count=0
    local failed_count=0
    
    # Create backup of group memberships
    local group_backup_file="${BACKUP_DIR}/${user}-groups-$(date +%Y%m%d_%H%M%S).txt"
    echo "$groups" > "$group_backup_file"
    log_info "Group membership backup created: $group_backup_file"
    
    while IFS= read -r group; do
        if [[ -n "$group" ]]; then
            echo -n "  Removing from $group... "
            if $GAM update group "$group" remove member "$user" >/dev/null 2>&1; then
                echo -e "${GREEN}âœ“${NC}"
                ((removed_count++))
                log_operation "remove_from_group" "$user" "SUCCESS" "Removed from group: $group"
                
                # Log to the same file format as the original script
                echo "$(date '+%Y-%m-%d %H:%M:%S'),$user,$group" >> "${SCRIPTPATH}/users-removed-from-groups.txt"
            else
                echo -e "${RED}âœ—${NC}"
                ((failed_count++))
                log_operation "remove_from_group" "$user" "ERROR" "Failed to remove from group: $group"
                log_error "Failed to remove user $user from group $group"
            fi
        fi
    done <<< "$groups"
    
    echo ""
    if [[ $removed_count -gt 0 ]]; then
        echo -e "${GREEN}Successfully removed user from $removed_count groups${NC}"
        log_info "Successfully removed user $user from $removed_count groups"
    fi
    
    if [[ $failed_count -gt 0 ]]; then
        echo -e "${YELLOW}Failed to remove user from $failed_count groups${NC}"
        log_warning "Failed to remove user $user from $failed_count groups"
    fi
    
    echo -e "${CYAN}Group removal log: ${SCRIPTPATH}/users-removed-from-groups.txt${NC}"
}

# Function to restore user to groups (for reactivation)
restore_user_to_groups() {
    local user="$1"
    local backup_file="$2"
    
    if [[ ! -f "$backup_file" ]]; then
        echo -e "${YELLOW}No group backup file found: $backup_file${NC}"
        echo -e "${YELLOW}Skipping group restoration. You may need to manually restore group memberships.${NC}"
        log_warning "No group backup found for user $user - manual group restoration may be needed"
        return 1
    fi
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would restore user $user to groups from backup: $backup_file${NC}"
        return 0
    fi
    
    echo -e "${CYAN}Restoring user $user to groups from backup...${NC}"
    log_info "Restoring user $user to groups from backup: $backup_file"
    
    local restored_count=0
    local failed_count=0
    
    while IFS= read -r group; do
        if [[ -n "$group" ]]; then
            echo -n "  Adding to $group... "
            if $GAM update group "$group" add member "$user" >/dev/null 2>&1; then
                echo -e "${GREEN}âœ“${NC}"
                ((restored_count++))
                log_operation "add_to_group" "$user" "SUCCESS" "Restored to group: $group"
            else
                echo -e "${RED}âœ—${NC}"
                ((failed_count++))
                log_operation "add_to_group" "$user" "ERROR" "Failed to restore to group: $group"
                log_error "Failed to restore user $user to group $group"
            fi
        fi
    done < "$backup_file"
    
    echo ""
    if [[ $restored_count -gt 0 ]]; then
        echo -e "${GREEN}Successfully restored user to $restored_count groups${NC}"
        log_info "Successfully restored user $user to $restored_count groups"
    fi
    
    if [[ $failed_count -gt 0 ]]; then
        echo -e "${YELLOW}Failed to restore user to $failed_count groups${NC}"
        log_warning "Failed to restore user $user to $failed_count groups"
    fi
}

# Function to get user's current OU
get_user_ou() {
    local user="$1"
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo "$OU_TEMPHOLD"  # Simulate for dry-run/discovery
        return 0
    fi
    
    local ou=$($GAM info user "$user" | awk -F': ' '/Org Unit Path:/ {print $2}')
    echo "$ou"
}

# Function to query users in temporary hold OU
query_gwombat_hold_users() {
    echo -e "${CYAN}Querying users in Temporary Hold OU...${NC}"
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}[DISCOVERY] Would query: $GAM print users ou \"$OU_TEMPHOLD\"${NC}"
        echo ""
        echo "Simulated results:"
        echo "user1@domain.com,John,Doe (Suspended Account - Temporary Hold)"
        echo "user2@domain.com,Jane,Smith (Suspended Account - Temporary Hold)"
        echo "user3@domain.com,Bob,Johnson"
        return 0
    fi
    
    echo "Users in $OU_TEMPHOLD:"
    $GAM print users ou "$OU_TEMPHOLD" firstname lastname
}

# Function to query users in pending deletion OU
query_pending_users() {
    echo -e "${CYAN}Querying users in Pending Deletion OU...${NC}"
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}[DISCOVERY] Would query: $GAM print users ou \"$OU_PENDING_DELETION\"${NC}"
        echo ""
        echo "Simulated results:"
        echo "user4@domain.com,Alice,Brown (PENDING DELETION - CONTACT OIT)"
        echo "user5@domain.com,David,Wilson (PENDING DELETION - CONTACT OIT)"
        echo "user6@domain.com,Carol,Davis (PENDING DELETION - CONTACT OIT)"
        return 0
    fi
    
    echo "Users in $OU_PENDING_DELETION:"
    $GAM print users ou "$OU_PENDING_DELETION" firstname lastname
}

# Function to query all suspended users
query_all_suspended_users() {
    echo -e "${CYAN}Querying all users in Suspended Accounts OUs...${NC}"
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}[DISCOVERY] Would query all suspended OUs${NC}"
        echo ""
        echo "Simulated results:"
        echo "OU: $OU_SUSPENDED"
        echo "  user7@domain.com,Emma,Taylor"
        echo "  user8@domain.com,Frank,Moore"
        echo ""
        echo "OU: $OU_TEMPHOLD"
        echo "  user1@domain.com,John,Doe (Suspended Account - Temporary Hold)"
        echo "  user2@domain.com,Jane,Smith (Suspended Account - Temporary Hold)"
        echo ""
        echo "OU: $OU_PENDING_DELETION"
        echo "  user4@domain.com,Alice,Brown (PENDING DELETION - CONTACT OIT)"
        echo "  user5@domain.com,David,Wilson (PENDING DELETION - CONTACT OIT)"
        return 0
    fi
    
    echo "=== Users in General Suspended OU ==="
    local suspended_users=$($GAM print users ou "$OU_SUSPENDED" firstname lastname 2>/dev/null)
    echo "$suspended_users"
    echo ""
    
    echo "=== Users in Temporary Hold OU ==="
    local temphold_users=$($GAM print users ou "$OU_TEMPHOLD" firstname lastname 2>/dev/null)
    echo "$temphold_users"
    echo ""
    
    echo "=== Users in Pending Deletion OU ==="
    local pending_users=$($GAM print users ou "$OU_PENDING_DELETION" firstname lastname 2>/dev/null)
    echo "$pending_users"
    
    # Offer CSV export option
    local combined_data=""
    if [[ -n "$suspended_users" ]]; then
        combined_data+="# Users in General Suspended OU ($OU_SUSPENDED)"$'\n'
        combined_data+="$suspended_users"$'\n'$'\n'
    fi
    if [[ -n "$temphold_users" ]]; then
        combined_data+="# Users in Temporary Hold OU ($OU_TEMPHOLD)"$'\n'
        combined_data+="$temphold_users"$'\n'$'\n'
    fi
    if [[ -n "$pending_users" ]]; then
        combined_data+="# Users in Pending Deletion OU ($OU_PENDING_DELETION)"$'\n'
        combined_data+="$pending_users"$'\n'
    fi
    
    if [[ -n "$combined_data" ]] && type quick_export >/dev/null 2>&1; then
        quick_export "$combined_data" "suspended_users" "All suspended users across all suspended account OUs"
    fi
}

# Function to scan active accounts for orphaned pending deletion files
scan_active_accounts() {
    echo -e "${CYAN}Scanning active accounts for orphaned pending deletion files...${NC}"
    echo ""
    
    # Create scan directory
    local scan_dir="${SCRIPTPATH}/active-account-scan"
    execute_command "mkdir -p \"$scan_dir\"" "Create scan directory"
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}[DISCOVERY] Would scan all active users for pending deletion files${NC}"
        echo "Simulated scan results:"
        echo "Found 3 active users with orphaned pending deletion files:"
        echo "  active1@domain.com - 2 files with pending deletion markers"
        echo "  active2@domain.com - 1 file with pending deletion markers"  
        echo "  active3@domain.com - 5 files with pending deletion markers"
        echo ""
        echo "Results would be saved to: $scan_dir/"
        return 0
    fi
    
    echo "Retrieving list of all active (not suspended) users..."
    # Get list of active users
    local active_users=$($GAM print users query "isSuspended=False" | awk -F, 'NR>1 {print $1}')
    local total_users=$(echo "$active_users" | wc -l)
    local current=0
    local users_with_files=0
    
    echo "Scanning $total_users active users for pending deletion files..."
    echo ""
    
    # Iterate over each active user
    for user in $active_users; do
        ((current++))
        show_progress $current $total_users "Scanning $user"
        
        # Define output file for this user's scan results
        local output_file="${scan_dir}/gam_output_${user}.txt"
        
        # Scan for files with pending deletion markers
        $GAM user "$user" show filelist id name | \
        grep "(PENDING DELETION - CONTACT OIT)" > "$output_file"
        
        # If files found, count them and keep the file
        if [[ -s "$output_file" ]]; then
            local file_count=$(wc -l < "$output_file")
            ((users_with_files++))
            echo "Found $file_count pending deletion files for $user"
        else
            # Remove empty file
            rm -f "$output_file"
        fi
    done
    
    echo ""
    echo -e "${GREEN}Scan complete!${NC}"
    echo "Users scanned: $total_users"
    echo "Users with orphaned pending deletion files: $users_with_files"
    echo "Detailed results saved to: $scan_dir/"
    
    if [[ $users_with_files -gt 0 ]]; then
        echo ""
        echo -e "${YELLOW}âš ï¸  Found $users_with_files active users with orphaned pending deletion files${NC}"
        echo "These files should be cleaned up as they belong to active accounts."
        echo ""
        echo "Would you like to see a summary? (y/n)"
        read -p "> " show_summary
        if [[ "$show_summary" =~ ^[Yy] ]]; then
            echo ""
            echo "=== SUMMARY OF ORPHANED FILES ==="
            for file in "$scan_dir"/gam_output_*.txt; do
                if [[ -f "$file" ]]; then
                    local username=$(basename "$file" .txt | sed 's/gam_output_//')
                    local count=$(wc -l < "$file")
                    echo "$username: $count files"
                fi
            done
            
            echo ""
            echo "Would you like to perform bulk cleanup on these orphaned files? (y/n)"
            read -p "> " perform_cleanup
            if [[ "$perform_cleanup" =~ ^[Yy] ]]; then
                bulk_cleanup_orphaned_files "$scan_dir"
            fi
        fi
    fi
}

# Function to query users by department/type filter
query_users_by_filter() {
    echo -e "${CYAN}Query users by filter...${NC}"
    echo ""
    echo "Filter options:"
    echo "1. Students (department: Student)"
    echo "2. Faculty (department: Faculty)"
    echo "3. Staff (department: Staff)"
    echo "4. Custom query"
    echo "5. Return to discovery menu"
    echo ""
    read -p "Select filter (1-5): " filter_choice
    
    case $filter_choice in
        1) query_users_by_department "Student" ;;
        2) query_users_by_department "Faculty" ;;
        3) query_users_by_department "Staff" ;;
        4) query_users_custom ;;
        5) return ;;
    esac
}

# Function to query users by department
query_users_by_department() {
    local department="$1"
    echo -e "${CYAN}Querying $department users...${NC}"
    echo ""
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}[DISCOVERY] Would query: $GAM print users query \"department: $department\"${NC}"
        echo ""
        echo "Simulated results for $department:"
        case $department in
            "Student")
                echo "student1@domain.com,John,Doe,Student"
                echo "student2@domain.com,Jane,Smith,Student"
                echo "student3@domain.com,Bob,Johnson,Student"
                ;;
            "Faculty")
                echo "prof1@domain.com,Dr. Alice,Brown,Faculty"
                echo "prof2@domain.com,Dr. David,Wilson,Faculty"
                ;;
            "Staff")
                echo "staff1@domain.com,Carol,Davis,Staff"
                echo "staff2@domain.com,Frank,Moore,Staff"
                ;;
        esac
        return 0
    fi
    
    echo "=== $department Users ==="
    local active_users=$($GAM print users query "department: $department" fields primaryemail,firstname,lastname,department,suspended 2>/dev/null)
    echo "$active_users"
    
    # Also show suspended users in this department
    echo ""
    echo "=== Suspended $department Users ==="
    local suspended_users=$($GAM print users query "department: $department AND isSuspended=True" fields primaryemail,firstname,lastname,department,suspended 2>/dev/null)
    echo "$suspended_users"
    
    # Offer CSV export option
    local combined_data=""
    if [[ -n "$active_users" ]]; then
        combined_data+="# Active $department Users"$'\n'
        combined_data+="$active_users"$'\n'$'\n'
    fi
    if [[ -n "$suspended_users" ]]; then
        combined_data+="# Suspended $department Users"$'\n'
        combined_data+="$suspended_users"$'\n'
    fi
    
    if [[ -n "$combined_data" ]] && type quick_export >/dev/null 2>&1; then
        quick_export "$combined_data" "${department}_users" "$department users (active and suspended)"
    fi
}

# Function for custom user queries
query_users_custom() {
    echo -e "${CYAN}Custom user query...${NC}"
    echo ""
    echo "Examples:"
    echo "  - isSuspended=True"
    echo "  - department: Student AND isSuspended=True"
    echo "  - orgUnitPath: '/Suspended Accounts'"
    echo "  - creationTime>2024-01-01"
    echo ""
    read -p "Enter GAM query: " custom_query
    
    if [[ -z "$custom_query" ]]; then
        echo "No query entered."
        return
    fi
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}[DISCOVERY] Would query: $GAM print users query \"$custom_query\"${NC}"
        echo "Simulated results for custom query would be displayed here."
        return 0
    fi
    
    echo "=== Custom Query Results ==="
    echo "Query: $custom_query"
    echo ""
    local query_results=$($GAM print users query "$custom_query" fields primaryemail,firstname,lastname,department,suspended,orgunitpath 2>/dev/null)
    echo "$query_results"
    
    # Offer CSV export option
    if [[ -n "$query_results" ]] && type quick_export >/dev/null 2>&1; then
        local sanitized_query=$(echo "$custom_query" | sed 's/[^a-zA-Z0-9_-]/_/g')
        quick_export "$query_results" "custom_query_${sanitized_query}" "Custom GAM query: $custom_query"
    fi
}

# Function to bulk cleanup orphaned pending deletion files
bulk_cleanup_orphaned_files() {
    local scan_dir="$1"
    echo -e "${CYAN}Performing bulk cleanup of orphaned pending deletion files...${NC}"
    echo ""
    
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}[DISCOVERY] Would clean up orphaned files for all users in scan results${NC}"
        echo "This would remove pending deletion suffixes and labels from files belonging to active users."
        return 0
    fi
    
    local files_processed=0
    local users_processed=0
    
    # Process each user's scan results
    for scan_file in "$scan_dir"/gam_output_*.txt; do
        if [[ -f "$scan_file" ]]; then
            local username=$(basename "$scan_file" .txt | sed 's/gam_output_//')
            local file_count=$(wc -l < "$scan_file")
            
            # Sanitize username to prevent command injection
            username=$(sanitize_gam_input "$username")
            
            if [[ -z "$username" ]]; then
                echo -e "${RED}Error: Username became empty after sanitization, skipping file: $scan_file${NC}"
                continue
            fi
            
            ((users_processed++))
            echo "Processing $username ($file_count files)..."
            
            # Read each file ID and clean it up
            while IFS=, read -r owner fileid filename; do
                if [[ -n "$fileid" && "$fileid" != "id" ]]; then
                    ((files_processed++))
                    
                    # Sanitize fileid to prevent command injection
                    fileid=$(sanitize_gam_input "$fileid")
                    filename=$(sanitize_gam_input "$filename")
                    
                    if [[ -z "$fileid" ]]; then
                        echo -e "${RED}Warning: File ID became empty after sanitization, skipping${NC}"
                        continue
                    fi
                    
                    # Remove pending deletion suffix from filename
                    local new_filename=${filename//" (PENDING DELETION - CONTACT OIT)"/}
                    if [[ "$new_filename" != "$filename" ]]; then
                        execute_command "$GAM user \"$username\" update drivefile \"$fileid\" newfilename \"$new_filename\"" "Clean filename: $filename"
                    fi
                    
                    # Remove drive label
                    execute_command "$GAM user \"$username\" process filedrivelabels \"$fileid\" deletelabelfield \"$LABEL_ID\" \"$FIELD_ID\"" "Remove drive label"
                    
                    echo "Cleaned: $fileid" >> "${SCRIPTPATH}/local-config/logs/orphaned-files-cleaned.txt"
                fi
            done < "$scan_file"
        fi
    done
    
    echo ""
    echo -e "${GREEN}Bulk cleanup complete!${NC}"
    echo "Users processed: $users_processed"
    echo "Files cleaned: $files_processed"
    echo "Log saved to: ${SCRIPTPATH}/local-config/logs/orphaned-files-cleaned.txt"
}

# Function to offer bulk operations on query results
offer_bulk_operations() {
    local result_file="$1"
    local operation_context="$2"
    
    if [[ ! -f "$result_file" || ! -s "$result_file" ]]; then
        return 0
    fi
    
    local user_count=$(wc -l < "$result_file")
    echo ""
    echo "Found $user_count users. Would you like to perform bulk operations on these users? (y/n)"
    read -p "> " perform_bulk
    
    if [[ "$perform_bulk" =~ ^[Yy] ]]; then
        echo ""
        echo "Select bulk operation:"
        echo "1. Add temporary hold to all users"
        echo "2. Remove temporary hold from all users"
        echo "3. Mark all users for pending deletion"
        echo "4. Remove pending deletion from all users"
        echo "5. Diagnose all users"
        echo "6. Cancel"
        echo ""
        read -p "Select operation (1-6): " bulk_op
        
        case $bulk_op in
            1) bulk_process_users "$result_file" "add_gwombat_hold" ;;
            2) bulk_process_users "$result_file" "remove_gwombat_hold" ;;
            3) bulk_process_users "$result_file" "add_pending" ;;
            4) bulk_process_users "$result_file" "remove_pending" ;;
            5) bulk_diagnose_users "$result_file" ;;
            15) echo "Bulk operation cancelled." ;;
        esac
    fi
}

# Function to process bulk operations on users
bulk_process_users() {
    local user_file="$1"
    local operation="$2"
    local user_count=$(wc -l < "$user_file")
    
    echo ""
    echo -e "${YELLOW}âš ï¸  BULK OPERATION WARNING âš ï¸${NC}"
    echo "You are about to perform '$operation' on $user_count users."
    echo ""
    
    if ! enhanced_confirm "bulk $operation" "$user_count" "high"; then
        echo "Bulk operation cancelled."
        return
    fi
    
    echo ""
    echo "Processing $user_count users..."
    local current=0
    
    while IFS= read -r user; do
        ((current++))
        echo -e "${YELLOW}Progress: $current/$user_count${NC}"
        
        case $operation in
            "add_gwombat_hold") process_user "$user" ;;
            "remove_gwombat_hold") remove_gwombat_hold_user "$user" ;;
            "add_pending") process_pending_user "$user" ;;
            "remove_pending") remove_pending_user "$user" ;;
        esac
        
        echo "----------------------------------------"
    done < "$user_file"
    
    echo -e "${GREEN}Bulk operation completed for $user_count users.${NC}"
}

# Function to bulk diagnose users
bulk_diagnose_users() {
    local user_file="$1"
    local user_count=$(wc -l < "$user_file")
    
    echo ""
    echo "Diagnosing $user_count users..."
    echo ""
    
    local current=0
    local consistent_users=0
    local inconsistent_users=0
    
    while IFS= read -r user; do
        ((current++))
        show_progress $current $user_count "Diagnosing users"
        
        # Quick diagnosis without full output
        diagnose_account "$user" > /tmp/diagnosis_$user.txt 2>&1
        if grep -q "âœ… Account appears to be in consistent" /tmp/diagnosis_$user.txt; then
            ((consistent_users++))
        else
            ((inconsistent_users++))
            echo "$user" >> "${SCRIPTPATH}/local-config/logs/inconsistent-users.txt"
        fi
        rm -f /tmp/diagnosis_$user.txt
    done < "$user_file"
    
    echo ""
    echo -e "${GREEN}Bulk diagnosis complete!${NC}"
    echo "Users diagnosed: $user_count"
    echo "Consistent accounts: $consistent_users"
    echo "Inconsistent accounts: $inconsistent_users"
    
    if [[ $inconsistent_users -gt 0 ]]; then
        echo "Inconsistent users logged to: ${SCRIPTPATH}/local-config/logs/inconsistent-users.txt"
    fi
}

# Function to diagnose account consistency
diagnose_account() {
    local user="$1"
    echo -e "${CYAN}=== DIAGNOSING ACCOUNT: $user ===${NC}"
    echo ""
    
    # Check user's current OU
    echo -e "${YELLOW}1. Checking Organizational Unit...${NC}"
    current_ou=$(get_user_ou "$user")
    echo "Current OU: $current_ou"
    
    # Check user's last name
    echo -e "${YELLOW}2. Checking user last name...${NC}"
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        lastname="Sample User (Suspended Account - Temporary Hold)"
        echo -e "${CYAN}[DISCOVERY] Would query user info for: $user${NC}"
    else
        lastname=$($GAM info user "$user" | awk -F': ' '/Last Name:/ {print $2}')
    fi
    echo "Last name: $lastname"
    
    # Check files with temporary hold suffix
    echo -e "${YELLOW}3. Checking files with temporary hold suffix...${NC}"
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo -e "${CYAN}[DISCOVERY] Would query files for user: $user${NC}"
        echo "Simulated: Found 15 files with '(Suspended Account - Temporary Hold)' suffix"
        files_with_suffix=15
    else
        gamadmin_hold_files=$($GAM user "$user" show filelist id name | grep -c "(Suspended Account - Temporary Hold)")
        echo "Files with suffix: $gamadmin_hold_files"
        files_with_suffix=$gamadmin_hold_files
    fi
    
    # Check files without suffix (should be 0 for consistent account)
    echo -e "${YELLOW}4. Checking files without temporary hold suffix...${NC}"
    if [[ "$DRY_RUN" == "true" || "$DISCOVERY_MODE" == "true" ]]; then
        echo "Simulated: Found 2 files without required suffix"
        files_without_suffix=2
    else
        # This would need more complex logic to count all files vs suffixed files
        echo "Manual verification recommended for file consistency"
        files_without_suffix=0
    fi
    
    # Summary
    echo ""
    echo -e "${MAGENTA}=== DIAGNOSIS SUMMARY ===${NC}"
    echo "OU Status: $([ "$current_ou" == "$OU_TEMPHOLD" ] && echo "âœ… Correct" || echo "âŒ Incorrect")"
    echo "Name Status: $([[ "$lastname" == *"(Suspended Account - Temporary Hold)" ]] && echo "âœ… Correct" || echo "âŒ Missing suffix")"
    echo "Files with suffix: $files_with_suffix"
    echo "Files without suffix: $files_without_suffix"
    
    if [[ "$current_ou" == "$OU_TEMPHOLD" && "$lastname" == *"(Suspended Account - Temporary Hold)" && $files_without_suffix -eq 0 ]]; then
        echo -e "${GREEN}âœ… Account appears to be in consistent temporary hold state${NC}"
    else
        echo -e "${RED}âŒ Account has inconsistencies that may need attention${NC}"
    fi
}

# Function to add pending deletion to user's last name
add_pending_lastname() {
    local email="$1"
    echo -e "${GREEN}Step 1: Adding pending deletion to last name for $email${NC}"
    
    # Get the current last name of the user using GAM
    if [[ "$DRY_RUN" == "true" ]]; then
        current_lastname="Sample User"
        echo -e "${CYAN}[DRY-RUN] Would query user info for: $email${NC}"
    else
        current_lastname=$($GAM info user "$email" | awk -F': ' '/Last Name:/ {print $2}')
    fi
    
    # Check if the current last name already has pending deletion
    if [[ "$current_lastname" == *"(PENDING DELETION - CONTACT OIT)" ]]; then
        echo "No change needed for $email, already has pending deletion: '$current_lastname'"
    else
        # Add the "(PENDING DELETION - CONTACT OIT)" suffix to the current last name
        new_lastname="$current_lastname (PENDING DELETION - CONTACT OIT)"
        
        # Update the last name
        echo "Updating $email from '$current_lastname' to '$new_lastname'"
        execute_command "$GAM update user \"$email\" lastname \"$new_lastname\"" "Update user lastname"
    fi
}

# Function to add pending deletion to all files
add_pending_to_files() {
    local user_email_full="$1"
    local user_email=$(echo $user_email_full | awk -F@ '{print $1}')
    
    echo -e "${GREEN}Step 2: Adding pending deletion to all files for $user_email_full${NC}"
    
    # Define files
    CSV_DIR="${SCRIPTPATH}/csv-files"
    INPUT_FILE="${CSV_DIR}/${user_email}_active-shares.csv"
    UNIQUE_FILE="${CSV_DIR}/${user_email}_unique_files.csv"
    TEMP_FILE="${CSV_DIR}/${user_email}_temp.csv"
    ALL_FILE="${CSV_DIR}/${user_email}_all_files.csv"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would run file listing and renaming for: $user_email_full${NC}"
        echo "Simulated: Found 25 files to rename with pending deletion"
        
        # Simulate file processing
        for ((counter=1; counter<=5; counter++)); do
            show_progress $counter 5 "Processing file $counter"
            filename="Sample File $counter.pdf"
            new_filename="Sample File $counter.pdf (PENDING DELETION - CONTACT OIT)"
            echo -e "${CYAN}[DRY-RUN] Would rename: $filename -> $new_filename${NC}"
            sleep 0.1
        done
        return 0
    fi
    
    # Generate file sharing analysis using integrated functions
    analyze_user_file_sharing "$user_email_full"
    
    # Generate the master list of all files owned by this account
    $GAM user ${user_email_full} show filelist id title > "$ALL_FILE"
    cat "$INPUT_FILE" | awk -F, '{print $1","$2","$3","$4","$5","$6","$7}' | sort | uniq > "$UNIQUE_FILE"
    
    # Create temp file with updated filenames
    rm -f "$TEMP_FILE"
    touch "$TEMP_FILE"
    counter=0
    total=$(cat "$UNIQUE_FILE" | sort | uniq | wc -l)
    
    for file_id in $(cat "$UNIQUE_FILE" | sort | uniq | egrep -v mimeType | awk -F, '{print $2}'); do
        ((counter++))
        show_progress $counter $total "Collecting file info"
        grep $file_id "$ALL_FILE" >> "$TEMP_FILE"
    done
    
    echo "Total shared files: $(cat $TEMP_FILE | wc -l)"
    
    # Initialize the counter for renaming
    counter=0
    total=$(cat "$TEMP_FILE" | egrep -v "Owner,id,name" | egrep -v "PENDING DELETION" | wc -l)
    echo "$total files need pending deletion suffix"
    
    if [[ $total -eq 0 ]]; then
        echo "All files already have the pending deletion suffix."
        return
    fi
    
    # Read in the temporary file and extract the relevant information
    while IFS=, read -r fileid filename; do
        ((counter++))
        show_progress $counter $total "Adding pending deletion"
        
        if [[ $fileid != *"http"* ]]; then
            # Get the current filename directly from Google Drive
            current_filename=$($GAM user "$user_email_full" show fileinfo "$fileid" fields name | grep 'name:' | sed 's/name: //')
            
            # Only rename if the filename does not already contain "PENDING DELETION"
            if [[ $current_filename != *"PENDING DELETION - CONTACT OIT"* ]]; then
                # Construct the new filename
                new_filename="${current_filename} (PENDING DELETION - CONTACT OIT)"
                # Update the filename in Google Drive
                execute_command "$GAM user \"$user_email_full\" update drivefile \"$fileid\" newfilename \"$new_filename\"" "Rename file: $current_filename"
                echo "Renamed file: $fileid, $current_filename -> $new_filename" >> "${SCRIPTPATH}/local-config/tmp/$user_email-pending-added.txt"
            fi
        fi
    done < <(cat "$TEMP_FILE" | egrep -v "PENDING DELETION" | egrep -v "Owner,id,name" | awk -F, '{print $2","$3}')
    
    echo "Completed adding pending deletion to files for $user_email"
    echo "See ${SCRIPTPATH}/local-config/tmp/$user_email-pending-added.txt for details"
}

# Function to add drive labels to files
add_drive_labels() {
    local user_email_full="$1"
    
    # Sanitize input to prevent command injection
    user_email_full=$(sanitize_gam_input "$user_email_full")
    
    if [[ -z "$user_email_full" ]]; then
        echo -e "${RED}Error: User email became empty after sanitization${NC}"
        return 1
    fi
    
    local user_email=$(echo "$user_email_full" | awk -F@ '{print $1}')
    
    echo -e "${GREEN}Step 3: Adding drive labels to files for $user_email_full${NC}"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would add Education Plus license temporarily${NC}"
        echo -e "${CYAN}[DRY-RUN] Would add drive labels to all files${NC}"
        echo -e "${CYAN}[DRY-RUN] Would remove Education Plus license${NC}"
        return 0
    fi
    
    # Add Education Plus license temporarily for drive labels
    execute_command "$GAM user \"$user_email_full\" add license \"Google Workspace for Education Plus\"" "Add temporary license"
    echo "Waiting 30 seconds for license to take effect..."
    sleep 30
    
    CSV_DIR="${SCRIPTPATH}/csv-files"
    UNIQUE_FILE="${CSV_DIR}/${user_email}_unique_files.csv"
    LOG_FILE="${SCRIPTPATH}/local-config/logs/${user_email}_drive-labels.txt"
    
    if [[ ! -f "$UNIQUE_FILE" ]]; then
        echo "No unique files CSV found, skipping drive labels"
        return
    fi
    
    # Add labels to all files
    counter=0
    total=$(cat "$UNIQUE_FILE" | egrep -v "vnd.google-apps.folder" | egrep -v "mimeType" | wc -l)
    echo "Adding drive labels to $total files"
    
    while IFS=, read -r owner file_id filename; do
        if [[ "$file_id" != "id" && "$file_id" != *"mimeType"* ]]; then
            ((counter++))
            show_progress $counter $total "Adding drive labels"
            
            # Sanitize file_id to prevent command injection
            file_id=$(sanitize_gam_input "$file_id")
            
            if [[ -z "$file_id" ]]; then
                echo -e "${RED}Warning: File ID became empty after sanitization, skipping${NC}"
                continue
            fi
            
            execute_command "$GAM user \"$user_email_full\" process filedrivelabels \"$file_id\" addlabelfield \"$LABEL_ID\" \"$FIELD_ID\" selection \"$SELECTION_ID\"" "Add label to file"
        fi
    done < "$UNIQUE_FILE"
    
    # Remove the temporary license
    execute_command "$GAM user \"$user_email_full\" delete license \"Google Workspace for Education Plus\"" "Remove temporary license"
    
    echo "Completed adding drive labels for $user_email"
}

# Function to remove pending deletion from user's last name
remove_pending_lastname() {
    local email="$1"
    echo -e "${GREEN}Step 1: Removing pending deletion from last name for $email${NC}"
    
    # Get the current last name of the user using GAM
    if [[ "$DRY_RUN" == "true" ]]; then
        current_lastname="Sample User (PENDING DELETION - CONTACT OIT)"
        echo -e "${CYAN}[DRY-RUN] Would query user info for: $email${NC}"
    else
        current_lastname=$($GAM info user "$email" | awk -F': ' '/Last Name:/ {print $2}')
    fi
    
    # Check if the current last name ends with "(PENDING DELETION - CONTACT OIT)"
    if [[ "$current_lastname" == *"(PENDING DELETION - CONTACT OIT)" ]]; then
        # Remove the "(PENDING DELETION - CONTACT OIT)" suffix from the current last name
        original_lastname="${current_lastname% (PENDING DELETION - CONTACT OIT)}"
        
        # Restore the original last name
        echo "Restoring $email from '$current_lastname' to '$original_lastname'"
        execute_command "$GAM update user \"$email\" lastname \"$original_lastname\"" "Update user lastname"
    else
        echo "No change needed for $email, current last name is '$current_lastname'"
    fi
}

# Function to remove pending deletion from all files
remove_pending_from_files() {
    local user_email_full="$1"
    local user_email=$(echo $user_email_full | awk -F@ '{print $1}')
    
    echo -e "${GREEN}Step 2: Removing pending deletion from all files for $user_email_full${NC}"
    
    # Create tmp directory if it doesn't exist
    execute_command "mkdir -p \"${SCRIPTPATH}/tmp\"" "Create tmp directory"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would query files for user: $user_email_full${NC}"
        TOTAL=4  # Simulate some files for dry-run
        echo "Found $TOTAL files to process (simulated)"
        
        # Simulate file processing
        for ((counter=1; counter<=TOTAL; counter++)); do
            show_progress $counter $TOTAL "Processing file $counter"
            filename="Sample File $counter (PENDING DELETION - CONTACT OIT).pdf"
            new_filename="Sample File $counter.pdf"
            echo -e "${CYAN}[DRY-RUN] Would rename: $filename -> $new_filename${NC}"
            echo -e "${CYAN}[DRY-RUN] Would remove drive label from file${NC}"
            sleep 0.1
        done
        return 0
    fi
    
    # Query the user's files and output only the files with (PENDING DELETION - CONTACT OIT) in the name
    $GAM user "$user_email_full" show filelist id name | grep "(PENDING DELETION - CONTACT OIT)" > "${SCRIPTPATH}/local-config/tmp/gam_output_pending_$user_email.txt"
    TOTAL=$(cat "${SCRIPTPATH}/local-config/tmp/gam_output_pending_$user_email.txt" | wc -l)
    counter=0
    
    if [[ $TOTAL -eq 0 ]]; then
        echo "No files found with '(PENDING DELETION - CONTACT OIT)' in the name."
        return
    fi
    
    echo "Found $TOTAL files to process"
    
    # Read in the temporary file and extract the relevant information, skipping the header line
    while IFS=, read -r owner fileid filename; do
        ((counter++))
        show_progress $counter $TOTAL "Processing files"
        
        # Remove the "(PENDING DELETION - CONTACT OIT)" string from filename
        new_filename=${filename//" (PENDING DELETION - CONTACT OIT)"/}
        if [[ "$new_filename" != "$filename" ]]; then
            # Rename the file
            execute_command "$GAM user \"$owner\" update drivefile \"$fileid\" newfilename \"$new_filename\"" "Rename file: $filename"
            echo "Renamed file: $fileid, $filename -> $new_filename" >> "${SCRIPTPATH}/local-config/tmp/$user_email-pending-removed.txt"
        fi
        
        # Remove drive label from file
        if [[ -n "$fileid" ]]; then
            execute_command "$GAM user $owner process filedrivelabels $fileid deletelabelfield $LABEL_ID $FIELD_ID" "Remove drive label"
        fi
    done < <(tail -n +2 "${SCRIPTPATH}/local-config/tmp/gam_output_pending_$user_email.txt") # Skip the first line (header)
    
    echo "Completed removing pending deletion from files for $user_email"
    echo "See ${SCRIPTPATH}/local-config/tmp/$user_email-pending-removed.txt for details"
}

# Function to remove user from all groups
remove_from_groups() {
    local user="$1"
    
    # Sanitize input to prevent command injection
    user=$(sanitize_gam_input "$user")
    
    if [[ -z "$user" ]]; then
        echo -e "${RED}Error: User became empty after sanitization${NC}"
        return 1
    fi
    
    echo -e "${GREEN}Step 4: Removing user from all groups for $user${NC}"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would query user groups for: $user${NC}"
        echo -e "${CYAN}[DRY-RUN] Would remove user from all groups${NC}"
        echo "Simulated: User would be removed from 8 groups"
        return 0
    fi
    
    # Get list of groups user is a member of
    groups=$($GAM print groups member "$user" 2>/dev/null | grep ${DOMAIN:-yourdomain.edu})
    
    if [[ -z "$groups" ]]; then
        echo "User $user is not a member of any groups"
        return
    fi
    
    echo "Removing user from groups..."
    for group in $groups; do
        echo "Removing user: $user from group: $group"
        execute_command "$GAM update group \"$group\" remove member \"$user\"" "Remove from group: $group"
        echo "Removed $user from $group" >> "${SCRIPTPATH}/users-removed-from-groups.txt"
    done
    
    echo "Completed removing user from groups"
}

# Function to confirm action
confirm_action() {
    while true; do
        read -p "Do you want to proceed with these changes? (y/n): " yn
        case $yn in
            [Yy]* ) return 0;;
            [Nn]* ) return 1;;
            * ) echo "Please answer yes or no.";;
        esac
    done
}

# Function to restore last name (from restore-lastname.sh)
restore_lastname() {
    local email="$1"
    echo -e "${GREEN}Step 1: Restoring last name for $email${NC}"
    
    # Get the current last name of the user using GAM
    if [[ "$DRY_RUN" == "true" ]]; then
        current_lastname="Sample User (PENDING DELETION - CONTACT OIT)"
        echo -e "${CYAN}[DRY-RUN] Would query user info for: $email${NC}"
    else
        current_lastname=$($GAM info user "$email" | awk -F': ' '/Last Name:/ {print $2}')
    fi
    
    # Check if the current last name ends with "(PENDING DELETION - CONTACT OIT)"
    if [[ "$current_lastname" == *"(PENDING DELETION - CONTACT OIT)" ]]; then
        # Remove the "(PENDING DELETION - CONTACT OIT)" suffix from the current last name
        original_lastname="${current_lastname% (PENDING DELETION - CONTACT OIT)}"
        
        # Restore the original last name
        echo "Restoring $email from '$current_lastname' to '$original_lastname'"
        execute_command "$GAM update user \"$email\" lastname \"$original_lastname\"" "Update user lastname"
    else
        echo "No change needed for $email, current last name is '$current_lastname'"
    fi
}

# Function to fix filenames (from gamadmin-filesfix.sh)
fix_filenames() {
    local user="$1"
    echo -e "${GREEN}Step 2: Fixing filenames for $user${NC}"
    
    # Create tmp directory if it doesn't exist
    execute_command "mkdir -p \"${SCRIPTPATH}/tmp\"" "Create tmp directory"
    
    # Query the user's files and output only the files with (PENDING DELETION - CONTACT OIT) in the name
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would query files for user: $user${NC}"
        TOTAL=3  # Simulate some files for dry-run
        echo "Found $TOTAL files to rename (simulated)"
        
        # Simulate file processing
        for ((counter=1; counter<=TOTAL; counter++)); do
            show_progress $counter $TOTAL "Processing file $counter"
            filename="Sample File $counter (PENDING DELETION - CONTACT OIT).pdf"
            new_filename="Sample File $counter.pdf"
            echo -e "${CYAN}[DRY-RUN] Would rename: $filename -> $new_filename (Suspended Account - Temporary Hold)${NC}"
            sleep 0.1  # Brief pause for visual effect
        done
    else
        $GAM user "$user" show filelist id name | grep "(PENDING DELETION - CONTACT OIT)" > "${SCRIPTPATH}/local-config/tmp/gam_output_$user.txt"
        TOTAL=$(cat "${SCRIPTPATH}/local-config/tmp/gam_output_$user.txt" | wc -l)
        counter=0
        
        if [[ $TOTAL -eq 0 ]]; then
            echo "No files found with '(PENDING DELETION - CONTACT OIT)' in the name."
            return
        fi
        
        echo "Found $TOTAL files to rename"
        
        # Read in the temporary file and extract the relevant information, skipping the header line
        while IFS=, read -r owner fileid filename; do
            ((counter++))
            show_progress $counter $TOTAL "Processing files"
            
            # Rename the file by removing the "(PENDING DELETION - CONTACT OIT)" string
            new_filename=${filename//"(PENDING DELETION - CONTACT OIT)"/}
            if [[ "$new_filename" != "$filename" ]]; then
                # If the filename has been changed, rename the file and print a message
                execute_command "$GAM user \"$owner\" update drivefile \"$fileid\" newfilename \"$new_filename (Suspended Account - Temporary Hold)\"" "Rename file: $filename"
                echo "Renamed file: $fileid, $filename -> $new_filename (Suspended Account - Temporary Hold)" >> "${SCRIPTPATH}/local-config/tmp/$user-fixed.txt"
            fi
        done < <(tail -n +2 "${SCRIPTPATH}/local-config/tmp/gam_output_$user.txt") # Skip the first line (header)
    fi
    
    echo "Completed renaming files for $user"
    if [[ "$DRY_RUN" != "true" ]]; then
        echo "See ${SCRIPTPATH}/local-config/tmp/$user-fixed.txt for details"
    fi
}

# Function to rename all files (from gamadmin-file-rename.sh)
rename_all_files() {
    local user_email_full="$1"
    local user_email=$(echo $user_email_full | awk -F@ '{print $1}')
    
    echo -e "${GREEN}Step 3: Renaming all files for $user_email_full${NC}"
    
    # Define files
    CSV_DIR="${SCRIPTPATH}/csv-files"
    INPUT_FILE="${CSV_DIR}/${user_email}_active-shares.csv"
    UNIQUE_FILE="${CSV_DIR}/${user_email}_unique_files.csv"
    TEMP_FILE="${CSV_DIR}/${user_email}_temp.csv"
    
    # Generate file sharing analysis using integrated functions
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${CYAN}[DRY-RUN] Would generate shared file list for $user_email${NC}"
        # Create simulated data for dry-run
        mkdir -p "$(dirname "$INPUT_FILE")"
        echo "owner,id,filename,shared_with,permission" > "$INPUT_FILE"
        echo "$user_email,123abc,Document1.pdf,activeuser1@${DOMAIN:-yourdomain.edu},reader" >> "$INPUT_FILE"
        echo "$user_email,456def,Document2.pdf,activeuser2@${DOMAIN:-yourdomain.edu},writer" >> "$INPUT_FILE"
        echo "$user_email,789ghi,Document3.pdf,externaluser@gmail.com,reader" >> "$INPUT_FILE"
    else
        analyze_user_file_sharing "$user_email_full"
    fi
    
    if [[ ! -f "$INPUT_FILE" ]]; then
        echo -e "${YELLOW}Warning: Shared file list not found at $INPUT_FILE${NC}"
        echo -e "${YELLOW}This may indicate that list-users-files.sh is not available or failed to run${NC}"
        echo -e "${CYAN}Skipping file renaming step. Only files already marked as pending deletion were processed.${NC}"
        log_warning "Shared file list not found for user $user_email_full - skipping bulk file renaming"
        return 0
    fi
    
    # Filter for files shared with active ${DOMAIN:-yourdomain.edu} accounts ONLY
    # Exclude external domains and already suspended accounts
    echo "Filtering for files shared with active ${DOMAIN:-yourdomain.edu} accounts..."
    
    # Generate list of files shared ONLY with active ${DOMAIN:-yourdomain.edu} accounts
    # This excludes files shared with external domains or already suspended users
    awk -F, '
    NR==1 {next}  # Skip header
    $4 ~ /@'"${DOMAIN:-your-domain.edu}"'$/ && $4 !~ /suspended|pending|temporary/ {
        print $1","$2","$3
    }' "$INPUT_FILE" | sort | uniq > "$UNIQUE_FILE"
    
    # Create final temp file with unique shared files
    rm -f "$TEMP_FILE"
    touch "$TEMP_FILE"
    cat "$UNIQUE_FILE" > "$TEMP_FILE"
    
    local shared_count=$(cat "$TEMP_FILE" | wc -l)
    local total_files=$(awk -F, 'NR>1 {print $3}' "$INPUT_FILE" | wc -l)
    local external_files=$((total_files - shared_count))
    
    echo "Analysis of $user_email file sharing:"
    echo "  Total files owned: $total_files"
    echo "  Files shared with active ${DOMAIN:-yourdomain.edu} accounts: $shared_count"
    echo "  Files shared externally or with suspended accounts: $external_files"
    echo ""
    
    if [[ $shared_count -eq 0 ]]; then
        echo -e "${GREEN}âœ“ No files are shared with active ${DOMAIN:-yourdomain.edu} accounts.${NC}"
        echo -e "${CYAN}This is ideal for security - no internal sharing to protect.${NC}"
        echo -e "${CYAN}Only files already marked as pending deletion were processed.${NC}"
        log_info "User $user_email_full has no files shared with active ${DOMAIN:-yourdomain.edu} accounts - optimal security state"
        return 0
    fi
    
    if [[ $external_files -gt 0 ]]; then
        echo -e "${CYAN}Note: $external_files files shared externally/with suspended accounts will NOT be renamed${NC}"
        echo -e "${CYAN}This preserves access for external collaborators and already-processed accounts${NC}"
        log_info "User $user_email_full: $external_files files preserved (external/suspended sharing)"
    fi
    
    # Initialize the counter
    total=$(cat "$TEMP_FILE" | egrep -v "(Suspended Account - Temporary Hold)" | egrep -v "owner,id,filename" | wc -l)
    echo "$total files need to be renamed"
    
    if [[ $total -eq 0 ]]; then
        echo "All files already have the required suffix."
        return
    fi
    
    # Read in the temporary file and extract the relevant information
    while IFS=, read -r owner id filename; do
        if [[ -n "$filename" && $filename != *"(Suspended Account - Temporary Hold)"* ]]; then
            new_filename="$filename (Suspended Account - Temporary Hold)"
            echo "Renaming: $filename -> $new_filename"
            $GAM user "$user_email_full" update drivefile id "$id" newfilename "$new_filename"
        fi
    done < <(awk -F, 'NR != 1 && !/owner,id,filename/' "$TEMP_FILE" | egrep -v "(Suspended Account - Temporary Hold)" | awk -F, '{print $1","$2","$3}')
    
    echo "Completed renaming all files for $user_email"
    echo "${user_email},$(date '+%Y-%m-%d %H:%M:%S')" >> "${SCRIPTPATH}/file-rename-done.txt"
}

# Function to update user last name (from gamadmin-namechange.sh)
update_user_lastname() {
    local username="$1"
    echo -e "${GREEN}Step 4: Updating last name for $username${NC}"
    
    # Get the current last name of the user using GAM
    lastname=$($GAM info user "$username" | awk -F': ' '/Last Name:/ {print $2}')
    
    # Check if the last name already ends with "(Suspended Account - Temporary Hold)"
    if [[ "$lastname" == *"(Suspended Account - Temporary Hold)" ]]; then
        echo "Last name already updated - $lastname"
    else
        # Add "(Suspended Account - Temporary Hold)" to the last name
        new_lastname="$lastname (Suspended Account - Temporary Hold)"
        
        echo "Updating $username from '$lastname' to '$new_lastname'"
        $GAM update user "$username" lastname "$new_lastname"
    fi
}

# Function to remove temporary hold from user's last name
remove_gwombat_hold_lastname() {
    local email="$1"
    echo -e "${GREEN}Step 1: Removing temporary hold from last name for $email${NC}"
    
    # Get the current last name of the user using GAM
    current_lastname=$($GAM info user "$email" | awk -F': ' '/Last Name:/ {print $2}')
    
    # Check if the current last name ends with "(Suspended Account - Temporary Hold)"
    if [[ "$current_lastname" == *"(Suspended Account - Temporary Hold)" ]]; then
        # Remove the "(Suspended Account - Temporary Hold)" suffix from the current last name
        original_lastname="${current_lastname% (Suspended Account - Temporary Hold)}"
        
        # Restore the original last name
        echo "Restoring $email from '$current_lastname' to '$original_lastname'"
        $GAM update user "$email" lastname "$original_lastname"
    else
        echo "No change needed for $email, current last name is '$current_lastname'"
    fi
}

# Function to remove temporary hold from all files
remove_gwombat_hold_from_files() {
    local user_email_full="$1"
    local user_email=$(echo $user_email_full | awk -F@ '{print $1}')
    
    echo -e "${GREEN}Step 2: Removing temporary hold from all files for $user_email_full${NC}"
    
    # Create tmp directory if it doesn't exist
    mkdir -p "${SCRIPTPATH}/tmp"
    
    # Query the user's files and output only the files with (Suspended Account - Temporary Hold) in the name
    $GAM user "$user_email_full" show filelist id name | grep "(Suspended Account - Temporary Hold)" > "${SCRIPTPATH}/local-config/tmp/gam_output_removal_$user_email.txt"
    TOTAL=$(cat "${SCRIPTPATH}/local-config/tmp/gam_output_removal_$user_email.txt" | wc -l)
    counter=0
    
    if [[ $TOTAL -eq 0 ]]; then
        echo "No files found with '(Suspended Account - Temporary Hold)' in the name."
        return
    fi
    
    echo "Found $TOTAL files to rename"
    
    # Read in the temporary file and extract the relevant information, skipping the header line
    while IFS=, read -r owner fileid filename; do
        ((counter++))
        # Remove the "(Suspended Account - Temporary Hold)" string from filename
        new_filename=${filename//" (Suspended Account - Temporary Hold)"/}
        if [[ "$new_filename" != "$filename" ]]; then
            # If the filename has been changed, rename the file and print a message
            $GAM user "$owner" update drivefile "$fileid" newfilename "$new_filename"
            echo "$counter of $TOTAL - Renamed file: $filename -> $new_filename"
            echo "Renamed file: $fileid, $filename -> $new_filename" >> "${SCRIPTPATH}/local-config/tmp/$user_email-removal.txt"
        fi
    done < <(tail -n +2 "${SCRIPTPATH}/local-config/tmp/gam_output_removal_$user_email.txt") # Skip the first line (header)
    
    echo "Completed removing temporary hold from files for $user_email"
    echo "See ${SCRIPTPATH}/local-config/tmp/$user_email-removal.txt for details"
}

# Function to remove temporary hold from a single user
remove_gwombat_hold_user() {
    local user="$1"
    
    echo -e "${BLUE}=== Removing temporary hold from user: $user ===${NC}"
    echo ""
    
    # Step 1: Remove temporary hold from lastname
    show_progress 1 3 "Removing temporary hold from lastname"
    remove_gwombat_hold_lastname "$user"
    echo ""
    
    # Step 2: Remove temporary hold from all files
    show_progress 2 3 "Removing temporary hold from all files"
    remove_gwombat_hold_from_files "$user"
    echo ""
    
    # Step 3: Move user to appropriate OU
    show_progress 3 3 "Moving user to destination OU"
    if [[ "$DRY_RUN" != "true" ]]; then
        destination_ou=$(get_destination_ou)
        move_user_to_ou "$user" "$destination_ou"
    else
        echo -e "${CYAN}[DRY-RUN] Would prompt for destination OU selection${NC}"
    fi
    echo ""
    
    # Step 4: Log completion
    if [[ "$DRY_RUN" != "true" ]]; then
        echo "$user" >> "${SCRIPTPATH}/gamadmin-removed.log"
        echo "$(date '+%Y-%m-%d %H:%M:%S'),$user" >> "${SCRIPTPATH}/file-removal-done.txt"
    else
        echo -e "${CYAN}[DRY-RUN] Would log user removal${NC}"
    fi
    echo -e "${GREEN}Temporary hold removed from user $user successfully.${NC}"
    echo ""
}

# Function to remove temporary hold from multiple users from file
remove_gwombat_hold_users_from_file() {
    local file_path="$1"
    local user_count=$(wc -l < "$file_path")
    local current=0
    
    echo -e "${BLUE}Removing temporary hold from $user_count users from file: $file_path${NC}"
    echo ""
    
    while IFS= read -r user; do
        # Skip empty lines and comments
        if [[ -n "$user" && ! "$user" =~ ^[[:space:]]*# ]]; then
            ((current++))
            echo -e "${YELLOW}Progress: $current/$user_count${NC}"
            remove_gwombat_hold_user "$user"
            echo "----------------------------------------"
        fi
    done < "$file_path"
    
    echo -e "${GREEN}Temporary hold removed from all users in file.${NC}"
}

# Function to process a single user for pending deletion
process_pending_user() {
    local user="$1"
    
    echo -e "${BLUE}=== Adding pending deletion for user: $user ===${NC}"
    echo ""
    
    # Step 1: Add pending deletion to lastname
    show_progress 1 5 "Adding pending deletion to lastname"
    add_pending_lastname "$user"
    echo ""
    
    # Step 2: Add pending deletion to all files
    show_progress 2 5 "Adding pending deletion to all files"
    add_pending_to_files "$user"
    echo ""
    
    # Step 3: Add drive labels to files
    show_progress 3 5 "Adding drive labels to files"
    add_drive_labels "$user"
    echo ""
    
    # Step 4: Remove user from all groups
    show_progress 4 5 "Removing user from all groups"
    remove_from_groups "$user"
    echo ""
    
    # Step 5: Move user to Pending Deletion OU
    show_progress 5 5 "Moving to Pending Deletion OU"
    move_user_to_ou "$user" "$OU_PENDING_DELETION"
    echo ""
    
    # Step 6: Log completion
    if [[ "$DRY_RUN" != "true" ]]; then
        echo "$user" >> "${SCRIPTPATH}/pending-deletion-done.log"
    else
        echo -e "${CYAN}[DRY-RUN] Would log user to pending-deletion-done.log${NC}"
    fi
    echo -e "${GREEN}User $user has been marked for pending deletion successfully.${NC}"
    echo ""
}

# Function to remove pending deletion from a single user
remove_pending_user() {
    local user="$1"
    
    echo -e "${BLUE}=== Removing pending deletion from user: $user ===${NC}"
    echo ""
    
    # Step 1: Remove pending deletion from lastname
    show_progress 1 3 "Removing pending deletion from lastname"
    remove_pending_lastname "$user"
    echo ""
    
    # Step 2: Remove pending deletion from all files (includes label removal)
    show_progress 2 3 "Removing pending deletion from all files"
    remove_pending_from_files "$user"
    echo ""
    
    # Step 3: Move user to appropriate OU
    show_progress 3 3 "Moving user to destination OU"
    if [[ "$DRY_RUN" != "true" ]]; then
        destination_ou=$(get_destination_ou)
        move_user_to_ou "$user" "$destination_ou"
    else
        echo -e "${CYAN}[DRY-RUN] Would prompt for destination OU selection${NC}"
    fi
    echo ""
    
    # Step 4: Log completion
    if [[ "$DRY_RUN" != "true" ]]; then
        echo "$user" >> "${SCRIPTPATH}/pending-deletion-removed.log"
        echo "$(date '+%Y-%m-%d %H:%M:%S'),$user" >> "${SCRIPTPATH}/pending-removal-done.txt"
    else
        echo -e "${CYAN}[DRY-RUN] Would log user removal${NC}"
    fi
    echo -e "${GREEN}Pending deletion removed from user $user successfully.${NC}"
    echo ""
}

# Function to process multiple users from file for pending deletion
process_pending_users_from_file() {
    local file_path="$1"
    local user_count=$(wc -l < "$file_path")
    local current=0
    
    echo -e "${BLUE}Adding pending deletion for $user_count users from file: $file_path${NC}"
    echo ""
    
    while IFS= read -r user; do
        # Skip empty lines and comments
        if [[ -n "$user" && ! "$user" =~ ^[[:space:]]*# ]]; then
            ((current++))
            echo -e "${YELLOW}Progress: $current/$user_count${NC}"
            process_pending_user "$user"
            echo "----------------------------------------"
        fi
    done < "$file_path"
    
    echo -e "${GREEN}Pending deletion added for all users in file.${NC}"
}

# Function to remove pending deletion from multiple users from file
remove_pending_users_from_file() {
    local file_path="$1"
    local user_count=$(wc -l < "$file_path")
    local current=0
    
    echo -e "${BLUE}Removing pending deletion from $user_count users from file: $file_path${NC}"
    echo ""
    
    while IFS= read -r user; do
        # Skip empty lines and comments
        if [[ -n "$user" && ! "$user" =~ ^[[:space:]]*# ]]; then
            ((current++))
            echo -e "${YELLOW}Progress: $current/$user_count${NC}"
            remove_pending_user "$user"
            echo "----------------------------------------"
        fi
    done < "$file_path"
    
    echo -e "${GREEN}Pending deletion removed from all users in file.${NC}"
}

# Function to process a single user
process_user() {
    local user="$1"
    
    log_info "Starting add_gwombat_hold operation for user: $user" "console"
    start_operation_timer
    
    echo -e "${BLUE}=== Processing user: $user ===${NC}"
    echo ""
    
    # Step 1: Restore lastname
    show_progress 1 5 "Restoring lastname"
    restore_lastname "$user"
    echo ""
    
    # Step 2: Fix filenames
    show_progress 2 5 "Fixing filenames"
    fix_filenames "$user"
    echo ""
    
    # Step 3: Rename all files
    show_progress 3 5 "Renaming all files"
    rename_all_files "$user"
    echo ""
    
    # Step 4: Update user lastname
    show_progress 4 5 "Updating user lastname"
    update_user_lastname "$user"
    echo ""
    
    # Step 5: Move user to Temporary Hold OU
    show_progress 5 5 "Moving to Temporary Hold OU"
    move_user_to_ou "$user" "$OU_TEMPHOLD"
    echo ""
    
    # Step 6: Log completion
    if [[ "$DRY_RUN" != "true" ]]; then
        echo "$user" >> "${SCRIPTPATH}/gwombat-done.log"
        log_operation "add_gwombat_hold" "$user" "SUCCESS" "Temporary hold added successfully"
    else
        echo -e "${CYAN}[DRY-RUN] Would log user to gwombat-done.log${NC}"
        log_operation "add_gwombat_hold" "$user" "DRY-RUN" "Dry-run mode - no changes made"
    fi
    
    end_operation_timer "add_gwombat_hold" 1
    log_info "Completed add_gwombat_hold operation for user: $user" "console"
    echo -e "${GREEN}User $user has been processed successfully.${NC}"
    echo ""
}

# Function to process multiple users from file
process_users_from_file() {
    local file_path="$1"
    local user_count=$(wc -l < "$file_path")
    local current=0
    
    echo -e "${BLUE}Processing $user_count users from file: $file_path${NC}"
    echo ""
    
    while IFS= read -r user; do
        # Skip empty lines and comments
        if [[ -n "$user" && ! "$user" =~ ^[[:space:]]*# ]]; then
            ((current++))
            echo -e "${YELLOW}Progress: $current/$user_count${NC}"
            process_user "$user"
            echo "----------------------------------------"
        fi
    done < "$file_path"
    
    echo -e "${GREEN}All users from file have been processed.${NC}"
}

# Lifecycle Stage Menus

# Suspended Account Lifecycle Management Menu
lifecycle_management_menu() {
    render_menu "user_group_management"
}

# Function to re-scan all domain accounts
rescan_all_accounts() {
    clear
    echo -e "${GREEN}=== Re-scan All Domain Accounts ===${NC}"
    echo ""
    echo -e "${CYAN}This will sync the database with current Google Workspace accounts.${NC}"
    echo -e "${YELLOW}This may take a few minutes for large domains.${NC}"
    echo ""
    
    read -p "Continue with account re-scan? (y/n): " confirm_scan
    if [[ "$confirm_scan" != "y" && "$confirm_scan" != "Y" ]]; then
        echo "Scan cancelled."
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    sync_domain_to_database
    mark_stats_dirty  # Force stats refresh
    
    echo ""
    echo -e "${GREEN}âœ… Account re-scan completed!${NC}"
    
    # Show summary
    local total_accounts=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "0")
    local active_accounts=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts WHERE current_stage = 'active';" 2>/dev/null || echo "0")
    local suspended_accounts=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM accounts WHERE current_stage IN ('recently_suspended', 'pending_deletion', 'temporary_hold', 'exit_row');" 2>/dev/null || echo "0")
    
    echo ""
    echo -e "${CYAN}ðŸ“Š Updated Account Summary:${NC}"
    echo "  Total accounts: $total_accounts"
    echo "  Active accounts: $active_accounts"
    echo "  Suspended accounts: $suspended_accounts"
    echo ""
    read -p "Press Enter to continue..."
}

# Function to list all accounts with filtering
list_all_accounts_menu() {
    render_menu "account_list_management"
}

# Function to list accounts with filters
list_accounts_filtered() {
    local filter_type="$1"
    
    echo -e "${CYAN}=== Listing $filter_type accounts ===${NC}"
    echo ""
    
    # Check if we should use database or fresh GAM data
    if choose_data_source "account listing"; then
        # Use database
        case $filter_type in
            "all")
                sqlite3 local-config/gwombat.db -header "SELECT email, current_stage, ou_path, updated_at FROM accounts ORDER BY email;"
                ;;
            "active")
                sqlite3 local-config/gwombat.db -header "SELECT email, current_stage, ou_path, updated_at FROM accounts WHERE current_stage = 'active' ORDER BY email;"
                ;;
            "suspended")
                sqlite3 local-config/gwombat.db -header "SELECT email, current_stage, ou_path, updated_at FROM accounts WHERE current_stage IN ('recently_suspended', 'pending_deletion', 'temporary_hold', 'exit_row') ORDER BY email;"
                ;;
        esac
    else
        # Use fresh GAM data - GAM7 syntax
        case $filter_type in
            "all")
                echo "Getting fresh account list from GAM7..."
                if ! $GAM print users fields name,primaryemail,suspended,orgunitpath,lastlogintime 2>/dev/null; then
                    echo -e "${RED}Error: GAM command failed. Please check GAM configuration.${NC}"
                    echo "You may need to run the setup wizard to configure GAM."
                fi
                ;;
            "active")
                echo "Getting active accounts from GAM7..."
                if ! $GAM print users query "isSuspended=false" fields name,primaryemail,suspended,orgunitpath,lastlogintime 2>/dev/null; then
                    echo -e "${RED}Error: GAM command failed. Please check GAM configuration.${NC}"
                fi
                ;;
            "suspended")
                echo "Getting suspended accounts from GAM7..."
                if ! $GAM print users query "isSuspended=true" fields name,primaryemail,suspended,orgunitpath,lastlogintime 2>/dev/null; then
                    echo -e "${RED}Error: GAM command failed. Please check GAM configuration.${NC}"
                fi
                ;;
        esac
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Function to list accounts by organizational unit
list_accounts_by_ou() {
    echo -e "${CYAN}=== List Accounts by OU ===${NC}"
    echo ""
    
    # Get list of OUs from GAM
    echo "Getting organizational units..."
    local ou_list
    if ! ou_list=$($GAM print orgunits 2>&1); then
        echo -e "${RED}Error retrieving organizational units: $ou_list${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo ""
    echo -e "${YELLOW}Available Organizational Units:${NC}"
    echo "$ou_list" | grep -v "orgUnitPath" | head -20
    echo ""
    
    read -p "Enter OU path (e.g., /Students, /Faculty): " ou_path
    
    if [[ -z "$ou_path" ]]; then
        echo -e "${RED}OU path cannot be empty${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo ""
    echo -e "${CYAN}=== Accounts in OU: $ou_path ===${NC}"
    
    # Get accounts in the specified OU
    if ! $GAM print users query "orgUnitPath='$ou_path'" fields name,primaryemail,suspended,orgunitpath 2>/dev/null; then
        echo -e "${RED}Error retrieving accounts for OU: $ou_path${NC}"
        echo "Note: OU path must be exact (case-sensitive)"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Function to search accounts by email or name
search_accounts() {
    echo -e "${CYAN}=== Search Accounts ===${NC}"
    echo ""
    
    echo "Search options:"
    echo "1. Search by email (partial match)"
    echo "2. Search by name (partial match)"
    echo "3. Search by exact email"
    echo ""
    
    read -p "Select search type (1-3): " search_type
    
    case $search_type in
        1)
            read -p "Enter email pattern to search for: " email_pattern
            if [[ -n "$email_pattern" ]]; then
                echo ""
                echo -e "${CYAN}=== Accounts matching email pattern: $email_pattern ===${NC}"
                $GAM print users query "email:$email_pattern*" fields name,primaryemail,suspended,orgunitpath 2>/dev/null
            fi
            ;;
        2)
            read -p "Enter name pattern to search for: " name_pattern
            if [[ -n "$name_pattern" ]]; then
                echo ""
                echo -e "${CYAN}=== Accounts matching name pattern: $name_pattern ===${NC}"
                $GAM print users query "name:$name_pattern*" fields name,primaryemail,suspended,orgunitpath 2>/dev/null
            fi
            ;;
        3)
            read -p "Enter exact email address: " exact_email
            if [[ -n "$exact_email" ]]; then
                echo ""
                echo -e "${CYAN}=== Account details for: $exact_email ===${NC}"
                $GAM info user "$exact_email" 2>/dev/null
            fi
            ;;
        *)
            echo -e "${RED}Invalid search type${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Function to calculate account storage sizes
calculate_account_sizes_menu() {
    render_menu "account_analysis"
}

# Function to calculate storage for all accounts
calculate_all_account_sizes() {
    echo -e "${CYAN}Calculating storage sizes for all accounts...${NC}"
    echo ""
    
    # Generate unique scan session ID
    local scan_session_id="scan_$(date +%Y%m%d_%H%M%S)"
    
    # Choose data source for account list
    if choose_data_source "storage calculation"; then
        # Use database
        local account_list=$(sqlite3 local-config/gwombat.db "SELECT email FROM accounts ORDER BY email;" 2>/dev/null)
    else
        # Use fresh GAM data
        local account_list=$($GAM print users fields primaryemail 2>/dev/null | tail -n +2 | cut -d',' -f1)
    fi
    
    if [[ -z "$account_list" ]]; then
        echo -e "${RED}No accounts found.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local counter=0
    local total_accounts=$(echo "$account_list" | wc -l)
    local accounts_processed=0
    local errors=0
    
    echo "Processing $total_accounts accounts with session ID: $scan_session_id"
    echo ""
    
    # Process accounts and store in new schema
    echo "$account_list" | while read email; do
        [[ -z "$email" ]] && continue
        
        ((counter++))
        show_progress $counter $total_accounts "Analyzing: $email"
        
        # Get storage info from GAM7 with quota information
        local user_info=$($GAM info user "$email" 2>/dev/null)
        
        if [[ -z "$user_info" ]]; then
            ((errors++))
            continue
        fi
        
        # Extract storage information
        local storage_used_bytes=0
        local storage_quota_bytes=0
        local display_name=""
        
        # Get display name from the user info
        display_name=$(echo "$user_info" | grep "Full Name:" | cut -d':' -f2- | xargs)
        
        # Parse storage used (try multiple patterns for GAM7)
        if echo "$user_info" | grep -q "Storage Used:"; then
            local storage_line=$(echo "$user_info" | grep "Storage Used:" | head -1)
            # Extract size value and unit with improved parsing
            local size_value=$(echo "$storage_line" | sed -n 's/.*Storage Used: *\([0-9.]*\).*/\1/p')
            local size_unit=$(echo "$storage_line" | sed -n 's/.*Storage Used: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
            
            if [[ -n "$size_value" && -n "$size_unit" ]]; then
                case "${size_unit,,}" in
                    "gb") storage_used_bytes=$(echo "$size_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_used_bytes=$(echo "$size_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_used_bytes=$(echo "$size_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_used_bytes="$size_value" ;;
                    *) storage_used_bytes=0 ;;
                esac
            fi
        # Try alternate patterns that GAM7 might use
        elif echo "$user_info" | grep -qi "quota.*used"; then
            local quota_line=$(echo "$user_info" | grep -i "quota.*used" | head -1)
            local size_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
            local size_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
            
            if [[ -n "$size_value" && -n "$size_unit" ]]; then
                case "${size_unit,,}" in
                    "gb") storage_used_bytes=$(echo "$size_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_used_bytes=$(echo "$size_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_used_bytes=$(echo "$size_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_used_bytes="$size_value" ;;
                    *) storage_used_bytes=0 ;;
                esac
            fi
        fi
        
        # Parse storage quota (try multiple patterns for GAM7)
        if echo "$user_info" | grep -q "Storage Limit:"; then
            local quota_line=$(echo "$user_info" | grep "Storage Limit:" | head -1)
            # Improved parsing with sed for better reliability
            local quota_value=$(echo "$quota_line" | sed -n 's/.*Storage Limit: *\([0-9.]*\).*/\1/p')
            local quota_unit=$(echo "$quota_line" | sed -n 's/.*Storage Limit: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
            
            if [[ -n "$quota_value" && -n "$quota_unit" ]]; then
                case "${quota_unit,,}" in
                    "gb") storage_quota_bytes=$(echo "$quota_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_quota_bytes=$(echo "$quota_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_quota_bytes=$(echo "$quota_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_quota_bytes="$quota_value" ;;
                    *) storage_quota_bytes=0 ;;
                esac
            fi
        # Try alternate quota patterns
        elif echo "$user_info" | grep -qi "quota.*limit"; then
            local quota_line=$(echo "$user_info" | grep -i "quota.*limit" | head -1)
            local quota_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
            local quota_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
            
            if [[ -n "$quota_value" && -n "$quota_unit" ]]; then
                case "${quota_unit,,}" in
                    "gb") storage_quota_bytes=$(echo "$quota_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_quota_bytes=$(echo "$quota_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_quota_bytes=$(echo "$quota_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_quota_bytes="$quota_value" ;;
                    *) storage_quota_bytes=0 ;;
                esac
            fi
        fi
        
        # Calculate derived values
        local storage_used_gb=$(echo "scale=3; $storage_used_bytes / 1073741824" | bc 2>/dev/null || echo "0")
        local storage_quota_gb=$(echo "scale=3; $storage_quota_bytes / 1073741824" | bc 2>/dev/null || echo "0")
        local usage_percentage=0
        
        if [[ $storage_quota_bytes -gt 0 ]]; then
            usage_percentage=$(echo "scale=2; ($storage_used_bytes * 100) / $storage_quota_bytes" | bc 2>/dev/null || echo "0")
        fi
        
        # Store in new account_storage_sizes table
        sqlite3 local-config/gwombat.db "
            INSERT OR REPLACE INTO account_storage_sizes (
                email, display_name, storage_used_bytes, storage_used_gb,
                storage_quota_bytes, storage_quota_gb, usage_percentage,
                measurement_date, scan_session_id
            ) VALUES (
                '$(echo "$email" | sed "s/'/''/g")',
                '$(echo "$display_name" | sed "s/'/''/g")',
                $storage_used_bytes,
                $storage_used_gb,
                $storage_quota_bytes,
                $storage_quota_gb,
                $usage_percentage,
                DATE('now'),
                '$scan_session_id'
            );
        " 2>/dev/null && ((accounts_processed++))
    done
    
    echo ""
    echo -e "${GREEN}âœ… Storage analysis completed!${NC}"
    echo "   Accounts processed: $accounts_processed"
    echo "   Session ID: $scan_session_id"
    echo ""
    
    # Display top 10 largest accounts
    echo -e "${CYAN}ðŸ“Š Top 10 Largest Accounts:${NC}"
    echo ""
    
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            SUBSTR(email, 1, 30) as Email,
            PRINTF('%.2f GB', storage_used_gb) as 'Storage Used',
            PRINTF('%.1f%%', usage_percentage) as 'Usage %',
            measurement_date as 'Measured'
        FROM account_storage_sizes 
        WHERE measurement_date = DATE('now')
        ORDER BY storage_used_gb DESC 
        LIMIT 10;
    " 2>/dev/null
    
    echo ""
    echo -e "${YELLOW}ðŸ’¡ Use menu option 'View Account Storage Sizes' for filtering and detailed analysis${NC}"
    echo ""
    read -p "Press Enter to continue..."
}

# Function to calculate storage for suspended accounts only
calculate_suspended_account_sizes() {
    echo -e "${CYAN}Calculating storage sizes for suspended accounts...${NC}"
    echo ""
    
    # Generate unique scan session ID
    local scan_session_id="scan_$(date +%Y%m%d_%H%M%S)"
    
    # Choose data source for suspended account list
    if choose_data_source "storage calculation"; then
        # Use database
        local account_list=$(sqlite3 local-config/gwombat.db "SELECT email FROM accounts WHERE current_stage IN ('recently_suspended', 'pending_deletion', 'temporary_hold', 'exit_row') ORDER BY email;" 2>/dev/null)
    else
        # Use fresh GAM data for suspended users
        local account_list=$($GAM print users query "isSuspended=true" fields primaryemail 2>/dev/null | tail -n +2 | cut -d',' -f1)
    fi
    
    if [[ -z "$account_list" ]]; then
        echo -e "${RED}No suspended accounts found.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local counter=0
    local total_accounts=$(echo "$account_list" | wc -l)
    local accounts_processed=0
    local errors=0
    
    echo "Processing $total_accounts suspended accounts with session ID: $scan_session_id"
    echo ""
    
    # Process accounts and store in new schema
    echo "$account_list" | while read email; do
        [[ -z "$email" ]] && continue
        
        ((counter++))
        show_progress $counter $total_accounts "Analyzing: $email"
        
        # Get storage info from GAM7 with quota information
        local user_info=$($GAM info user "$email" 2>/dev/null)
        
        if [[ -z "$user_info" ]]; then
            ((errors++))
            continue
        fi
        
        # Extract storage information
        local storage_used_bytes=0
        local storage_quota_bytes=0
        local display_name=""
        
        # Get display name from the user info
        display_name=$(echo "$user_info" | grep "Full Name:" | cut -d':' -f2- | xargs)
        
        # Parse storage used (try multiple patterns for GAM7)
        if echo "$user_info" | grep -q "Storage Used:"; then
            local storage_line=$(echo "$user_info" | grep "Storage Used:" | head -1)
            # Extract size value and unit with improved parsing
            local size_value=$(echo "$storage_line" | sed -n 's/.*Storage Used: *\([0-9.]*\).*/\1/p')
            local size_unit=$(echo "$storage_line" | sed -n 's/.*Storage Used: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
            
            if [[ -n "$size_value" && -n "$size_unit" ]]; then
                case "${size_unit,,}" in
                    "gb") storage_used_bytes=$(echo "$size_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_used_bytes=$(echo "$size_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_used_bytes=$(echo "$size_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_used_bytes="$size_value" ;;
                    *) storage_used_bytes=0 ;;
                esac
            fi
        # Try alternate patterns that GAM7 might use
        elif echo "$user_info" | grep -qi "quota.*used"; then
            local quota_line=$(echo "$user_info" | grep -i "quota.*used" | head -1)
            local size_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
            local size_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
            
            if [[ -n "$size_value" && -n "$size_unit" ]]; then
                case "${size_unit,,}" in
                    "gb") storage_used_bytes=$(echo "$size_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_used_bytes=$(echo "$size_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_used_bytes=$(echo "$size_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_used_bytes="$size_value" ;;
                    *) storage_used_bytes=0 ;;
                esac
            fi
        fi
        
        # Parse storage quota (try multiple patterns for GAM7)
        if echo "$user_info" | grep -q "Storage Limit:"; then
            local quota_line=$(echo "$user_info" | grep "Storage Limit:" | head -1)
            # Improved parsing with sed for better reliability
            local quota_value=$(echo "$quota_line" | sed -n 's/.*Storage Limit: *\([0-9.]*\).*/\1/p')
            local quota_unit=$(echo "$quota_line" | sed -n 's/.*Storage Limit: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
            
            if [[ -n "$quota_value" && -n "$quota_unit" ]]; then
                case "${quota_unit,,}" in
                    "gb") storage_quota_bytes=$(echo "$quota_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_quota_bytes=$(echo "$quota_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_quota_bytes=$(echo "$quota_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_quota_bytes="$quota_value" ;;
                    *) storage_quota_bytes=0 ;;
                esac
            fi
        elif echo "$user_info" | grep -qi "quota.*limit"; then
            local quota_line=$(echo "$user_info" | grep -i "quota.*limit" | head -1)
            local quota_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
            local quota_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
            
            if [[ -n "$quota_value" && -n "$quota_unit" ]]; then
                case "${quota_unit,,}" in
                    "gb") storage_quota_bytes=$(echo "$quota_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_quota_bytes=$(echo "$quota_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_quota_bytes=$(echo "$quota_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_quota_bytes="$quota_value" ;;
                    *) storage_quota_bytes=0 ;;
                esac
            fi
        fi
        
        # Calculate derived values
        local storage_used_gb=$(echo "scale=3; $storage_used_bytes / 1073741824" | bc 2>/dev/null || echo "0")
        local storage_quota_gb=$(echo "scale=3; $storage_quota_bytes / 1073741824" | bc 2>/dev/null || echo "0")
        local usage_percentage=0
        
        if [[ $storage_quota_bytes -gt 0 ]]; then
            usage_percentage=$(echo "scale=2; ($storage_used_bytes * 100) / $storage_quota_bytes" | bc 2>/dev/null || echo "0")
        fi
        
        # Store in account_storage_sizes table
        sqlite3 local-config/gwombat.db "
            INSERT OR REPLACE INTO account_storage_sizes (
                email, display_name, storage_used_bytes, storage_used_gb,
                storage_quota_bytes, storage_quota_gb, usage_percentage,
                measurement_date, scan_session_id
            ) VALUES (
                '$(echo "$email" | sed "s/'/''/g")',
                '$(echo "$display_name" | sed "s/'/''/g")',
                $storage_used_bytes,
                $storage_used_gb,
                $storage_quota_bytes,
                $storage_quota_gb,
                $usage_percentage,
                DATE('now'),
                '$scan_session_id'
            );
        " 2>/dev/null && ((accounts_processed++))
    done
    
    echo ""
    echo -e "${GREEN}âœ… Suspended account storage analysis completed!${NC}"
    echo "   Accounts processed: $accounts_processed"
    echo "   Session ID: $scan_session_id"
    echo ""
    read -p "Press Enter to continue..."
}

# Function to calculate storage for a single specific account
calculate_single_account_size() {
    echo -e "${CYAN}Calculate Storage Size for Single Account${NC}"
    echo ""
    read -p "Enter the email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Validate email format (basic validation)
    if [[ ! "$email" =~ ^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$ ]]; then
        echo -e "${RED}Invalid email format.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}Analyzing storage for: $email${NC}"
    echo ""
    
    # Generate unique scan session ID
    local scan_session_id="scan_$(date +%Y%m%d_%H%M%S)_single"
    
    # Get storage info from GAM7
    local user_info=$($GAM info user "$email" 2>/dev/null)
    
    if [[ -z "$user_info" ]]; then
        echo -e "${RED}âŒ Account not found or inaccessible: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${GREEN}âœ… Account found${NC}"
    echo ""
    
    # Extract storage information
    local storage_used_bytes=0
    local storage_quota_bytes=0
    local display_name=""
    
    # Get display name from the user info
    display_name=$(echo "$user_info" | grep "Full Name:" | cut -d':' -f2- | xargs)
    
    # Parse storage used (try multiple patterns for GAM7)
    if echo "$user_info" | grep -q "Storage Used:"; then
        local storage_line=$(echo "$user_info" | grep "Storage Used:" | head -1)
        # Extract size value and unit with improved parsing
        local size_value=$(echo "$storage_line" | sed -n 's/.*Storage Used: *\([0-9.]*\).*/\1/p')
        local size_unit=$(echo "$storage_line" | sed -n 's/.*Storage Used: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
        
        if [[ -n "$size_value" && -n "$size_unit" ]]; then
            case "${size_unit,,}" in
                "gb") storage_used_bytes=$(echo "$size_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                "mb") storage_used_bytes=$(echo "$size_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                "kb") storage_used_bytes=$(echo "$size_value * 1024" | bc 2>/dev/null || echo "0") ;;
                "bytes"|"b") storage_used_bytes="$size_value" ;;
                *) storage_used_bytes=0 ;;
            esac
        fi
    # Try alternate patterns that GAM7 might use
    elif echo "$user_info" | grep -qi "quota.*used"; then
        local quota_line=$(echo "$user_info" | grep -i "quota.*used" | head -1)
        local size_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
        local size_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
        
        if [[ -n "$size_value" && -n "$size_unit" ]]; then
            case "${size_unit,,}" in
                "gb") storage_used_bytes=$(echo "$size_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                "mb") storage_used_bytes=$(echo "$size_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                "kb") storage_used_bytes=$(echo "$size_value * 1024" | bc 2>/dev/null || echo "0") ;;
                "bytes"|"b") storage_used_bytes="$size_value" ;;
                *) storage_used_bytes=0 ;;
            esac
        fi
    fi
    
    # Parse storage quota (try multiple patterns for GAM7)
    if echo "$user_info" | grep -q "Storage Limit:"; then
        local quota_line=$(echo "$user_info" | grep "Storage Limit:" | head -1)
        # Improved parsing with sed for better reliability
        local quota_value=$(echo "$quota_line" | sed -n 's/.*Storage Limit: *\([0-9.]*\).*/\1/p')
        local quota_unit=$(echo "$quota_line" | sed -n 's/.*Storage Limit: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
        
        if [[ -n "$quota_value" && -n "$quota_unit" ]]; then
            case "${quota_unit,,}" in
                "gb") storage_quota_bytes=$(echo "$quota_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                "mb") storage_quota_bytes=$(echo "$quota_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                "kb") storage_quota_bytes=$(echo "$quota_value * 1024" | bc 2>/dev/null || echo "0") ;;
                "bytes"|"b") storage_quota_bytes="$quota_value" ;;
                *) storage_quota_bytes=0 ;;
            esac
        fi
    elif echo "$user_info" | grep -qi "quota.*limit"; then
        local quota_line=$(echo "$user_info" | grep -i "quota.*limit" | head -1)
        local quota_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
        local quota_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
        
        if [[ -n "$quota_value" && -n "$quota_unit" ]]; then
            case "${quota_unit,,}" in
                "gb") storage_quota_bytes=$(echo "$quota_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                "mb") storage_quota_bytes=$(echo "$quota_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                "kb") storage_quota_bytes=$(echo "$quota_value * 1024" | bc 2>/dev/null || echo "0") ;;
                "bytes"|"b") storage_quota_bytes="$quota_value" ;;
                *) storage_quota_bytes=0 ;;
            esac
        fi
    fi
    
    # Calculate derived values
    local storage_used_gb=$(echo "scale=3; $storage_used_bytes / 1073741824" | bc 2>/dev/null || echo "0")
    local storage_quota_gb=$(echo "scale=3; $storage_quota_bytes / 1073741824" | bc 2>/dev/null || echo "0")
    local usage_percentage=0
    
    if [[ $storage_quota_bytes -gt 0 ]]; then
        usage_percentage=$(echo "scale=2; ($storage_used_bytes * 100) / $storage_quota_bytes" | bc 2>/dev/null || echo "0")
    fi
    
    # Store in account_storage_sizes table
    sqlite3 local-config/gwombat.db "
        INSERT OR REPLACE INTO account_storage_sizes (
            email, display_name, storage_used_bytes, storage_used_gb,
            storage_quota_bytes, storage_quota_gb, usage_percentage,
            measurement_date, scan_session_id
        ) VALUES (
            '$(echo "$email" | sed "s/'/''/g")',
            '$(echo "$display_name" | sed "s/'/''/g")',
            $storage_used_bytes,
            $storage_used_gb,
            $storage_quota_bytes,
            $storage_quota_gb,
            $usage_percentage,
            DATE('now'),
            '$scan_session_id'
        );
    " 2>/dev/null
    
    echo -e "${GREEN}ðŸ“Š Storage Analysis Results:${NC}"
    echo ""
    echo -e "${CYAN}Account:${NC} $email"
    [[ -n "$display_name" ]] && echo -e "${CYAN}Name:${NC} $display_name"
    echo -e "${CYAN}Storage Used:${NC} $(printf '%.2f' $storage_used_gb) GB"
    echo -e "${CYAN}Storage Quota:${NC} $(printf '%.2f' $storage_quota_gb) GB"
    if [[ $storage_quota_bytes -gt 0 ]]; then
        echo -e "${CYAN}Usage Percentage:${NC} $(printf '%.1f' $usage_percentage)%"
        
        # Color-code the usage level
        if (( $(echo "$usage_percentage >= 95" | bc -l) )); then
            echo -e "${RED}Status:${NC} Critical (95%+)"
        elif (( $(echo "$usage_percentage >= 85" | bc -l) )); then
            echo -e "${YELLOW}Status:${NC} High (85%+)"
        elif (( $(echo "$usage_percentage >= 70" | bc -l) )); then
            echo -e "${CYAN}Status:${NC} Medium (70%+)"
        else
            echo -e "${GREEN}Status:${NC} Normal (<70%)"
        fi
    else
        echo -e "${YELLOW}Status:${NC} Unable to determine quota"
    fi
    echo ""
    echo -e "${GREEN}âœ… Analysis completed and stored in database${NC}"
    echo "   Session ID: $scan_session_id"
    echo ""
    read -p "Press Enter to continue..."
}

# Function to calculate storage from a list or CSV file
calculate_account_sizes_from_list() {
    echo -e "${CYAN}Calculate Storage from Account List/CSV${NC}"
    echo ""
    echo "Select source:"
    echo "1. Load from account list (from database)"
    echo "2. Load from CSV file"
    echo "3. Enter emails manually"
    echo ""
    echo "m) Return to main menu"
    echo "x) Exit"
    echo ""
    read -p "Select source (1-3, m, x): " source_choice
    echo ""
    
    local account_list=""
    
    case $source_choice in
        1)
            # Load from existing account lists
            echo "Available account lists:"
            sqlite3 local-config/gwombat.db "SELECT DISTINCT list_name FROM account_list_members ORDER BY list_name;" 2>/dev/null | nl -w3 -s'. '
            echo ""
            read -p "Enter list name: " list_name
            
            if [[ -z "$list_name" ]]; then
                echo -e "${RED}List name cannot be empty.${NC}"
                read -p "Press Enter to continue..."
                return
            fi
            
            account_list=$(sqlite3 local-config/gwombat.db "SELECT email FROM account_list_members WHERE list_name='$list_name' ORDER BY email;" 2>/dev/null)
            
            if [[ -z "$account_list" ]]; then
                echo -e "${RED}No accounts found in list '$list_name'.${NC}"
                read -p "Press Enter to continue..."
                return
            fi
            ;;
        2)
            # Load from CSV file
            read -p "Enter path to CSV file: " csv_path
            
            if [[ ! -f "$csv_path" ]]; then
                echo -e "${RED}File not found: $csv_path${NC}"
                read -p "Press Enter to continue..."
                return
            fi
            
            echo "Select email column format:"
            echo "1. First column contains emails"
            echo "2. Specify column number"
            echo "3. Specify column header name"
            echo ""
            read -p "Select format (1-3): " format_choice
            
            case $format_choice in
                1) account_list=$(tail -n +2 "$csv_path" | cut -d',' -f1 | tr -d '"') ;;
                2) 
                    read -p "Enter column number (1-based): " col_num
                    account_list=$(tail -n +2 "$csv_path" | cut -d',' -f$col_num | tr -d '"')
                    ;;
                3)
                    read -p "Enter column header name: " header_name
                    local col_num=$(head -1 "$csv_path" | tr ',' '\n' | nl -w1 -s: | grep -i ":$header_name" | cut -d':' -f1)
                    if [[ -n "$col_num" ]]; then
                        account_list=$(tail -n +2 "$csv_path" | cut -d',' -f$col_num | tr -d '"')
                    else
                        echo -e "${RED}Header '$header_name' not found.${NC}"
                        read -p "Press Enter to continue..."
                        return
                    fi
                    ;;
                *) 
                    echo -e "${RED}Invalid option.${NC}"
                    read -p "Press Enter to continue..."
                    return
                    ;;
            esac
            ;;
        3)
            # Manual entry
            echo "Enter email addresses (one per line, empty line to finish):"
            account_list=""
            while true; do
                read -p "> " email
                [[ -z "$email" ]] && break
                account_list="$account_list$email"$'\n'
            done
            account_list=$(echo "$account_list" | sed '/^$/d')
            ;;
        m) return ;;
        x) exit 0 ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    if [[ -z "$account_list" ]]; then
        echo -e "${RED}No accounts to process.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local total_accounts=$(echo "$account_list" | wc -l)
    echo "Found $total_accounts accounts to analyze"
    echo ""
    read -p "Proceed with storage analysis? (y/N): " confirm
    
    if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
        echo "Analysis cancelled."
        read -p "Press Enter to continue..."
        return
    fi
    
    # Generate unique scan session ID
    local scan_session_id="scan_$(date +%Y%m%d_%H%M%S)_list"
    
    local counter=0
    local accounts_processed=0
    local errors=0
    
    echo ""
    echo "Processing $total_accounts accounts with session ID: $scan_session_id"
    echo ""
    
    # Process each account
    echo "$account_list" | while read email; do
        [[ -z "$email" ]] && continue
        
        ((counter++))
        show_progress $counter $total_accounts "Analyzing: $email"
        
        # Get storage info from GAM7
        local user_info=$($GAM info user "$email" 2>/dev/null)
        
        if [[ -z "$user_info" ]]; then
            ((errors++))
            continue
        fi
        
        # Extract storage information (same logic as other functions)
        local storage_used_bytes=0
        local storage_quota_bytes=0
        local display_name=""
        
        # Get display name from the user info
        display_name=$(echo "$user_info" | grep "Full Name:" | cut -d':' -f2- | xargs)
        
        # Parse storage used (try multiple patterns for GAM7)
        if echo "$user_info" | grep -q "Storage Used:"; then
            local storage_line=$(echo "$user_info" | grep "Storage Used:" | head -1)
            local size_value=$(echo "$storage_line" | sed -n 's/.*Storage Used: *\([0-9.]*\).*/\1/p')
            local size_unit=$(echo "$storage_line" | sed -n 's/.*Storage Used: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
            
            if [[ -n "$size_value" && -n "$size_unit" ]]; then
                case "${size_unit,,}" in
                    "gb") storage_used_bytes=$(echo "$size_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_used_bytes=$(echo "$size_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_used_bytes=$(echo "$size_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_used_bytes="$size_value" ;;
                    *) storage_used_bytes=0 ;;
                esac
            fi
        elif echo "$user_info" | grep -qi "quota.*used"; then
            local quota_line=$(echo "$user_info" | grep -i "quota.*used" | head -1)
            local size_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
            local size_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
            
            if [[ -n "$size_value" && -n "$size_unit" ]]; then
                case "${size_unit,,}" in
                    "gb") storage_used_bytes=$(echo "$size_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_used_bytes=$(echo "$size_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_used_bytes=$(echo "$size_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_used_bytes="$size_value" ;;
                    *) storage_used_bytes=0 ;;
                esac
            fi
        fi
        
        # Parse storage quota (same logic)
        if echo "$user_info" | grep -q "Storage Limit:"; then
            local quota_line=$(echo "$user_info" | grep "Storage Limit:" | head -1)
            local quota_value=$(echo "$quota_line" | sed -n 's/.*Storage Limit: *\([0-9.]*\).*/\1/p')
            local quota_unit=$(echo "$quota_line" | sed -n 's/.*Storage Limit: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
            
            if [[ -n "$quota_value" && -n "$quota_unit" ]]; then
                case "${quota_unit,,}" in
                    "gb") storage_quota_bytes=$(echo "$quota_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_quota_bytes=$(echo "$quota_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_quota_bytes=$(echo "$quota_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_quota_bytes="$quota_value" ;;
                    *) storage_quota_bytes=0 ;;
                esac
            fi
        elif echo "$user_info" | grep -qi "quota.*limit"; then
            local quota_line=$(echo "$user_info" | grep -i "quota.*limit" | head -1)
            local quota_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
            local quota_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
            
            if [[ -n "$quota_value" && -n "$quota_unit" ]]; then
                case "${quota_unit,,}" in
                    "gb") storage_quota_bytes=$(echo "$quota_value * 1073741824" | bc 2>/dev/null || echo "0") ;;
                    "mb") storage_quota_bytes=$(echo "$quota_value * 1048576" | bc 2>/dev/null || echo "0") ;;
                    "kb") storage_quota_bytes=$(echo "$quota_value * 1024" | bc 2>/dev/null || echo "0") ;;
                    "bytes"|"b") storage_quota_bytes="$quota_value" ;;
                    *) storage_quota_bytes=0 ;;
                esac
            fi
        fi
        
        # Calculate derived values
        local storage_used_gb=$(echo "scale=3; $storage_used_bytes / 1073741824" | bc 2>/dev/null || echo "0")
        local storage_quota_gb=$(echo "scale=3; $storage_quota_bytes / 1073741824" | bc 2>/dev/null || echo "0")
        local usage_percentage=0
        
        if [[ $storage_quota_bytes -gt 0 ]]; then
            usage_percentage=$(echo "scale=2; ($storage_used_bytes * 100) / $storage_quota_bytes" | bc 2>/dev/null || echo "0")
        fi
        
        # Store in account_storage_sizes table
        sqlite3 local-config/gwombat.db "
            INSERT OR REPLACE INTO account_storage_sizes (
                email, display_name, storage_used_bytes, storage_used_gb,
                storage_quota_bytes, storage_quota_gb, usage_percentage,
                measurement_date, scan_session_id
            ) VALUES (
                '$(echo "$email" | sed "s/'/''/g")',
                '$(echo "$display_name" | sed "s/'/''/g")',
                $storage_used_bytes,
                $storage_used_gb,
                $storage_quota_bytes,
                $storage_quota_gb,
                $usage_percentage,
                DATE('now'),
                '$scan_session_id'
            );
        " 2>/dev/null && ((accounts_processed++))
    done
    
    echo ""
    echo -e "${GREEN}âœ… List-based storage analysis completed!${NC}"
    echo "   Accounts processed: $accounts_processed"
    echo "   Session ID: $scan_session_id"
    echo ""
    read -p "Press Enter to continue..."
}

# Function to view account storage sizes with filtering and sorting
view_account_storage_sizes_menu() {
    render_menu "account_analysis"
}

# Show all storage sizes (latest measurement)
show_all_storage_sizes() {
    echo -e "${CYAN}ðŸ“Š All Account Storage Sizes (Latest Measurements)${NC}"
    echo ""
    
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            ROW_NUMBER() OVER (ORDER BY storage_used_gb DESC) as Rank,
            SUBSTR(email, 1, 35) as Email,
            COALESCE(SUBSTR(display_name, 1, 25), 'N/A') as Name,
            PRINTF('%.2f GB', storage_used_gb) as 'Storage Used',
            PRINTF('%.2f GB', storage_quota_gb) as 'Quota',
            PRINTF('%.1f%%', usage_percentage) as 'Usage %',
            CASE 
                WHEN usage_percentage >= 95 THEN 'Critical'
                WHEN usage_percentage >= 85 THEN 'High'
                WHEN usage_percentage >= 70 THEN 'Medium'
                ELSE 'Normal'
            END as Status,
            measurement_date as 'Measured'
        FROM latest_account_sizes
        ORDER BY storage_used_gb DESC;
    " 2>/dev/null
    
    echo ""
    echo -e "${YELLOW}ðŸ’¡ Legend: Critical (95%+), High (85%+), Medium (70%+), Normal (<70%)${NC}"
    echo ""
    read -p "Press Enter to continue..."
}

# Filter by size threshold
filter_by_size_threshold() {
    echo -e "${CYAN}Filter by Storage Size Threshold${NC}"
    echo ""
    echo "Select threshold:"
    echo "1. > 10 GB"
    echo "2. > 25 GB" 
    echo "3. > 50 GB"
    echo "4. > 100 GB"
    echo "5. Custom threshold"
    echo ""
    read -p "Select threshold (1-5): " threshold_choice
    echo ""
    
    local threshold_gb=0
    case $threshold_choice in
        1) threshold_gb=10 ;;
        2) threshold_gb=25 ;;
        3) threshold_gb=50 ;;
        4) threshold_gb=100 ;;
        5) 
            read -p "Enter custom threshold in GB: " threshold_gb
            if ! [[ "$threshold_gb" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
                echo -e "${RED}Invalid number format.${NC}"
                read -p "Press Enter to continue..."
                return
            fi
            ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    echo -e "${CYAN}ðŸ“Š Accounts with Storage > ${threshold_gb} GB${NC}"
    echo ""
    
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            SUBSTR(email, 1, 35) as Email,
            COALESCE(SUBSTR(display_name, 1, 25), 'N/A') as Name,
            PRINTF('%.2f GB', storage_used_gb) as 'Storage Used',
            PRINTF('%.1f%%', usage_percentage) as 'Usage %',
            measurement_date as 'Measured'
        FROM latest_account_sizes
        WHERE storage_used_gb > $threshold_gb
        ORDER BY storage_used_gb DESC;
    " 2>/dev/null
    
    echo ""
    read -p "Press Enter to continue..."
}

# Filter by usage percentage
filter_by_usage_percentage() {
    echo -e "${CYAN}Filter by Usage Percentage${NC}"
    echo ""
    echo "Select usage level:"
    echo "1. Critical usage (95%+)"
    echo "2. High usage (85%+)"
    echo "3. Medium usage (70%+)"
    echo "4. Custom percentage"
    echo ""
    read -p "Select usage level (1-4): " usage_choice
    echo ""
    
    local usage_threshold=0
    local usage_label=""
    case $usage_choice in
        1) usage_threshold=95; usage_label="Critical (95%+)" ;;
        2) usage_threshold=85; usage_label="High (85%+)" ;;
        3) usage_threshold=70; usage_label="Medium (70%+)" ;;
        4) 
            read -p "Enter custom usage percentage (0-100): " usage_threshold
            if ! [[ "$usage_threshold" =~ ^[0-9]+(\.[0-9]+)?$ ]] || (( $(echo "$usage_threshold > 100" | bc -l) )); then
                echo -e "${RED}Invalid percentage (must be 0-100).${NC}"
                read -p "Press Enter to continue..."
                return
            fi
            usage_label="Custom (${usage_threshold}%+)"
            ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    echo -e "${CYAN}ðŸ“Š Accounts with ${usage_label} Usage${NC}"
    echo ""
    
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            SUBSTR(email, 1, 35) as Email,
            COALESCE(SUBSTR(display_name, 1, 25), 'N/A') as Name,
            PRINTF('%.2f GB', storage_used_gb) as 'Storage Used',
            PRINTF('%.2f GB', storage_quota_gb) as 'Quota',
            PRINTF('%.1f%%', usage_percentage) as 'Usage %',
            measurement_date as 'Measured'
        FROM latest_account_sizes
        WHERE usage_percentage >= $usage_threshold
        ORDER BY usage_percentage DESC;
    " 2>/dev/null
    
    echo ""
    read -p "Press Enter to continue..."
}

# Search storage by account
search_storage_by_account() {
    echo -e "${CYAN}Search Account Storage${NC}"
    echo ""
    read -p "Enter email or name search term: " search_term
    echo ""
    
    if [[ -z "$search_term" ]]; then
        echo -e "${RED}Search term cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}ðŸ“Š Search Results for: '$search_term'${NC}"
    echo ""
    
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            email as Email,
            COALESCE(display_name, 'N/A') as 'Display Name',
            PRINTF('%.2f GB', storage_used_gb) as 'Storage Used',
            PRINTF('%.2f GB', storage_quota_gb) as 'Quota',
            PRINTF('%.1f%%', usage_percentage) as 'Usage %',
            measurement_date as 'Measured'
        FROM latest_account_sizes
        WHERE email LIKE '%$search_term%' OR display_name LIKE '%$search_term%'
        ORDER BY storage_used_gb DESC;
    " 2>/dev/null
    
    echo ""
    read -p "Press Enter to continue..."
}

# Sort storage sizes
sort_storage_sizes() {
    echo -e "${CYAN}Sort Account Storage${NC}"
    echo ""
    echo "Sort by:"
    echo "1. Storage size (largest first)"
    echo "2. Storage size (smallest first)"
    echo "3. Usage percentage (highest first)"
    echo "4. Usage percentage (lowest first)"
    echo "5. Email address (A-Z)"
    echo "6. Display name (A-Z)"
    echo ""
    read -p "Select sort option (1-6): " sort_choice
    echo ""
    
    local order_clause=""
    local sort_label=""
    case $sort_choice in
        1) order_clause="ORDER BY storage_used_gb DESC"; sort_label="Storage Size (Largest First)" ;;
        2) order_clause="ORDER BY storage_used_gb ASC"; sort_label="Storage Size (Smallest First)" ;;
        3) order_clause="ORDER BY usage_percentage DESC"; sort_label="Usage Percentage (Highest First)" ;;
        4) order_clause="ORDER BY usage_percentage ASC"; sort_label="Usage Percentage (Lowest First)" ;;
        5) order_clause="ORDER BY email ASC"; sort_label="Email Address (A-Z)" ;;
        6) order_clause="ORDER BY display_name ASC"; sort_label="Display Name (A-Z)" ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    echo -e "${CYAN}ðŸ“Š Account Storage - ${sort_label}${NC}"
    echo ""
    
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            SUBSTR(email, 1, 30) as Email,
            COALESCE(SUBSTR(display_name, 1, 20), 'N/A') as Name,
            PRINTF('%.2f GB', storage_used_gb) as 'Storage Used',
            PRINTF('%.1f%%', usage_percentage) as 'Usage %',
            measurement_date as 'Measured'
        FROM latest_account_sizes
        $order_clause;
    " 2>/dev/null
    
    echo ""
    read -p "Press Enter to continue..."
}

# Function to show storage size history and trends
storage_size_history_menu() {
    render_menu "statistics_metrics"
}

# Function to show storage change analytics
storage_change_analytics_menu() {
    render_menu "statistics_metrics"
}

# Show rapid growth accounts
show_rapid_growth_accounts() {
    echo -e "${CYAN}ðŸ“ˆ Accounts with Rapid Storage Growth${NC}"
    echo ""
    
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            SUBSTR(email, 1, 30) as Email,
            COALESCE(SUBSTR(display_name, 1, 20), 'N/A') as Name,
            PRINTF('%.2f GB', size_change_gb) as 'Growth',
            PRINTF('%.1f%%', percentage_change) as 'Change %',
            period_start as 'Period Start',
            period_end as 'Period End',
            PRINTF('%.2f GB', current_size_gb) as 'Current Size'
        FROM rapid_growth_accounts
        ORDER BY size_change_gb DESC
        LIMIT 20;
    " 2>/dev/null
    
    echo ""
    echo -e "${YELLOW}ðŸ’¡ Showing accounts with >1GB growth or >25% increase${NC}"
    echo ""
    read -p "Press Enter to continue..."
}

# Show biggest storage deltas
show_biggest_storage_deltas() {
    echo -e "${CYAN}ðŸ“Š Biggest Storage Changes (All Types)${NC}"
    echo ""
    
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            SUBSTR(email, 1, 30) as Email,
            COALESCE(SUBSTR(display_name, 1, 20), 'N/A') as Name,
            PRINTF('%.2f GB', size_change_gb) as 'Change',
            PRINTF('%.1f%%', percentage_change) as 'Change %',
            change_type as 'Type',
            period_start as 'Start',
            period_end as 'End'
        FROM storage_change_analysis
        ORDER BY ABS(size_change_gb) DESC
        LIMIT 25;
    " 2>/dev/null
    
    echo ""
    read -p "Press Enter to continue..."
}

# Show storage change summary
show_storage_change_summary() {
    echo -e "${CYAN}ðŸ“‹ Storage Change Summary${NC}"
    echo ""
    
    echo -e "${YELLOW}=== Change Type Distribution ===${NC}"
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            change_type as 'Change Type',
            COUNT(*) as 'Count',
            PRINTF('%.2f GB', AVG(size_change_gb)) as 'Avg Change',
            PRINTF('%.2f GB', MAX(ABS(size_change_gb))) as 'Max Change'
        FROM storage_change_analysis
        GROUP BY change_type
        ORDER BY COUNT(*) DESC;
    " 2>/dev/null
    
    echo ""
    echo -e "${YELLOW}=== Recent Activity (Last 30 Days) ===${NC}"
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            COUNT(*) as 'Total Changes',
            COUNT(CASE WHEN change_type = 'increase' THEN 1 END) as 'Increases',
            COUNT(CASE WHEN change_type = 'decrease' THEN 1 END) as 'Decreases',
            COUNT(CASE WHEN change_type = 'stable' THEN 1 END) as 'Stable',
            PRINTF('%.2f GB', SUM(CASE WHEN change_type = 'increase' THEN size_change_gb ELSE 0 END)) as 'Total Growth'
        FROM storage_change_analysis
        WHERE period_end >= DATE('now', '-30 days');
    " 2>/dev/null
    
    echo ""
    read -p "Press Enter to continue..."
}

# Show growth patterns
show_growth_patterns() {
    echo -e "${CYAN}ðŸ“ˆ Storage Growth Patterns${NC}"
    echo ""
    
    echo -e "${YELLOW}=== Top Growth Trends ===${NC}"
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            SUBSTR(email, 1, 25) as Email,
            analysis_period as 'Period',
            PRINTF('%.2f GB', avg_change_gb) as 'Avg Change',
            PRINTF('%.2f GB', max_change_gb) as 'Max Change',
            measurement_count as 'Measurements',
            latest_measurement as 'Latest'
        FROM storage_size_trends
        WHERE avg_change_gb > 0.1
        ORDER BY avg_change_gb DESC
        LIMIT 15;
    " 2>/dev/null
    
    echo ""
    read -p "Press Enter to continue..."
}

# Storage History Analysis Functions
show_account_storage_trends() {
    echo -e "${CYAN}Account Storage Trends${NC}"
    echo ""
    
    # Check if we have historical data
    local history_count=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM storage_size_history;" 2>/dev/null || echo "0")
    
    if [[ $history_count -eq 0 ]]; then
        echo -e "${YELLOW}No historical storage data found.${NC}"
        echo "Run storage calculations regularly to build historical trends."
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "Select an option:"
    echo "1. Show trends for specific account"
    echo "2. Show top accounts by growth rate"
    echo "3. Show accounts with declining storage"
    echo "4. Export trends data to CSV"
    echo ""
    read -p "Enter choice (1-4): " trend_choice
    
    case $trend_choice in
        1)
            read -p "Enter email address: " email
            if [[ -n "$email" ]]; then
                echo ""
                echo -e "${YELLOW}Storage Trend for: $email${NC}"
                echo ""
                
                sqlite3 -header -column local-config/gwombat.db "
                SELECT 
                    date(scan_time) as Date,
                    ROUND(total_size_gb, 2) as 'Storage (GB)',
                    ROUND(total_size_gb - LAG(total_size_gb) OVER (ORDER BY scan_time), 2) as 'Change (GB)',
                    ROUND(((total_size_gb - LAG(total_size_gb) OVER (ORDER BY scan_time)) / LAG(total_size_gb) OVER (ORDER BY scan_time)) * 100, 2) as 'Change %'
                FROM storage_size_history 
                WHERE email = '$email' 
                ORDER BY scan_time DESC 
                LIMIT 20;" 2>/dev/null
            else
                echo -e "${RED}Email address required.${NC}"
            fi
            ;;
        2)
            echo ""
            echo -e "${YELLOW}Top Accounts by Growth Rate (Last 30 Days)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                email as Email,
                ROUND(latest_size - earliest_size, 2) as 'Growth (GB)',
                ROUND(((latest_size - earliest_size) / earliest_size) * 100, 2) as 'Growth %',
                date(latest_scan) as 'Latest Scan'
            FROM (
                SELECT 
                    email,
                    MIN(total_size_gb) as earliest_size,
                    MAX(total_size_gb) as latest_size,
                    MIN(scan_time) as earliest_scan,
                    MAX(scan_time) as latest_scan
                FROM storage_size_history 
                WHERE scan_time >= datetime('now', '-30 days')
                GROUP BY email
                HAVING COUNT(*) >= 2
            )
            WHERE latest_size > earliest_size
            ORDER BY (latest_size - earliest_size) DESC
            LIMIT 15;" 2>/dev/null
            ;;
        3)
            echo ""
            echo -e "${YELLOW}Accounts with Declining Storage (Last 30 Days)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                email as Email,
                ROUND(latest_size - earliest_size, 2) as 'Reduction (GB)',
                ROUND(((latest_size - earliest_size) / earliest_size) * 100, 2) as 'Reduction %',
                date(latest_scan) as 'Latest Scan'
            FROM (
                SELECT 
                    email,
                    MIN(total_size_gb) as earliest_size,
                    MAX(total_size_gb) as latest_size,
                    MIN(scan_time) as earliest_scan,
                    MAX(scan_time) as latest_scan
                FROM storage_size_history 
                WHERE scan_time >= datetime('now', '-30 days')
                GROUP BY email
                HAVING COUNT(*) >= 2
            )
            WHERE latest_size < earliest_size
            ORDER BY (latest_size - earliest_size) ASC
            LIMIT 15;" 2>/dev/null
            ;;
        4)
            local export_file="storage_trends_$(date +%Y%m%d_%H%M%S).csv"
            
            sqlite3 -header -csv local-config/gwombat.db "
            SELECT 
                email,
                scan_time,
                total_size_gb,
                used_gb,
                free_gb,
                scan_session_id
            FROM storage_size_history 
            ORDER BY email, scan_time;" > "$export_file" 2>/dev/null
            
            echo -e "${GREEN}âœ“ Trends data exported to: $export_file${NC}"
            ;;
        *)
            echo -e "${RED}Invalid choice.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

show_systemwide_storage_growth() {
    echo -e "${CYAN}System-wide Storage Growth Analysis${NC}"
    echo ""
    
    # Check if we have historical data
    local history_count=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM storage_size_history;" 2>/dev/null || echo "0")
    
    if [[ $history_count -eq 0 ]]; then
        echo -e "${YELLOW}No historical storage data found.${NC}"
        echo "Run storage calculations regularly to build historical trends."
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "Select analysis period:"
    echo "1. Last 7 days"
    echo "2. Last 30 days"
    echo "3. Last 90 days"
    echo "4. Last 6 months"
    echo "5. All time"
    echo ""
    read -p "Enter choice (1-5): " period_choice
    
    local period_filter=""
    local period_desc=""
    
    case $period_choice in
        1) period_filter="WHERE scan_time >= datetime('now', '-7 days')"; period_desc="Last 7 Days" ;;
        2) period_filter="WHERE scan_time >= datetime('now', '-30 days')"; period_desc="Last 30 Days" ;;
        3) period_filter="WHERE scan_time >= datetime('now', '-90 days')"; period_desc="Last 90 Days" ;;
        4) period_filter="WHERE scan_time >= datetime('now', '-6 months')"; period_desc="Last 6 Months" ;;
        5) period_filter=""; period_desc="All Time" ;;
        *) echo -e "${RED}Invalid choice.${NC}"; read -p "Press Enter to continue..."; return ;;
    esac
    
    echo ""
    echo -e "${YELLOW}System-wide Storage Growth - $period_desc${NC}"
    echo ""
    
    # Overall growth statistics
    echo -e "${CYAN}Growth Summary:${NC}"
    sqlite3 -header -column local-config/gwombat.db "
    SELECT 
        'Total Storage' as Metric,
        ROUND(MIN(total_storage), 2) as 'Start (GB)',
        ROUND(MAX(total_storage), 2) as 'Current (GB)',
        ROUND(MAX(total_storage) - MIN(total_storage), 2) as 'Growth (GB)',
        ROUND(((MAX(total_storage) - MIN(total_storage)) / MIN(total_storage)) * 100, 2) as 'Growth %'
    FROM (
        SELECT 
            scan_time,
            SUM(total_size_gb) as total_storage
        FROM storage_size_history 
        $period_filter
        GROUP BY date(scan_time)
        ORDER BY scan_time
    );" 2>/dev/null
    
    echo ""
    echo -e "${CYAN}Daily Growth Trend:${NC}"
    sqlite3 -header -column local-config/gwombat.db "
    SELECT 
        date(scan_time) as Date,
        COUNT(DISTINCT email) as 'Accounts Scanned',
        ROUND(SUM(total_size_gb), 2) as 'Total Storage (GB)',
        ROUND(SUM(total_size_gb) - LAG(SUM(total_size_gb)) OVER (ORDER BY date(scan_time)), 2) as 'Daily Change (GB)'
    FROM storage_size_history 
    $period_filter
    GROUP BY date(scan_time)
    ORDER BY scan_time DESC
    LIMIT 14;" 2>/dev/null
    
    echo ""
    echo -e "${CYAN}Storage Distribution:${NC}"
    sqlite3 -header -column local-config/gwombat.db "
    SELECT 
        'Small (< 1GB)' as 'Storage Range',
        COUNT(*) as 'Account Count',
        ROUND(SUM(latest_size), 2) as 'Total (GB)'
    FROM (
        SELECT email, MAX(total_size_gb) as latest_size
        FROM storage_size_history 
        $period_filter
        GROUP BY email
    ) WHERE latest_size < 1
    UNION ALL
    SELECT 
        'Medium (1-10GB)' as 'Storage Range',
        COUNT(*) as 'Account Count',
        ROUND(SUM(latest_size), 2) as 'Total (GB)'
    FROM (
        SELECT email, MAX(total_size_gb) as latest_size
        FROM storage_size_history 
        $period_filter
        GROUP BY email
    ) WHERE latest_size >= 1 AND latest_size < 10
    UNION ALL
    SELECT 
        'Large (10-50GB)' as 'Storage Range',
        COUNT(*) as 'Account Count',
        ROUND(SUM(latest_size), 2) as 'Total (GB)'
    FROM (
        SELECT email, MAX(total_size_gb) as latest_size
        FROM storage_size_history 
        $period_filter
        GROUP BY email
    ) WHERE latest_size >= 10 AND latest_size < 50
    UNION ALL
    SELECT 
        'Extra Large (50GB+)' as 'Storage Range',
        COUNT(*) as 'Account Count',
        ROUND(SUM(latest_size), 2) as 'Total (GB)'
    FROM (
        SELECT email, MAX(total_size_gb) as latest_size
        FROM storage_size_history 
        $period_filter
        GROUP BY email
    ) WHERE latest_size >= 50;" 2>/dev/null
    
    echo ""
    read -p "Press Enter to continue..."
}

show_storage_changes_by_period() {
    echo -e "${CYAN}Storage Changes by Time Period${NC}"
    echo ""
    
    # Check if we have historical data
    local history_count=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM storage_size_history;" 2>/dev/null || echo "0")
    
    if [[ $history_count -eq 0 ]]; then
        echo -e "${YELLOW}No historical storage data found.${NC}"
        echo "Run storage calculations regularly to build historical trends."
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "Select reporting period:"
    echo "1. Daily changes (last 14 days)"
    echo "2. Weekly changes (last 8 weeks)"
    echo "3. Monthly changes (last 12 months)"
    echo "4. Custom date range"
    echo ""
    read -p "Enter choice (1-4): " period_choice
    
    case $period_choice in
        1)
            echo ""
            echo -e "${YELLOW}Daily Storage Changes (Last 14 Days)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                date(scan_time) as Date,
                COUNT(DISTINCT email) as 'Accounts',
                ROUND(SUM(total_size_gb), 2) as 'Total (GB)',
                ROUND(AVG(total_size_gb), 2) as 'Avg per Account (GB)',
                ROUND(SUM(total_size_gb) - LAG(SUM(total_size_gb)) OVER (ORDER BY date(scan_time)), 2) as 'Change (GB)'
            FROM storage_size_history 
            WHERE scan_time >= datetime('now', '-14 days')
            GROUP BY date(scan_time)
            ORDER BY scan_time DESC;" 2>/dev/null
            ;;
        2)
            echo ""
            echo -e "${YELLOW}Weekly Storage Changes (Last 8 Weeks)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                strftime('%Y-W%W', scan_time) as Week,
                COUNT(DISTINCT email) as 'Accounts',
                ROUND(SUM(total_size_gb), 2) as 'Total (GB)',
                ROUND(AVG(total_size_gb), 2) as 'Avg per Account (GB)',
                ROUND(SUM(total_size_gb) - LAG(SUM(total_size_gb)) OVER (ORDER BY strftime('%Y-W%W', scan_time)), 2) as 'Change (GB)'
            FROM storage_size_history 
            WHERE scan_time >= datetime('now', '-56 days')
            GROUP BY strftime('%Y-W%W', scan_time)
            ORDER BY Week DESC;" 2>/dev/null
            ;;
        3)
            echo ""
            echo -e "${YELLOW}Monthly Storage Changes (Last 12 Months)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                strftime('%Y-%m', scan_time) as Month,
                COUNT(DISTINCT email) as 'Accounts',
                ROUND(SUM(total_size_gb), 2) as 'Total (GB)',
                ROUND(AVG(total_size_gb), 2) as 'Avg per Account (GB)',
                ROUND(SUM(total_size_gb) - LAG(SUM(total_size_gb)) OVER (ORDER BY strftime('%Y-%m', scan_time)), 2) as 'Change (GB)'
            FROM storage_size_history 
            WHERE scan_time >= datetime('now', '-12 months')
            GROUP BY strftime('%Y-%m', scan_time)
            ORDER BY Month DESC;" 2>/dev/null
            ;;
        4)
            echo ""
            read -p "Enter start date (YYYY-MM-DD): " start_date
            read -p "Enter end date (YYYY-MM-DD): " end_date
            
            if [[ -n "$start_date" && -n "$end_date" ]]; then
                echo ""
                echo -e "${YELLOW}Storage Changes from $start_date to $end_date${NC}"
                echo ""
                
                sqlite3 -header -column local-config/gwombat.db "
                SELECT 
                    date(scan_time) as Date,
                    COUNT(DISTINCT email) as 'Accounts',
                    ROUND(SUM(total_size_gb), 2) as 'Total (GB)',
                    ROUND(AVG(total_size_gb), 2) as 'Avg per Account (GB)',
                    ROUND(SUM(total_size_gb) - LAG(SUM(total_size_gb)) OVER (ORDER BY date(scan_time)), 2) as 'Change (GB)'
                FROM storage_size_history 
                WHERE date(scan_time) BETWEEN '$start_date' AND '$end_date'
                GROUP BY date(scan_time)
                ORDER BY scan_time DESC;" 2>/dev/null
            else
                echo -e "${RED}Both start and end dates are required.${NC}"
            fi
            ;;
        *)
            echo -e "${RED}Invalid choice.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

show_top_storage_growth() {
    echo -e "${CYAN}Top Storage Growth Accounts${NC}"
    echo ""
    
    # Check if we have historical data
    local history_count=$(sqlite3 local-config/gwombat.db "SELECT COUNT(*) FROM storage_size_history;" 2>/dev/null || echo "0")
    
    if [[ $history_count -eq 0 ]]; then
        echo -e "${YELLOW}No historical storage data found.${NC}"
        echo "Run storage calculations regularly to build historical trends."
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "Select analysis type:"
    echo "1. Absolute growth (GB)"
    echo "2. Percentage growth (%)"
    echo "3. Rapid growth (last 7 days)"
    echo "4. Consistent growth pattern"
    echo "5. Export top growth accounts"
    echo ""
    read -p "Enter choice (1-5): " growth_choice
    
    case $growth_choice in
        1)
            echo ""
            echo -e "${YELLOW}Top Accounts by Absolute Growth (Last 30 Days)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                email as Email,
                ROUND(earliest_size, 2) as 'Start (GB)',
                ROUND(latest_size, 2) as 'Current (GB)',
                ROUND(latest_size - earliest_size, 2) as 'Growth (GB)',
                ROUND(((latest_size - earliest_size) / earliest_size) * 100, 2) as 'Growth %',
                date(latest_scan) as 'Latest Scan'
            FROM (
                SELECT 
                    email,
                    MIN(total_size_gb) as earliest_size,
                    MAX(total_size_gb) as latest_size,
                    MIN(scan_time) as earliest_scan,
                    MAX(scan_time) as latest_scan
                FROM storage_size_history 
                WHERE scan_time >= datetime('now', '-30 days')
                GROUP BY email
                HAVING COUNT(*) >= 2 AND latest_size > earliest_size
            )
            ORDER BY (latest_size - earliest_size) DESC
            LIMIT 20;" 2>/dev/null
            ;;
        2)
            echo ""
            echo -e "${YELLOW}Top Accounts by Percentage Growth (Last 30 Days)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                email as Email,
                ROUND(earliest_size, 2) as 'Start (GB)',
                ROUND(latest_size, 2) as 'Current (GB)',
                ROUND(latest_size - earliest_size, 2) as 'Growth (GB)',
                ROUND(((latest_size - earliest_size) / earliest_size) * 100, 2) as 'Growth %',
                date(latest_scan) as 'Latest Scan'
            FROM (
                SELECT 
                    email,
                    MIN(total_size_gb) as earliest_size,
                    MAX(total_size_gb) as latest_size,
                    MIN(scan_time) as earliest_scan,
                    MAX(scan_time) as latest_scan
                FROM storage_size_history 
                WHERE scan_time >= datetime('now', '-30 days')
                GROUP BY email
                HAVING COUNT(*) >= 2 AND latest_size > earliest_size AND earliest_size > 0.1
            )
            ORDER BY ((latest_size - earliest_size) / earliest_size) DESC
            LIMIT 20;" 2>/dev/null
            ;;
        3)
            echo ""
            echo -e "${YELLOW}Rapid Growth Accounts (Last 7 Days)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                email as Email,
                ROUND(earliest_size, 2) as 'Start (GB)',
                ROUND(latest_size, 2) as 'Current (GB)',
                ROUND(latest_size - earliest_size, 2) as 'Growth (GB)',
                ROUND(((latest_size - earliest_size) / earliest_size) * 100, 2) as 'Growth %',
                ROUND((latest_size - earliest_size) / 7, 2) as 'Daily Avg (GB)'
            FROM (
                SELECT 
                    email,
                    MIN(total_size_gb) as earliest_size,
                    MAX(total_size_gb) as latest_size,
                    MIN(scan_time) as earliest_scan,
                    MAX(scan_time) as latest_scan
                FROM storage_size_history 
                WHERE scan_time >= datetime('now', '-7 days')
                GROUP BY email
                HAVING COUNT(*) >= 2 AND latest_size > earliest_size
            )
            WHERE (latest_size - earliest_size) / 7 > 0.5  -- More than 0.5GB per day
            ORDER BY (latest_size - earliest_size) DESC
            LIMIT 15;" 2>/dev/null
            ;;
        4)
            echo ""
            echo -e "${YELLOW}Consistent Growth Pattern (Accounts with growth in multiple periods)${NC}"
            echo ""
            
            sqlite3 -header -column local-config/gwombat.db "
            SELECT 
                email as Email,
                COUNT(*) as 'Growth Periods',
                ROUND(MIN(total_size_gb), 2) as 'Minimum (GB)',
                ROUND(MAX(total_size_gb), 2) as 'Maximum (GB)',
                ROUND(MAX(total_size_gb) - MIN(total_size_gb), 2) as 'Total Growth (GB)',
                ROUND(AVG(total_size_gb), 2) as 'Average (GB)'
            FROM storage_size_history 
            WHERE scan_time >= datetime('now', '-60 days')
            GROUP BY email
            HAVING COUNT(*) >= 4 AND MAX(total_size_gb) > MIN(total_size_gb)
            ORDER BY (MAX(total_size_gb) - MIN(total_size_gb)) DESC
            LIMIT 15;" 2>/dev/null
            ;;
        5)
            local export_file="top_growth_accounts_$(date +%Y%m%d_%H%M%S).csv"
            
            sqlite3 -header -csv local-config/gwombat.db "
            SELECT 
                email,
                earliest_size as start_gb,
                latest_size as current_gb,
                latest_size - earliest_size as growth_gb,
                ROUND(((latest_size - earliest_size) / earliest_size) * 100, 2) as growth_percent,
                earliest_scan,
                latest_scan
            FROM (
                SELECT 
                    email,
                    MIN(total_size_gb) as earliest_size,
                    MAX(total_size_gb) as latest_size,
                    MIN(scan_time) as earliest_scan,
                    MAX(scan_time) as latest_scan
                FROM storage_size_history 
                WHERE scan_time >= datetime('now', '-90 days')
                GROUP BY email
                HAVING COUNT(*) >= 2 AND latest_size > earliest_size
            )
            ORDER BY (latest_size - earliest_size) DESC;" > "$export_file" 2>/dev/null
            
            echo -e "${GREEN}âœ“ Top growth accounts exported to: $export_file${NC}"
            
            # Show summary
            local total_accounts=$(wc -l < "$export_file")
            total_accounts=$((total_accounts - 1))  # Subtract header
            echo "Total growing accounts: $total_accounts"
            ;;
        *)
            echo -e "${RED}Invalid choice.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Stub functions for menu items (to be implemented)
account_search_diagnostics_menu() {
    render_menu "account_analysis"
}

# Search accounts by email address
search_accounts_by_email() {
    echo -e "${CYAN}Search Accounts by Email Address${NC}"
    echo ""
    read -p "Enter email address or partial email: " email_search
    echo ""
    
    if [[ -z "$email_search" ]]; then
        echo -e "${RED}Search term cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}ðŸ“§ Search Results for: '$email_search'${NC}"
    echo ""
    
    # Search in database first
    local db_results=$(sqlite3 local-config/gwombat.db "
        SELECT email, current_stage, ou_path, updated_at 
        FROM accounts 
        WHERE email LIKE '%$email_search%' 
        ORDER BY email;
    " 2>/dev/null)
    
    if [[ -n "$db_results" ]]; then
        echo -e "${YELLOW}=== Database Results ===${NC}"
        echo "$db_results" | while IFS='|' read -r email stage ou updated; do
            echo -e "${GREEN}Email:${NC} $email"
            echo -e "${GREEN}Stage:${NC} $stage"
            echo -e "${GREEN}OU:${NC} $ou"
            echo -e "${GREEN}Updated:${NC} $updated"
            echo ""
        done
    fi
    
    # Search with GAM for live data
    echo -e "${YELLOW}=== Live GAM Search ===${NC}"
    local gam_results=$($GAM print users query "$email_search" fields primaryemail,name,orgunitpath,suspended 2>/dev/null | tail -n +2)
    
    if [[ -n "$gam_results" ]]; then
        echo "$gam_results" | while IFS=',' read -r email name ou suspended; do
            echo -e "${GREEN}Email:${NC} $email"
            echo -e "${GREEN}Name:${NC} $name"
            echo -e "${GREEN}OU:${NC} $ou"
            echo -e "${GREEN}Status:${NC} $([ "$suspended" = "True" ] && echo "Suspended" || echo "Active")"
            echo ""
        done
    else
        echo "No results found in GAM."
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Search accounts by display name
search_accounts_by_name() {
    echo -e "${CYAN}Search Accounts by Display Name${NC}"
    echo ""
    read -p "Enter display name or partial name: " name_search
    echo ""
    
    if [[ -z "$name_search" ]]; then
        echo -e "${RED}Search term cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}ðŸ‘¤ Search Results for: '$name_search'${NC}"
    echo ""
    
    # Search with GAM for live data
    local gam_results=$($GAM print users query "givenName:$name_search OR familyName:$name_search OR name:$name_search" fields primaryemail,name,orgunitpath,suspended 2>/dev/null | tail -n +2)
    
    if [[ -n "$gam_results" ]]; then
        echo "$gam_results" | while IFS=',' read -r email name ou suspended; do
            echo -e "${GREEN}Email:${NC} $email"
            echo -e "${GREEN}Name:${NC} $name"
            echo -e "${GREEN}OU:${NC} $ou"
            echo -e "${GREEN}Status:${NC} $([ "$suspended" = "True" ] && echo "Suspended" || echo "Active")"
            
            # Check if we have additional info in database
            local db_info=$(sqlite3 local-config/gwombat.db "
                SELECT current_stage, updated_at 
                FROM accounts 
                WHERE email = '$email';
            " 2>/dev/null)
            
            if [[ -n "$db_info" ]]; then
                local stage=$(echo "$db_info" | cut -d'|' -f1)
                local updated=$(echo "$db_info" | cut -d'|' -f2)
                echo -e "${GREEN}Lifecycle Stage:${NC} $stage"
                echo -e "${GREEN}Last Updated:${NC} $updated"
            fi
            echo ""
        done
    else
        echo "No results found."
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Search accounts by organizational unit
search_accounts_by_ou() {
    echo -e "${CYAN}Search Accounts by Organizational Unit${NC}"
    echo ""
    echo "Common OUs:"
    echo "1. /Students"
    echo "2. /Faculty"
    echo "3. /Staff"
    echo "4. /Suspended Users"
    echo "5. /Suspended Users/Pending Deletion"
    echo "6. Custom OU path"
    echo ""
    read -p "Select OU option (1-6): " ou_choice
    echo ""
    
    local ou_path=""
    case $ou_choice in
        1) ou_path="/Students" ;;
        2) ou_path="/Faculty" ;;
        3) ou_path="/Staff" ;;
        4) ou_path="/Suspended Users" ;;
        5) ou_path="/Suspended Users/Pending Deletion" ;;
        6) 
            read -p "Enter custom OU path: " ou_path
            ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    if [[ -z "$ou_path" ]]; then
        echo -e "${RED}OU path cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}ðŸ¢ Accounts in OU: '$ou_path'${NC}"
    echo ""
    
    # Search with GAM
    local gam_results=$($GAM print users query "orgUnitPath='$ou_path'" fields primaryemail,name,suspended,creationtime 2>/dev/null | tail -n +2)
    
    if [[ -n "$gam_results" ]]; then
        local count=0
        echo "$gam_results" | while IFS=',' read -r email name suspended created; do
            ((count++))
            echo -e "${GREEN}$count. Email:${NC} $email"
            echo -e "   ${GREEN}Name:${NC} $name"
            echo -e "   ${GREEN}Status:${NC} $([ "$suspended" = "True" ] && echo "Suspended" || echo "Active")"
            echo -e "   ${GREEN}Created:${NC} $created"
            echo ""
        done
        
        echo -e "${YELLOW}Total accounts found: $(echo "$gam_results" | wc -l)${NC}"
    else
        echo "No accounts found in this OU."
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Account health diagnostics
account_health_diagnostics() {
    echo -e "${CYAN}Account Health Diagnostics${NC}"
    echo ""
    read -p "Enter email address to diagnose: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}ðŸ” Diagnosing Account: $email${NC}"
    echo ""
    
    # Check if account exists in GAM
    local gam_info=$($GAM info user "$email" 2>/dev/null)
    
    if [[ -z "$gam_info" ]]; then
        echo -e "${RED}âŒ Account not found in Google Workspace${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${GREEN}âœ… Account exists in Google Workspace${NC}"
    echo ""
    
    # Extract key information
    local suspended=$(echo "$gam_info" | grep -i "suspended:" | cut -d':' -f2 | xargs)
    local ou_path=$(echo "$gam_info" | grep -i "org unit path:" | cut -d':' -f2- | xargs)
    local last_login=$(echo "$gam_info" | grep -i "last login time:" | cut -d':' -f2- | xargs)
    local creation_time=$(echo "$gam_info" | grep -i "creation time:" | cut -d':' -f2- | xargs)
    local storage_used=$(echo "$gam_info" | grep -i "storage used:" | cut -d':' -f2- | xargs)
    
    # Health checks
    echo -e "${YELLOW}=== Health Status ===${NC}"
    
    # Suspension status
    if [[ "$suspended" = "True" ]]; then
        echo -e "${RED}âš ï¸  Account is suspended${NC}"
    else
        echo -e "${GREEN}âœ… Account is active${NC}"
    fi
    
    # OU placement
    echo -e "${GREEN}ðŸ“ Organizational Unit:${NC} $ou_path"
    
    # Login activity
    if [[ -n "$last_login" && "$last_login" != "Never" ]]; then
        echo -e "${GREEN}ðŸ” Last Login:${NC} $last_login"
        
        # Check if login is recent (within 30 days)
        local login_date=$(date -d "$last_login" +%s 2>/dev/null)
        local thirty_days_ago=$(date -d "30 days ago" +%s)
        
        if [[ -n "$login_date" && $login_date -gt $thirty_days_ago ]]; then
            echo -e "${GREEN}âœ… Recent login activity${NC}"
        else
            echo -e "${YELLOW}âš ï¸  No recent login activity (>30 days)${NC}"
        fi
    else
        echo -e "${YELLOW}âš ï¸  No login activity recorded${NC}"
    fi
    
    # Storage usage
    if [[ -n "$storage_used" ]]; then
        echo -e "${GREEN}ðŸ’¾ Storage Used:${NC} $storage_used"
    fi
    
    # Check database information
    local db_info=$(sqlite3 local-config/gwombat.db "
        SELECT current_stage, updated_at 
        FROM accounts 
        WHERE email = '$email';
    " 2>/dev/null)
    
    if [[ -n "$db_info" ]]; then
        echo ""
        echo -e "${YELLOW}=== Database Information ===${NC}"
        local stage=$(echo "$db_info" | cut -d'|' -f1)
        local updated=$(echo "$db_info" | cut -d'|' -f2)
        echo -e "${GREEN}Lifecycle Stage:${NC} $stage"
        echo -e "${GREEN}Last Updated:${NC} $updated"
        
        # Check for data consistency
        if [[ "$suspended" = "True" && "$stage" = "active" ]]; then
            echo -e "${RED}âš ï¸  Data inconsistency: Account suspended but marked active in database${NC}"
        elif [[ "$suspended" != "True" && "$stage" != "active" ]]; then
            echo -e "${YELLOW}âš ï¸  Data inconsistency: Account active but marked $stage in database${NC}"
        else
            echo -e "${GREEN}âœ… Data consistency check passed${NC}"
        fi
    else
        echo -e "${YELLOW}â„¹ï¸  Account not found in local database${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Detailed account information
detailed_account_information() {
    echo -e "${CYAN}Detailed Account Information${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}ðŸ“‹ Complete Information for: $email${NC}"
    echo ""
    
    # Get comprehensive GAM information
    echo -e "${YELLOW}=== GAM Information ===${NC}"
    $GAM info user "$email" 2>/dev/null || echo "Account not found in GAM"
    
    echo ""
    echo -e "${YELLOW}=== Database Information ===${NC}"
    sqlite3 local-config/gwombat.db -header "
        SELECT * FROM accounts WHERE email = '$email';
    " 2>/dev/null || echo "Account not found in database"
    
    echo ""
    echo -e "${YELLOW}=== Storage Information ===${NC}"
    sqlite3 local-config/gwombat.db -header "
        SELECT 
            storage_used_gb,
            storage_quota_gb,
            usage_percentage,
            measurement_date
        FROM account_storage_sizes 
        WHERE email = '$email'
        ORDER BY measurement_date DESC 
        LIMIT 5;
    " 2>/dev/null || echo "No storage data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

# Account status verification
account_status_verification() {
    echo -e "${CYAN}Account Status Verification${NC}"
    echo ""
    read -p "Enter email address to verify: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}ðŸ”„ Verifying Status for: $email${NC}"
    echo ""
    
    # Get current GAM status
    local gam_suspended=$($GAM info user "$email" 2>/dev/null | grep -i "suspended:" | cut -d':' -f2 | xargs)
    local gam_ou=$($GAM info user "$email" 2>/dev/null | grep -i "org unit path:" | cut -d':' -f2- | xargs)
    
    # Get database status
    local db_info=$(sqlite3 local-config/gwombat.db "
        SELECT current_stage, ou_path 
        FROM accounts 
        WHERE email = '$email';
    " 2>/dev/null)
    
    if [[ -z "$gam_suspended" ]]; then
        echo -e "${RED}âŒ Account not found in GAM${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${YELLOW}=== Status Comparison ===${NC}"
    echo -e "${GREEN}GAM Suspended:${NC} $gam_suspended"
    echo -e "${GREEN}GAM OU:${NC} $gam_ou"
    
    if [[ -n "$db_info" ]]; then
        local db_stage=$(echo "$db_info" | cut -d'|' -f1)
        local db_ou=$(echo "$db_info" | cut -d'|' -f2)
        echo -e "${GREEN}DB Stage:${NC} $db_stage"
        echo -e "${GREEN}DB OU:${NC} $db_ou"
        
        # Verification logic
        echo ""
        echo -e "${YELLOW}=== Verification Results ===${NC}"
        
        if [[ "$gam_suspended" = "True" ]]; then
            if [[ "$db_stage" != "active" ]]; then
                echo -e "${GREEN}âœ… Suspension status matches database stage${NC}"
            else
                echo -e "${RED}âŒ Account suspended in GAM but marked active in database${NC}"
            fi
        else
            if [[ "$db_stage" = "active" ]]; then
                echo -e "${GREEN}âœ… Active status matches database stage${NC}"
            else
                echo -e "${YELLOW}âš ï¸  Account active in GAM but marked $db_stage in database${NC}"
            fi
        fi
        
        if [[ "$gam_ou" = "$db_ou" ]]; then
            echo -e "${GREEN}âœ… OU placement matches database${NC}"
        else
            echo -e "${YELLOW}âš ï¸  OU mismatch - GAM: '$gam_ou', DB: '$db_ou'${NC}"
        fi
    else
        echo -e "${YELLOW}âš ï¸  Account not found in database${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Account activity analysis
account_activity_analysis() {
    echo -e "${CYAN}Account Activity Analysis${NC}"
    echo ""
    read -p "Enter email address to analyze: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}ðŸ“ˆ Activity Analysis for: $email${NC}"
    echo ""
    
    # Get login activity
    local login_info=$($GAM info user "$email" 2>/dev/null | grep -i "last login time:")
    if [[ -n "$login_info" ]]; then
        echo -e "${GREEN}Last Login:${NC} $(echo "$login_info" | cut -d':' -f2- | xargs)"
    fi
    
    # Check for file activity (if we have sharing analysis data)
    local file_activity=$(sqlite3 local-config/gwombat.db "
        SELECT COUNT(*) 
        FROM file_sharing_analysis 
        WHERE email = '$email';
    " 2>/dev/null)
    
    if [[ -n "$file_activity" && "$file_activity" -gt 0 ]]; then
        echo -e "${GREEN}File Sharing Records:${NC} $file_activity"
    fi
    
    # Check stage history
    local stage_history=$(sqlite3 local-config/gwombat.db "
        SELECT stage_name, changed_at 
        FROM stage_history 
        WHERE email = '$email' 
        ORDER BY changed_at DESC 
        LIMIT 5;
    " 2>/dev/null)
    
    if [[ -n "$stage_history" ]]; then
        echo ""
        echo -e "${YELLOW}=== Recent Stage Changes ===${NC}"
        echo "$stage_history" | while IFS='|' read -r stage date; do
            echo -e "${GREEN}$date:${NC} $stage"
        done
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Quick account lookup
quick_account_lookup() {
    echo -e "${CYAN}Quick Account Lookup${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}âš¡ Quick Info for: $email${NC}"
    echo ""
    
    # Quick GAM check
    local quick_info=$($GAM info user "$email" fields primaryemail,name,suspended,orgunitpath 2>/dev/null)
    
    if [[ -n "$quick_info" ]]; then
        echo "$quick_info" | grep -E "(Email|Full Name|Suspended|Org Unit Path):"
    else
        echo -e "${RED}Account not found${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

individual_user_management_menu() {
    render_menu "user_group_management"
}

# Modify user details and settings
modify_user_details() {
    echo -e "${CYAN}Modify User Details and Settings${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Verify user exists
    if ! $GAM info user "$email" >/dev/null 2>&1; then
        echo -e "${RED}User not found: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}Current User Information:${NC}"
    $GAM info user "$email" fields name,primaryemail,recoveryemail
    echo ""
    
    echo "What would you like to modify?"
    echo "1. First name"
    echo "2. Last name"  
    echo "3. Recovery email"
    echo "4. User title/position"
    echo "5. Phone number"
    echo "6. Department"
    echo ""
    read -p "Select option (1-6): " modify_choice
    echo ""
    
    case $modify_choice in
        1)
            read -p "Enter new first name: " first_name
            if [[ -n "$first_name" ]]; then
                echo -e "${YELLOW}Updating first name to: $first_name${NC}"
                $GAM update user "$email" givenname "$first_name"
                echo -e "${GREEN}âœ… First name updated${NC}"
            fi
            ;;
        2)
            read -p "Enter new last name: " last_name
            if [[ -n "$last_name" ]]; then
                echo -e "${YELLOW}Updating last name to: $last_name${NC}"
                $GAM update user "$email" familyname "$last_name"
                echo -e "${GREEN}âœ… Last name updated${NC}"
            fi
            ;;
        3)
            read -p "Enter new recovery email: " recovery_email
            if [[ -n "$recovery_email" ]]; then
                echo -e "${YELLOW}Updating recovery email to: $recovery_email${NC}"
                $GAM update user "$email" recoveryemail "$recovery_email"
                echo -e "${GREEN}âœ… Recovery email updated${NC}"
            fi
            ;;
        4)
            read -p "Enter title/position: " title
            if [[ -n "$title" ]]; then
                echo -e "${YELLOW}Updating title to: $title${NC}"
                $GAM update user "$email" title "$title"
                echo -e "${GREEN}âœ… Title updated${NC}"
            fi
            ;;
        5)
            read -p "Enter phone number: " phone
            if [[ -n "$phone" ]]; then
                echo -e "${YELLOW}Updating phone number to: $phone${NC}"
                $GAM update user "$email" phone "$phone"
                echo -e "${GREEN}âœ… Phone number updated${NC}"
            fi
            ;;
        6)
            read -p "Enter department: " department
            if [[ -n "$department" ]]; then
                echo -e "${YELLOW}Updating department to: $department${NC}"
                $GAM update user "$email" department "$department"
                echo -e "${GREEN}âœ… Department updated${NC}"
            fi
            ;;
        *)
            echo -e "${RED}Invalid option.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Reset user password
reset_user_password() {
    echo -e "${CYAN}Reset User Password${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Verify user exists
    if ! $GAM info user "$email" >/dev/null 2>&1; then
        echo -e "${RED}User not found: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "Password reset options:"
    echo "1. Generate random password"
    echo "2. Set specific password"
    echo "3. Reset password and force change on next login"
    echo ""
    read -p "Select option (1-3): " pwd_choice
    echo ""
    
    case $pwd_choice in
        1)
            echo -e "${YELLOW}Generating random password...${NC}"
            local new_password=$(openssl rand -base64 12)
            $GAM update user "$email" password "$new_password"
            echo -e "${GREEN}âœ… Password reset successfully${NC}"
            echo -e "${YELLOW}New password: $new_password${NC}"
            echo -e "${RED}âš ï¸  Please securely share this password with the user${NC}"
            ;;
        2)
            read -s -p "Enter new password: " new_password
            echo ""
            read -s -p "Confirm password: " confirm_password
            echo ""
            
            if [[ "$new_password" = "$confirm_password" ]]; then
                $GAM update user "$email" password "$new_password"
                echo -e "${GREEN}âœ… Password updated successfully${NC}"
            else
                echo -e "${RED}âŒ Passwords do not match${NC}"
            fi
            ;;
        3)
            echo -e "${YELLOW}Generating temporary password and forcing change...${NC}"
            local temp_password=$(openssl rand -base64 12)
            $GAM update user "$email" password "$temp_password" changepasswordatnextlogin on
            echo -e "${GREEN}âœ… Temporary password set with forced change${NC}"
            echo -e "${YELLOW}Temporary password: $temp_password${NC}"
            echo -e "${RED}âš ï¸  User will be required to change password on next login${NC}"
            ;;
        *)
            echo -e "${RED}Invalid option.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Reset 2FA and app passwords
reset_user_2fa() {
    echo -e "${CYAN}Reset 2FA and App Passwords${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Verify user exists
    if ! $GAM info user "$email" >/dev/null 2>&1; then
        echo -e "${RED}User not found: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "2FA/Security reset options:"
    echo "1. Reset 2-Step Verification"
    echo "2. Reset App-Specific Passwords"
    echo "3. Reset both 2FA and App Passwords"
    echo ""
    read -p "Select option (1-3): " reset_choice
    echo ""
    
    case $reset_choice in
        1)
            echo -e "${YELLOW}Resetting 2-Step Verification...${NC}"
            $GAM user "$email" turnoff2sv
            echo -e "${GREEN}âœ… 2-Step Verification reset${NC}"
            echo -e "${YELLOW}â„¹ï¸  User can now set up 2FA again${NC}"
            ;;
        2)
            echo -e "${YELLOW}Resetting App-Specific Passwords...${NC}"
            $GAM user "$email" deprovision
            echo -e "${GREEN}âœ… App-Specific Passwords reset${NC}"
            echo -e "${YELLOW}â„¹ï¸  User will need to generate new app passwords${NC}"
            ;;
        3)
            echo -e "${YELLOW}Resetting both 2FA and App Passwords...${NC}"
            $GAM user "$email" turnoff2sv
            $GAM user "$email" deprovision
            echo -e "${GREEN}âœ… 2FA and App Passwords reset${NC}"
            echo -e "${YELLOW}â„¹ï¸  User can set up 2FA and app passwords again${NC}"
            ;;
        *)
            echo -e "${RED}Invalid option.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Change user organizational unit
change_user_ou() {
    echo -e "${CYAN}Change User Organizational Unit${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Verify user exists and show current OU
    local current_ou=$($GAM info user "$email" 2>/dev/null | grep -i "org unit path:" | cut -d':' -f2- | xargs)
    
    if [[ -z "$current_ou" ]]; then
        echo -e "${RED}User not found: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${GREEN}Current OU:${NC} $current_ou"
    echo ""
    
    echo "Select new organizational unit:"
    echo "1. /Students"
    echo "2. /Faculty"
    echo "3. /Staff"
    echo "4. /Suspended Users"
    echo "5. /Suspended Users/Pending Deletion"
    echo "6. /Suspended Users/Temporary Hold"
    echo "7. /Suspended Users/Exit Row"
    echo "8. Custom OU path"
    echo ""
    read -p "Select OU option (1-8): " ou_choice
    echo ""
    
    local new_ou=""
    case $ou_choice in
        1) new_ou="/Students" ;;
        2) new_ou="/Faculty" ;;
        3) new_ou="/Staff" ;;
        4) new_ou="/Suspended Users" ;;
        5) new_ou="/Suspended Users/Pending Deletion" ;;
        6) new_ou="/Suspended Users/Temporary Hold" ;;
        7) new_ou="/Suspended Users/Exit Row" ;;
        8) 
            read -p "Enter custom OU path: " new_ou
            ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    if [[ -z "$new_ou" ]]; then
        echo -e "${RED}OU path cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    if [[ "$new_ou" = "$current_ou" ]]; then
        echo -e "${YELLOW}User is already in that OU.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${YELLOW}Moving user from '$current_ou' to '$new_ou'${NC}"
    read -p "Confirm this change? (y/N): " confirm
    
    if [[ "$confirm" =~ ^[Yy]$ ]]; then
        $GAM update user "$email" ou "$new_ou"
        echo -e "${GREEN}âœ… User moved to $new_ou${NC}"
        
        # Update database if account exists there
        sqlite3 local-config/gwombat.db "
            UPDATE accounts 
            SET ou_path = '$new_ou', updated_at = CURRENT_TIMESTAMP 
            WHERE email = '$email';
        " 2>/dev/null
        
        if [[ $? -eq 0 ]]; then
            echo -e "${GREEN}âœ… Database updated${NC}"
        fi
    else
        echo -e "${YELLOW}Operation cancelled${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Update user aliases
update_user_aliases() {
    echo -e "${CYAN}Update User Aliases${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Verify user exists and show current aliases
    if ! $GAM info user "$email" >/dev/null 2>&1; then
        echo -e "${RED}User not found: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${GREEN}Current aliases for $email:${NC}"
    $GAM info user "$email" | grep -i "email aliases:" || echo "No aliases found"
    echo ""
    
    echo "Alias operations:"
    echo "1. Add new alias"
    echo "2. Remove existing alias"
    echo "3. List all aliases"
    echo ""
    read -p "Select option (1-3): " alias_choice
    echo ""
    
    case $alias_choice in
        1)
            read -p "Enter new alias email: " new_alias
            if [[ -n "$new_alias" ]]; then
                echo -e "${YELLOW}Adding alias: $new_alias${NC}"
                $GAM create alias "$new_alias" user "$email"
                echo -e "${GREEN}âœ… Alias added${NC}"
            fi
            ;;
        2)
            read -p "Enter alias to remove: " remove_alias
            if [[ -n "$remove_alias" ]]; then
                echo -e "${YELLOW}Removing alias: $remove_alias${NC}"
                $GAM delete alias "$remove_alias"
                echo -e "${GREEN}âœ… Alias removed${NC}"
            fi
            ;;
        3)
            echo -e "${CYAN}All aliases for $email:${NC}"
            $GAM print aliases | grep "$email" || echo "No aliases found"
            ;;
        *)
            echo -e "${RED}Invalid option.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Manage user groups
manage_user_groups() {
    echo -e "${CYAN}Manage User Group Memberships${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Verify user exists
    if ! $GAM info user "$email" >/dev/null 2>&1; then
        echo -e "${RED}User not found: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "Group operations:"
    echo "1. Show current group memberships"
    echo "2. Add user to group"
    echo "3. Remove user from group"
    echo ""
    read -p "Select option (1-3): " group_choice
    echo ""
    
    case $group_choice in
        1)
            echo -e "${CYAN}Current group memberships for $email:${NC}"
            $GAM print group-members | grep "$email" || echo "No group memberships found"
            ;;
        2)
            read -p "Enter group email to add user to: " group_email
            if [[ -n "$group_email" ]]; then
                echo -e "${YELLOW}Adding $email to group $group_email${NC}"
                $GAM update group "$group_email" add member user "$email"
                echo -e "${GREEN}âœ… User added to group${NC}"
            fi
            ;;
        3)
            read -p "Enter group email to remove user from: " group_email
            if [[ -n "$group_email" ]]; then
                echo -e "${YELLOW}Removing $email from group $group_email${NC}"
                $GAM update group "$group_email" remove member "$email"
                echo -e "${GREEN}âœ… User removed from group${NC}"
            fi
            ;;
        *)
            echo -e "${RED}Invalid option.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Manage user licenses
manage_user_licenses() {
    echo -e "${CYAN}Manage User Licenses${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Verify user exists
    if ! $GAM info user "$email" >/dev/null 2>&1; then
        echo -e "${RED}User not found: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "License operations:"
    echo "1. Show current licenses"
    echo "2. Add Google Workspace license"
    echo "3. Remove license"
    echo ""
    read -p "Select option (1-3): " license_choice
    echo ""
    
    case $license_choice in
        1)
            echo -e "${CYAN}Current licenses for $email:${NC}"
            $GAM print licenses | grep "$email" || echo "No licenses found"
            ;;
        2)
            echo "Available license types:"
            echo "1. Google Workspace Business Starter"
            echo "2. Google Workspace Business Standard"
            echo "3. Google Workspace Business Plus"
            echo "4. Google Workspace Enterprise"
            read -p "Select license type (1-4): " license_type
            
            local sku=""
            case $license_type in
                1) sku="1010020020" ;;  # Business Starter
                2) sku="1010020025" ;;  # Business Standard
                3) sku="1010020026" ;;  # Business Plus
                4) sku="1010020028" ;;  # Enterprise
                *) echo -e "${RED}Invalid license type${NC}"; return ;;
            esac
            
            echo -e "${YELLOW}Adding license to $email${NC}"
            $GAM insert license "$email" sku "$sku"
            echo -e "${GREEN}âœ… License added${NC}"
            ;;
        3)
            read -p "Enter SKU to remove: " sku
            if [[ -n "$sku" ]]; then
                echo -e "${YELLOW}Removing license from $email${NC}"
                $GAM delete license "$email" sku "$sku"
                echo -e "${GREEN}âœ… License removed${NC}"
            fi
            ;;
        *)
            echo -e "${RED}Invalid option.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Suspend/restore user
suspend_restore_user() {
    echo -e "${CYAN}Suspend/Restore User Account${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Check current status
    local suspended=$($GAM info user "$email" 2>/dev/null | grep -i "suspended:" | cut -d':' -f2 | xargs)
    
    if [[ -z "$suspended" ]]; then
        echo -e "${RED}User not found: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${GREEN}Current status:${NC} $([ "$suspended" = "True" ] && echo "Suspended" || echo "Active")"
    echo ""
    
    if [[ "$suspended" = "True" ]]; then
        echo "User is currently suspended."
        read -p "Restore this user? (y/N): " confirm
        
        if [[ "$confirm" =~ ^[Yy]$ ]]; then
            echo -e "${YELLOW}Restoring user account...${NC}"
            $GAM update user "$email" suspended off
            echo -e "${GREEN}âœ… User account restored${NC}"
            
            # Update database
            sqlite3 local-config/gwombat.db "
                UPDATE accounts 
                SET current_stage = 'active', updated_at = CURRENT_TIMESTAMP 
                WHERE email = '$email';
            " 2>/dev/null
        fi
    else
        echo "User is currently active."
        read -p "Suspend this user? (y/N): " confirm
        
        if [[ "$confirm" =~ ^[Yy]$ ]]; then
            read -p "Enter suspension reason (optional): " reason
            echo -e "${YELLOW}Suspending user account...${NC}"
            
            if [[ -n "$reason" ]]; then
                $GAM update user "$email" suspended on suspensionReason "$reason"
            else
                $GAM update user "$email" suspended on
            fi
            
            echo -e "${GREEN}âœ… User account suspended${NC}"
            
            # Update database
            sqlite3 local-config/gwombat.db "
                UPDATE accounts 
                SET current_stage = 'recently_suspended', updated_at = CURRENT_TIMESTAMP 
                WHERE email = '$email';
            " 2>/dev/null
        fi
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# View user information
view_user_information() {
    echo -e "${CYAN}View User Information${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo -e "${CYAN}Complete User Information for: $email${NC}"
    echo ""
    
    # GAM information
    echo -e "${YELLOW}=== Google Workspace Information ===${NC}"
    $GAM info user "$email" 2>/dev/null || echo "User not found in GAM"
    
    echo ""
    echo -e "${YELLOW}=== Group Memberships ===${NC}"
    $GAM print group-members | grep "$email" || echo "No group memberships"
    
    echo ""
    echo -e "${YELLOW}=== License Information ===${NC}"
    $GAM print licenses | grep "$email" || echo "No licenses assigned"
    
    echo ""
    read -p "Press Enter to continue..."
}

# Sync user to database
sync_user_to_database() {
    echo -e "${CYAN}Sync User to Database${NC}"
    echo ""
    read -p "Enter email address: " email
    echo ""
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Get user info from GAM
    local user_info=$($GAM info user "$email" 2>/dev/null)
    
    if [[ -z "$user_info" ]]; then
        echo -e "${RED}User not found in GAM: $email${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Extract information
    local suspended=$(echo "$user_info" | grep -i "suspended:" | cut -d':' -f2 | xargs)
    local ou_path=$(echo "$user_info" | grep -i "org unit path:" | cut -d':' -f2- | xargs)
    local full_name=$(echo "$user_info" | grep -i "full name:" | cut -d':' -f2- | xargs)
    
    # Determine stage based on suspension status and OU
    local stage="active"
    if [[ "$suspended" = "True" ]]; then
        if [[ "$ou_path" =~ "Pending Deletion" ]]; then
            stage="pending_deletion"
        elif [[ "$ou_path" =~ "Temporary Hold" ]]; then
            stage="temporary_hold"
        elif [[ "$ou_path" =~ "Exit Row" ]]; then
            stage="exit_row"
        else
            stage="recently_suspended"
        fi
    fi
    
    echo -e "${YELLOW}Syncing user data:${NC}"
    echo "  Email: $email"
    echo "  Name: $full_name"
    echo "  OU: $ou_path"
    echo "  Stage: $stage"
    echo "  Suspended: $suspended"
    echo ""
    
    # Insert or update in database
    sqlite3 local-config/gwombat.db "
        INSERT OR REPLACE INTO accounts (
            email, full_name, current_stage, ou_path, 
            suspended, updated_at, synced_at
        ) VALUES (
            '$email', 
            '$(echo "$full_name" | sed "s/'/''/g")', 
            '$stage', 
            '$(echo "$ou_path" | sed "s/'/''/g")', 
            '$([ "$suspended" = "True" ] && echo "1" || echo "0")', 
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        );
    " 2>/dev/null
    
    if [[ $? -eq 0 ]]; then
        echo -e "${GREEN}âœ… User synced to database successfully${NC}"
    else
        echo -e "${RED}âŒ Error syncing user to database${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

bulk_user_operations_menu() {
    render_menu "user_group_management"
}

# Bulk user creation from CSV
bulk_user_creation_from_csv() {
    echo -e "${CYAN}Bulk User Creation from CSV${NC}"
    echo ""
    echo "CSV format should include: email,firstName,lastName,orgUnit,password"
    echo "Example: john.doe@domain.edu,John,Doe,/Students,TempPass123"
    echo ""
    read -p "Enter path to CSV file: " csv_file
    echo ""
    
    if [[ ! -f "$csv_file" ]]; then
        echo -e "${RED}CSV file not found: $csv_file${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Validate CSV format
    local header=$(head -1 "$csv_file")
    if [[ ! "$header" =~ email.*firstName.*lastName ]]; then
        echo -e "${YELLOW}âš ï¸  CSV header doesn't match expected format${NC}"
        echo "Expected: email,firstName,lastName,orgUnit,password"
        echo "Found: $header"
        read -p "Continue anyway? (y/N): " confirm
        [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    fi
    
    local total_users=$(tail -n +2 "$csv_file" | wc -l)
    echo "Found $total_users users to create"
    echo ""
    
    read -p "Proceed with bulk user creation? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local created=0
    local failed=0
    local session_id="bulk_create_$(date +%Y%m%d_%H%M%S)"
    
    echo ""
    echo -e "${YELLOW}Creating users...${NC}"
    
    tail -n +2 "$csv_file" | while IFS=',' read -r email first_name last_name ou_path password; do
        # Skip empty lines
        [[ -z "$email" ]] && continue
        
        echo -n "Creating: $email... "
        
        # Create user with GAM
        if $GAM create user "$email" firstname "$first_name" lastname "$last_name" password "$password" org "$ou_path" 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            ((created++))
            
            # Add to database
            sqlite3 local-config/gwombat.db "
                INSERT OR REPLACE INTO accounts (
                    email, full_name, current_stage, ou_path, 
                    suspended, created_at, updated_at, session_id
                ) VALUES (
                    '$email', 
                    '$(echo "$first_name $last_name" | sed "s/'/''/g")', 
                    'active', 
                    '$(echo "$ou_path" | sed "s/'/''/g")', 
                    0, 
                    CURRENT_TIMESTAMP,
                    CURRENT_TIMESTAMP,
                    '$session_id'
                );
            " 2>/dev/null
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done
    
    echo ""
    echo -e "${GREEN}âœ… Bulk creation completed${NC}"
    echo "   Created: $created users"
    echo "   Failed: $failed users"
    echo "   Session ID: $session_id"
    echo ""
    read -p "Press Enter to continue..."
}

# Bulk organizational unit moves
bulk_organizational_unit_moves() {
    echo -e "${CYAN}Bulk Organizational Unit Moves${NC}"
    echo ""
    echo "Move multiple users to a new organizational unit"
    echo ""
    
    echo "Source options:"
    echo "1. From CSV file (email,newOU)"
    echo "2. From current OU to new OU"
    echo "3. From account list in database"
    echo ""
    read -p "Select source (1-3): " source_choice
    echo ""
    
    case $source_choice in
        1) bulk_ou_moves_from_csv ;;
        2) bulk_ou_moves_by_current_ou ;;
        3) bulk_ou_moves_from_list ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Bulk OU moves from CSV
bulk_ou_moves_from_csv() {
    read -p "Enter path to CSV file (email,newOU): " csv_file
    
    if [[ ! -f "$csv_file" ]]; then
        echo -e "${RED}CSV file not found: $csv_file${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local total_moves=$(tail -n +2 "$csv_file" | wc -l)
    echo "Found $total_moves OU moves to process"
    echo ""
    
    read -p "Proceed with bulk OU moves? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local moved=0
    local failed=0
    
    tail -n +2 "$csv_file" | while IFS=',' read -r email new_ou; do
        [[ -z "$email" || -z "$new_ou" ]] && continue
        
        echo -n "Moving $email to $new_ou... "
        
        if $GAM update user "$email" ou "$new_ou" 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            ((moved++))
            
            # Update database
            sqlite3 local-config/gwombat.db "
                UPDATE accounts 
                SET ou_path = '$new_ou', updated_at = CURRENT_TIMESTAMP 
                WHERE email = '$email';
            " 2>/dev/null
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done
    
    echo ""
    echo "Moved: $moved, Failed: $failed"
    read -p "Press Enter to continue..."
}

# Bulk OU moves by current OU
bulk_ou_moves_by_current_ou() {
    read -p "Enter current OU path: " current_ou
    read -p "Enter new OU path: " new_ou
    
    if [[ -z "$current_ou" || -z "$new_ou" ]]; then
        echo -e "${RED}Both OU paths are required.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo "Finding users in OU: $current_ou"
    
    local users=$($GAM print users query "orgUnitPath='$current_ou'" fields primaryemail 2>/dev/null | tail -n +2 | cut -d',' -f1)
    local user_count=$(echo "$users" | wc -l)
    
    echo "Found $user_count users to move"
    echo ""
    
    read -p "Move all users from '$current_ou' to '$new_ou'? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local moved=0
    local failed=0
    
    echo "$users" | while read -r email; do
        [[ -z "$email" ]] && continue
        
        echo -n "Moving $email... "
        
        if $GAM update user "$email" ou "$new_ou" 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            ((moved++))
            
            # Update database
            sqlite3 local-config/gwombat.db "
                UPDATE accounts 
                SET ou_path = '$new_ou', updated_at = CURRENT_TIMESTAMP 
                WHERE email = '$email';
            " 2>/dev/null
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done
    
    echo ""
    echo "Moved: $moved, Failed: $failed"
    read -p "Press Enter to continue..."
}

# Bulk OU moves from account list
bulk_ou_moves_from_list() {
    echo "Available account lists:"
    sqlite3 local-config/gwombat.db -header "
        SELECT list_name, description, 
               (SELECT COUNT(*) FROM account_list_memberships WHERE list_id = al.id) as account_count
        FROM account_lists al 
        WHERE is_active = 1
        ORDER BY list_name;
    " 2>/dev/null
    
    echo ""
    read -p "Enter list name: " list_name
    read -p "Enter new OU path: " new_ou
    
    if [[ -z "$list_name" || -z "$new_ou" ]]; then
        echo -e "${RED}Both list name and OU path are required.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local users=$(sqlite3 local-config/gwombat.db "
        SELECT a.email 
        FROM accounts a
        JOIN account_list_memberships alm ON a.email = alm.email
        JOIN account_lists al ON alm.list_id = al.id
        WHERE al.list_name = '$list_name' AND al.is_active = 1;
    " 2>/dev/null)
    
    local user_count=$(echo "$users" | wc -l)
    
    if [[ $user_count -eq 0 ]]; then
        echo -e "${RED}No users found in list: $list_name${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "Found $user_count users in list '$list_name'"
    echo ""
    
    read -p "Move all users to '$new_ou'? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local moved=0
    local failed=0
    
    echo "$users" | while read -r email; do
        [[ -z "$email" ]] && continue
        
        echo -n "Moving $email... "
        
        if $GAM update user "$email" ou "$new_ou" 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            ((moved++))
            
            # Update database
            sqlite3 local-config/gwombat.db "
                UPDATE accounts 
                SET ou_path = '$new_ou', updated_at = CURRENT_TIMESTAMP 
                WHERE email = '$email';
            " 2>/dev/null
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done
    
    echo ""
    echo "Moved: $moved, Failed: $failed"
    read -p "Press Enter to continue..."
}

# Batch password resets
batch_password_resets() {
    echo -e "${CYAN}Batch Password Resets${NC}"
    echo ""
    echo "Reset passwords for multiple users"
    echo ""
    
    echo "Options:"
    echo "1. Reset from CSV file (email,password)"
    echo "2. Generate random passwords for list of emails"
    echo "3. Reset with temporary passwords (force change on login)"
    echo ""
    read -p "Select option (1-3): " reset_option
    echo ""
    
    case $reset_option in
        1) batch_password_reset_from_csv ;;
        2) batch_random_password_reset ;;
        3) batch_temporary_password_reset ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Batch password reset from CSV
batch_password_reset_from_csv() {
    read -p "Enter path to CSV file (email,password): " csv_file
    
    if [[ ! -f "$csv_file" ]]; then
        echo -e "${RED}CSV file not found: $csv_file${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local total_resets=$(tail -n +2 "$csv_file" | wc -l)
    echo "Found $total_resets password resets to process"
    echo ""
    
    read -p "Proceed with batch password resets? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local reset=0
    local failed=0
    
    tail -n +2 "$csv_file" | while IFS=',' read -r email password; do
        [[ -z "$email" || -z "$password" ]] && continue
        
        echo -n "Resetting password for $email... "
        
        if $GAM update user "$email" password "$password" 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            ((reset++))
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done
    
    echo ""
    echo "Reset: $reset, Failed: $failed"
    read -p "Press Enter to continue..."
}

# Batch random password reset
batch_random_password_reset() {
    read -p "Enter path to file with email addresses (one per line): " email_file
    
    if [[ ! -f "$email_file" ]]; then
        echo -e "${RED}Email file not found: $email_file${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local total_users=$(wc -l < "$email_file")
    echo "Found $total_users users for password reset"
    echo ""
    
    read -p "Generate random passwords for all users? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local output_file="password_reset_$(date +%Y%m%d_%H%M%S).csv"
    echo "email,new_password" > "$output_file"
    
    local reset=0
    local failed=0
    
    while read -r email; do
        [[ -z "$email" ]] && continue
        
        local new_password=$(openssl rand -base64 12)
        echo -n "Resetting password for $email... "
        
        if $GAM update user "$email" password "$new_password" 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            echo "$email,$new_password" >> "$output_file"
            ((reset++))
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done < "$email_file"
    
    echo ""
    echo "Reset: $reset, Failed: $failed"
    echo "Passwords saved to: $output_file"
    echo -e "${RED}âš ï¸  Please securely distribute passwords and delete the file when done${NC}"
    read -p "Press Enter to continue..."
}

# Batch temporary password reset
batch_temporary_password_reset() {
    read -p "Enter path to file with email addresses (one per line): " email_file
    
    if [[ ! -f "$email_file" ]]; then
        echo -e "${RED}Email file not found: $email_file${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local total_users=$(wc -l < "$email_file")
    echo "Found $total_users users for temporary password reset"
    echo ""
    
    read -p "Set temporary passwords (force change on login)? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local output_file="temp_password_reset_$(date +%Y%m%d_%H%M%S).csv"
    echo "email,temp_password" > "$output_file"
    
    local reset=0
    local failed=0
    
    while read -r email; do
        [[ -z "$email" ]] && continue
        
        local temp_password=$(openssl rand -base64 12)
        echo -n "Setting temporary password for $email... "
        
        if $GAM update user "$email" password "$temp_password" changepasswordatnextlogin on 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            echo "$email,$temp_password" >> "$output_file"
            ((reset++))
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done < "$email_file"
    
    echo ""
    echo "Reset: $reset, Failed: $failed"
    echo "Temporary passwords saved to: $output_file"
    echo -e "${YELLOW}â„¹ï¸  Users will be required to change passwords on next login${NC}"
    read -p "Press Enter to continue..."
}

# Bulk email alias management
bulk_email_alias_management() {
    echo -e "${CYAN}Bulk Email Alias Management${NC}"
    echo ""
    echo "Manage email aliases for multiple users"
    echo ""
    
    echo "Options:"
    echo "1. Add aliases from CSV (email,alias)"
    echo "2. Remove aliases from CSV (alias)"
    echo "3. Generate aliases based on pattern"
    echo ""
    read -p "Select option (1-3): " alias_option
    echo ""
    
    case $alias_option in
        1) bulk_add_aliases_from_csv ;;
        2) bulk_remove_aliases_from_csv ;;
        3) bulk_generate_aliases_by_pattern ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Add aliases from CSV
bulk_add_aliases_from_csv() {
    read -p "Enter path to CSV file (email,alias): " csv_file
    
    if [[ ! -f "$csv_file" ]]; then
        echo -e "${RED}CSV file not found: $csv_file${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local total_aliases=$(tail -n +2 "$csv_file" | wc -l)
    echo "Found $total_aliases aliases to add"
    echo ""
    
    read -p "Proceed with adding aliases? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local added=0
    local failed=0
    
    tail -n +2 "$csv_file" | while IFS=',' read -r email alias; do
        [[ -z "$email" || -z "$alias" ]] && continue
        
        echo -n "Adding alias $alias for $email... "
        
        if $GAM create alias "$alias" user "$email" 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            ((added++))
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done
    
    echo ""
    echo "Added: $added, Failed: $failed"
    read -p "Press Enter to continue..."
}

# Remove aliases from CSV
bulk_remove_aliases_from_csv() {
    read -p "Enter path to CSV file with aliases (one per line): " csv_file
    
    if [[ ! -f "$csv_file" ]]; then
        echo -e "${RED}CSV file not found: $csv_file${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local total_aliases=$(wc -l < "$csv_file")
    echo "Found $total_aliases aliases to remove"
    echo ""
    
    read -p "Proceed with removing aliases? (y/N): " confirm
    [[ ! "$confirm" =~ ^[Yy]$ ]] && return
    
    local removed=0
    local failed=0
    
    while read -r alias; do
        [[ -z "$alias" ]] && continue
        
        echo -n "Removing alias $alias... "
        
        if $GAM delete alias "$alias" 2>/dev/null; then
            echo -e "${GREEN}âœ…${NC}"
            ((removed++))
        else
            echo -e "${RED}âŒ${NC}"
            ((failed++))
        fi
    done < "$csv_file"
    
    echo ""
    echo "Removed: $removed, Failed: $failed"
    read -p "Press Enter to continue..."
}

# Generate aliases by pattern
bulk_generate_aliases_by_pattern() {
    echo "Generate aliases based on username patterns"
    echo ""
    echo "Pattern examples:"
    echo "  first.last@domain.com â†’ fl@domain.com"
    echo "  john.doe@domain.com â†’ jdoe@domain.com"
    echo ""
    
    read -p "Enter pattern type (fl=first.last, jdoe=firstlast): " pattern_type
    read -p "Enter domain for aliases: " alias_domain
    read -p "Enter path to email list file: " email_file
    
    if [[ ! -f "$email_file" ]]; then
        echo -e "${RED}Email file not found: $email_file${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    local generated=0
    local failed=0
    
    while read -r email; do
        [[ -z "$email" ]] && continue
        
        local username=$(echo "$email" | cut -d'@' -f1)
        local alias=""
        
        case $pattern_type in
            "fl")
                if [[ "$username" =~ ^([^.]+)\.([^.]+)$ ]]; then
                    local first="${BASH_REMATCH[1]:0:1}"
                    local last="${BASH_REMATCH[2]}"
                    alias="${first}${last}@${alias_domain}"
                fi
                ;;
            "jdoe")
                if [[ "$username" =~ ^([^.]+)\.([^.]+)$ ]]; then
                    local first="${BASH_REMATCH[1]:0:1}"
                    local last="${BASH_REMATCH[2]:0:3}"
                    alias="${first}${last}@${alias_domain}"
                fi
                ;;
        esac
        
        if [[ -n "$alias" ]]; then
            echo -n "Adding alias $alias for $email... "
            
            if $GAM create alias "$alias" user "$email" 2>/dev/null; then
                echo -e "${GREEN}âœ…${NC}"
                ((generated++))
            else
                echo -e "${RED}âŒ${NC}"
                ((failed++))
            fi
        fi
    done < "$email_file"
    
    echo ""
    echo "Generated: $generated, Failed: $failed"
    read -p "Press Enter to continue..."
}

# Bulk group membership operations
bulk_group_membership_operations() {
    echo -e "${CYAN}Bulk Group Membership Operations${NC}"
    echo ""
    echo "Manage group memberships for multiple users"
    echo ""
    
    echo "Options:"
    echo "1. Add users to group from CSV (email,group)"
    echo "2. Remove users from group"
    echo "3. Add all users from list to group"
    echo "4. Bulk group membership report"
    echo ""
    read -p "Select option (1-4): " group_option
    echo ""
    
    case $group_option in
        1) bulk_add_users_to_groups_csv ;;
        2) bulk_remove_users_from_group ;;
        3) bulk_add_list_to_group ;;
        4) bulk_group_membership_report ;;
        *) 
            echo -e "${RED}Invalid option.${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Stub implementations for remaining functions (to keep response manageable)
bulk_add_users_to_groups_csv() {
    echo -e "${YELLOW}Add Users to Groups from CSV - Implementation in progress${NC}"
    echo "This will allow adding users to groups based on CSV input (email,group_email)"
    read -p "Press Enter to continue..."
}

bulk_remove_users_from_group() {
    echo -e "${YELLOW}Remove Users from Group - Implementation in progress${NC}"
    echo "This will allow removing multiple users from a specified group"
    read -p "Press Enter to continue..."
}

bulk_add_list_to_group() {
    echo -e "${YELLOW}Add List to Group - Implementation in progress${NC}"
    echo "This will add all users from an account list to a specified group"
    read -p "Press Enter to continue..."
}

bulk_group_membership_report() {
    echo -e "${YELLOW}Group Membership Report - Implementation in progress${NC}"
    echo "This will generate reports on group memberships across multiple users"
    read -p "Press Enter to continue..."
}

bulk_license_management() {
    echo -e "${YELLOW}Bulk License Management - Implementation in progress${NC}"
    echo "This will allow bulk assignment/removal of Google Workspace licenses"
    read -p "Press Enter to continue..."
}

bulk_suspend_restore_operations() {
    echo -e "${YELLOW}Bulk Suspend/Restore Operations - Implementation in progress${NC}"
    echo "This will allow bulk suspension or restoration of user accounts"
    read -p "Press Enter to continue..."
}

bulk_user_synchronization() {
    echo -e "${YELLOW}Bulk User Synchronization - Implementation in progress${NC}"
    echo "This will sync multiple users from GAM to the database"
    read -p "Press Enter to continue..."
}

bulk_operations_from_lists() {
    echo -e "${YELLOW}Bulk Operations from Lists - Implementation in progress${NC}"
    echo "This will perform bulk operations on users from account lists"
    read -p "Press Enter to continue..."
}

generate_bulk_operation_templates() {
    echo -e "${CYAN}Generate Bulk Operation Templates${NC}"
    echo ""
    echo "Generate CSV templates for bulk operations"
    echo ""
    
    echo "Available templates:"
    echo "1. User creation template"
    echo "2. OU move template"
    echo "3. Password reset template"
    echo "4. Alias management template"
    echo "5. Group membership template"
    echo ""
    read -p "Select template (1-5): " template_choice
    echo ""
    
    case $template_choice in
        1)
            echo "email,firstName,lastName,orgUnit,password" > "user_creation_template.csv"
            echo "john.doe@domain.edu,John,Doe,/Students,TempPass123" >> "user_creation_template.csv"
            echo "âœ… Created: user_creation_template.csv"
            ;;
        2)
            echo "email,newOU" > "ou_move_template.csv"
            echo "john.doe@domain.edu,/Faculty" >> "ou_move_template.csv"
            echo "âœ… Created: ou_move_template.csv"
            ;;
        3)
            echo "email,password" > "password_reset_template.csv"
            echo "john.doe@domain.edu,NewPass123" >> "password_reset_template.csv"
            echo "âœ… Created: password_reset_template.csv"
            ;;
        4)
            echo "email,alias" > "alias_template.csv"
            echo "john.doe@domain.edu,jdoe@domain.edu" >> "alias_template.csv"
            echo "âœ… Created: alias_template.csv"
            ;;
        5)
            echo "email,group_email" > "group_membership_template.csv"
            echo "john.doe@domain.edu,students@domain.edu" >> "group_membership_template.csv"
            echo "âœ… Created: group_membership_template.csv"
            ;;
        *)
            echo -e "${RED}Invalid template choice.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

account_status_operations_menu() {
    render_menu "user_group_management"
}

# Account Status Operations Functions
suspend_user_account() {
    echo -e "${CYAN}Suspend User Account${NC}"
    echo ""
    read -p "Enter the email address to suspend: " email
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Error: Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Suspension Reasons:${NC}"
    echo "1. Security violation"
    echo "2. Policy violation"
    echo "3. Inactive account"
    echo "4. Administrative action"
    echo "5. Other (specify)"
    echo ""
    read -p "Select suspension reason (1-5): " reason_choice
    
    case $reason_choice in
        1) reason="Security violation" ;;
        2) reason="Policy violation" ;;
        3) reason="Inactive account" ;;
        4) reason="Administrative action" ;;
        5) 
            read -p "Enter custom reason: " custom_reason
            reason="$custom_reason"
            ;;
        *) reason="Administrative action" ;;
    esac
    
    echo ""
    echo -e "${YELLOW}Suspending account: $email${NC}"
    echo -e "${YELLOW}Reason: $reason${NC}"
    echo ""
    read -p "Are you sure? (y/N): " confirm
    
    if [[ "$confirm" =~ ^[Yy]$ ]]; then
        echo "Executing: $GAM_PATH update user $email suspended on"
        if $GAM_PATH update user "$email" suspended on; then
            echo -e "${GREEN}âœ“ Account $email has been suspended successfully.${NC}"
            
            # Log the suspension
            local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
            echo "$timestamp | SUSPEND | $email | $reason | $USER" >> "$LOGFILE"
            
            # Update database if available
            if command -v sqlite3 >/dev/null 2>&1 && [[ -f "$DATABASE_PATH" ]]; then
                sqlite3 "$DATABASE_PATH" "INSERT OR IGNORE INTO account_operations (email, operation, reason, operator, timestamp) VALUES ('$email', 'suspend', '$reason', '$USER', '$timestamp');" 2>/dev/null || true
            fi
        else
            echo -e "${RED}âœ— Failed to suspend account $email.${NC}"
        fi
    else
        echo -e "${YELLOW}Operation cancelled.${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

restore_user_account() {
    echo -e "${CYAN}Restore User Account${NC}"
    echo ""
    read -p "Enter the email address to restore: " email
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Error: Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Restoring account: $email${NC}"
    echo ""
    read -p "Are you sure? (y/N): " confirm
    
    if [[ "$confirm" =~ ^[Yy]$ ]]; then
        echo "Executing: $GAM_PATH update user $email suspended off"
        if $GAM_PATH update user "$email" suspended off; then
            echo -e "${GREEN}âœ“ Account $email has been restored successfully.${NC}"
            
            # Log the restoration
            local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
            echo "$timestamp | RESTORE | $email | Account restored | $USER" >> "$LOGFILE"
            
            # Update database if available
            if command -v sqlite3 >/dev/null 2>&1 && [[ -f "$DATABASE_PATH" ]]; then
                sqlite3 "$DATABASE_PATH" "INSERT OR IGNORE INTO account_operations (email, operation, reason, operator, timestamp) VALUES ('$email', 'restore', 'Account restored', '$USER', '$timestamp');" 2>/dev/null || true
            fi
        else
            echo -e "${RED}âœ— Failed to restore account $email.${NC}"
        fi
    else
        echo -e "${YELLOW}Operation cancelled.${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

check_account_status() {
    echo -e "${CYAN}Check Account Status${NC}"
    echo ""
    read -p "Enter the email address to check: " email
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Error: Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Checking status for: $email${NC}"
    echo ""
    
    # Get user info
    user_info=$($GAM_PATH info user "$email" 2>/dev/null)
    
    if [[ $? -eq 0 ]]; then
        echo -e "${GREEN}Account Information:${NC}"
        echo "$user_info" | grep -E "(Email|Name|Suspended|Org Unit Path|Last Login|Creation Time)"
        
        # Check if suspended
        if echo "$user_info" | grep -q "Suspended: True"; then
            echo ""
            echo -e "${RED}âš  Account is currently SUSPENDED${NC}"
        else
            echo ""
            echo -e "${GREEN}âœ“ Account is ACTIVE${NC}"
        fi
    else
        echo -e "${RED}âœ— Account not found or error retrieving information.${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

view_account_suspension_history() {
    echo -e "${CYAN}Account Suspension History${NC}"
    echo ""
    read -p "Enter the email address (or press Enter for all): " email
    
    echo ""
    echo -e "${YELLOW}Suspension History:${NC}"
    echo ""
    
    if command -v sqlite3 >/dev/null 2>&1 && [[ -f "$DATABASE_PATH" ]]; then
        if [[ -n "$email" ]]; then
            sqlite3 -header -column "$DATABASE_PATH" "SELECT timestamp, operation, reason, operator FROM account_operations WHERE email = '$email' AND operation IN ('suspend', 'restore') ORDER BY timestamp DESC LIMIT 20;" 2>/dev/null || echo "No database history available."
        else
            sqlite3 -header -column "$DATABASE_PATH" "SELECT email, timestamp, operation, reason, operator FROM account_operations WHERE operation IN ('suspend', 'restore') ORDER BY timestamp DESC LIMIT 50;" 2>/dev/null || echo "No database history available."
        fi
    else
        echo "Database not available. Checking log file..."
        if [[ -f "$LOGFILE" ]]; then
            if [[ -n "$email" ]]; then
                grep -E "SUSPEND|RESTORE" "$LOGFILE" | grep "$email" | tail -20
            else
                grep -E "SUSPEND|RESTORE" "$LOGFILE" | tail -50
            fi
        else
            echo "No log file available."
        fi
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

bulk_suspend_accounts() {
    echo -e "${CYAN}Bulk Suspend Accounts${NC}"
    echo ""
    echo "Enter email addresses (one per line, empty line to finish):"
    
    emails=()
    while true; do
        read -p "> " email
        if [[ -z "$email" ]]; then
            break
        fi
        emails+=("$email")
    done
    
    if [[ ${#emails[@]} -eq 0 ]]; then
        echo -e "${YELLOW}No email addresses entered.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Accounts to suspend:${NC}"
    printf '%s\n' "${emails[@]}"
    
    echo ""
    read -p "Select suspension reason (1-Administrative, 2-Security, 3-Policy, 4-Other): " reason_choice
    case $reason_choice in
        1) reason="Administrative action" ;;
        2) reason="Security violation" ;;
        3) reason="Policy violation" ;;
        4) 
            read -p "Enter custom reason: " custom_reason
            reason="$custom_reason"
            ;;
        *) reason="Administrative action" ;;
    esac
    
    echo ""
    read -p "Proceed with bulk suspension? (y/N): " confirm
    
    if [[ "$confirm" =~ ^[Yy]$ ]]; then
        echo ""
        echo -e "${YELLOW}Processing bulk suspension...${NC}"
        
        success_count=0
        fail_count=0
        
        for email in "${emails[@]}"; do
            echo -n "Suspending $email... "
            if $GAM_PATH update user "$email" suspended on 2>/dev/null; then
                echo -e "${GREEN}âœ“${NC}"
                ((success_count++))
                
                # Log the suspension
                local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
                echo "$timestamp | BULK_SUSPEND | $email | $reason | $USER" >> "$LOGFILE"
            else
                echo -e "${RED}âœ—${NC}"
                ((fail_count++))
            fi
        done
        
        echo ""
        echo -e "${GREEN}Bulk suspension completed:${NC}"
        echo "  Successful: $success_count"
        echo "  Failed: $fail_count"
    else
        echo -e "${YELLOW}Operation cancelled.${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

bulk_restore_accounts() {
    echo -e "${CYAN}Bulk Restore Accounts${NC}"
    echo ""
    echo "Enter email addresses (one per line, empty line to finish):"
    
    emails=()
    while true; do
        read -p "> " email
        if [[ -z "$email" ]]; then
            break
        fi
        emails+=("$email")
    done
    
    if [[ ${#emails[@]} -eq 0 ]]; then
        echo -e "${YELLOW}No email addresses entered.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Accounts to restore:${NC}"
    printf '%s\n' "${emails[@]}"
    
    echo ""
    read -p "Proceed with bulk restoration? (y/N): " confirm
    
    if [[ "$confirm" =~ ^[Yy]$ ]]; then
        echo ""
        echo -e "${YELLOW}Processing bulk restoration...${NC}"
        
        success_count=0
        fail_count=0
        
        for email in "${emails[@]}"; do
            echo -n "Restoring $email... "
            if $GAM_PATH update user "$email" suspended off 2>/dev/null; then
                echo -e "${GREEN}âœ“${NC}"
                ((success_count++))
                
                # Log the restoration
                local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
                echo "$timestamp | BULK_RESTORE | $email | Account restored | $USER" >> "$LOGFILE"
            else
                echo -e "${RED}âœ—${NC}"
                ((fail_count++))
            fi
        done
        
        echo ""
        echo -e "${GREEN}Bulk restoration completed:${NC}"
        echo "  Successful: $success_count"
        echo "  Failed: $fail_count"
    else
        echo -e "${YELLOW}Operation cancelled.${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

bulk_status_check() {
    echo -e "${CYAN}Bulk Status Check${NC}"
    echo ""
    echo "Enter email addresses (one per line, empty line to finish):"
    
    emails=()
    while true; do
        read -p "> " email
        if [[ -z "$email" ]]; then
            break
        fi
        emails+=("$email")
    done
    
    if [[ ${#emails[@]} -eq 0 ]]; then
        echo -e "${YELLOW}No email addresses entered.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Checking status for ${#emails[@]} accounts...${NC}"
    echo ""
    
    printf "%-40s %-15s %-20s\n" "Email" "Status" "Last Login"
    echo "---------------------------------------- --------------- --------------------"
    
    for email in "${emails[@]}"; do
        user_info=$($GAM_PATH info user "$email" 2>/dev/null)
        
        if [[ $? -eq 0 ]]; then
            if echo "$user_info" | grep -q "Suspended: True"; then
                status="${RED}SUSPENDED${NC}"
            else
                status="${GREEN}ACTIVE${NC}"
            fi
            
            last_login=$(echo "$user_info" | grep "Last Login" | cut -d: -f2- | xargs)
            [[ -z "$last_login" ]] && last_login="Never"
            
            printf "%-40s %-25s %-20s\n" "$email" "$status" "$last_login"
        else
            printf "%-40s %-25s %-20s\n" "$email" "${RED}NOT FOUND${NC}" "N/A"
        fi
    done
    
    echo ""
    read -p "Press Enter to continue..."
}

set_suspension_reason() {
    echo -e "${CYAN}Set Suspension Reason${NC}"
    echo ""
    read -p "Enter the email address: " email
    
    if [[ -z "$email" ]]; then
        echo -e "${RED}Error: Email address cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Check if account is suspended
    user_info=$($GAM_PATH info user "$email" 2>/dev/null)
    if [[ $? -ne 0 ]]; then
        echo -e "${RED}Account not found.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    if ! echo "$user_info" | grep -q "Suspended: True"; then
        echo -e "${YELLOW}âš  Account is not currently suspended.${NC}"
    fi
    
    echo ""
    read -p "Enter suspension reason: " reason
    
    if [[ -z "$reason" ]]; then
        echo -e "${RED}Error: Reason cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    # Log the reason update
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "$timestamp | REASON_UPDATE | $email | $reason | $USER" >> "$LOGFILE"
    
    # Update database if available
    if command -v sqlite3 >/dev/null 2>&1 && [[ -f "$DATABASE_PATH" ]]; then
        sqlite3 "$DATABASE_PATH" "INSERT OR IGNORE INTO account_operations (email, operation, reason, operator, timestamp) VALUES ('$email', 'reason_update', '$reason', '$USER', '$timestamp');" 2>/dev/null || true
    fi
    
    echo -e "${GREEN}âœ“ Suspension reason updated for $email${NC}"
    echo ""
    read -p "Press Enter to continue..."
}

list_all_suspended_accounts() {
    echo -e "${CYAN}All Suspended Accounts${NC}"
    echo ""
    echo -e "${YELLOW}Retrieving suspended accounts...${NC}"
    echo ""
    
    # Get all suspended users
    suspended_users=$($GAM_PATH print users suspended 2>/dev/null)
    
    if [[ $? -eq 0 ]] && [[ -n "$suspended_users" ]]; then
        echo "$suspended_users" | head -1  # Header
        echo "$suspended_users" | tail -n +2 | sort
        
        count=$(echo "$suspended_users" | tail -n +2 | wc -l)
        echo ""
        echo -e "${YELLOW}Total suspended accounts: $count${NC}"
    else
        echo -e "${GREEN}No suspended accounts found.${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

export_status_report() {
    echo -e "${CYAN}Export Status Report${NC}"
    echo ""
    
    local report_file="account_status_report_$(date +%Y%m%d_%H%M%S).csv"
    
    echo -e "${YELLOW}Generating status report...${NC}"
    echo ""
    
    # Create CSV header
    echo "Email,Status,Creation_Time,Last_Login,Org_Unit" > "$report_file"
    
    # Get all users and their status
    echo "Retrieving all users..."
    all_users=$($GAM_PATH print users 2>/dev/null)
    
    if [[ $? -eq 0 ]] && [[ -n "$all_users" ]]; then
        # Process each user (skip header)
        echo "$all_users" | tail -n +2 | while IFS=',' read -r email suspended creation_time last_login org_unit rest; do
            if [[ "$suspended" == "True" ]]; then
                status="SUSPENDED"
            else
                status="ACTIVE"
            fi
            
            echo "$email,$status,$creation_time,$last_login,$org_unit" >> "$report_file"
        done
        
        echo -e "${GREEN}âœ“ Status report exported to: $report_file${NC}"
        
        # Show summary
        total=$(tail -n +2 "$report_file" | wc -l)
        suspended=$(tail -n +2 "$report_file" | grep -c "SUSPENDED")
        active=$((total - suspended))
        
        echo ""
        echo -e "${YELLOW}Report Summary:${NC}"
        echo "  Total accounts: $total"
        echo "  Active: $active"
        echo "  Suspended: $suspended"
    else
        echo -e "${RED}âœ— Failed to retrieve user data.${NC}"
        rm -f "$report_file"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

verify_status_consistency() {
    echo -e "${CYAN}Verify Status Consistency${NC}"
    echo ""
    echo -e "${YELLOW}Checking for status inconsistencies...${NC}"
    echo ""
    
    # Check for users in suspended OU but not marked as suspended
    echo "Checking users in suspended organizational units..."
    
    if [[ -n "$SUSPENDED_OU" ]]; then
        echo "Checking OU: $SUSPENDED_OU"
        
        suspended_ou_users=$($GAM_PATH print users query "orgUnitPath='$SUSPENDED_OU'" 2>/dev/null)
        
        if [[ $? -eq 0 ]] && [[ -n "$suspended_ou_users" ]]; then
            echo "$suspended_ou_users" | tail -n +2 | while IFS=',' read -r email suspended rest; do
                if [[ "$suspended" != "True" ]]; then
                    echo -e "${YELLOW}âš  Inconsistency: $email is in suspended OU but not marked as suspended${NC}"
                fi
            done
        fi
    fi
    
    # Check for suspended users not in suspended OU
    echo ""
    echo "Checking suspended users not in suspended OU..."
    
    suspended_users=$($GAM_PATH print users suspended 2>/dev/null)
    
    if [[ $? -eq 0 ]] && [[ -n "$suspended_users" ]]; then
        echo "$suspended_users" | tail -n +2 | while IFS=',' read -r email suspended creation_time last_login org_unit rest; do
            if [[ -n "$SUSPENDED_OU" ]] && [[ "$org_unit" != "$SUSPENDED_OU" ]]; then
                echo -e "${YELLOW}âš  Inconsistency: $email is suspended but not in suspended OU ($org_unit)${NC}"
            fi
        done
    fi
    
    echo ""
    echo -e "${GREEN}âœ“ Status consistency check completed.${NC}"
    echo ""
    read -p "Press Enter to continue..."
}

schedule_status_changes() {
    echo -e "${CYAN}Schedule Status Changes${NC}"
    echo ""
    echo -e "${YELLOW}This feature allows scheduling future status changes.${NC}"
    echo ""
    echo "Available operations:"
    echo "1. Schedule account suspension"
    echo "2. Schedule account restoration"
    echo "3. View scheduled operations"
    echo "4. Cancel scheduled operation"
    echo ""
    read -p "Select option (1-4): " sched_choice
    
    case $sched_choice in
        1)
            echo ""
            read -p "Email to schedule for suspension: " email
            read -p "Schedule date/time (YYYY-MM-DD HH:MM): " schedule_time
            read -p "Reason: " reason
            
            # Create at job (if available)
            if command -v at >/dev/null 2>&1; then
                echo "$GAM_PATH update user $email suspended on" | at "$schedule_time" 2>/dev/null
                if [[ $? -eq 0 ]]; then
                    echo -e "${GREEN}âœ“ Suspension scheduled for $email at $schedule_time${NC}"
                    
                    # Log the scheduled operation
                    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
                    echo "$timestamp | SCHEDULE_SUSPEND | $email | Scheduled for $schedule_time: $reason | $USER" >> "$LOGFILE"
                else
                    echo -e "${RED}âœ— Failed to schedule operation. 'at' command may not be available.${NC}"
                fi
            else
                echo -e "${YELLOW}âš  'at' command not available. Manual scheduling required.${NC}"
                echo "To manually schedule, use: echo '$GAM_PATH update user $email suspended on' | at '$schedule_time'"
            fi
            ;;
        2)
            echo ""
            read -p "Email to schedule for restoration: " email
            read -p "Schedule date/time (YYYY-MM-DD HH:MM): " schedule_time
            
            if command -v at >/dev/null 2>&1; then
                echo "$GAM_PATH update user $email suspended off" | at "$schedule_time" 2>/dev/null
                if [[ $? -eq 0 ]]; then
                    echo -e "${GREEN}âœ“ Restoration scheduled for $email at $schedule_time${NC}"
                    
                    # Log the scheduled operation
                    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
                    echo "$timestamp | SCHEDULE_RESTORE | $email | Scheduled for $schedule_time | $USER" >> "$LOGFILE"
                else
                    echo -e "${RED}âœ— Failed to schedule operation.${NC}"
                fi
            else
                echo -e "${YELLOW}âš  'at' command not available.${NC}"
            fi
            ;;
        3)
            echo ""
            echo -e "${YELLOW}Scheduled operations:${NC}"
            if command -v atq >/dev/null 2>&1; then
                atq
            else
                echo "'at' command not available."
            fi
            ;;
        4)
            echo ""
            if command -v atq >/dev/null 2>&1; then
                echo "Current scheduled jobs:"
                atq
                echo ""
                read -p "Enter job number to cancel: " job_num
                if [[ -n "$job_num" ]]; then
                    atrm "$job_num"
                    echo -e "${GREEN}âœ“ Job $job_num cancelled.${NC}"
                fi
            else
                echo "'at' command not available."
            fi
            ;;
        *)
            echo -e "${RED}Invalid option.${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

user_statistics_menu() {
    render_menu "statistics_metrics"
}

# User Statistics Functions

current_account_summary() {
    clear
    echo -e "${GREEN}=== Current Account Summary ===${NC}"
    echo ""
    
    echo -e "${CYAN}Generating account summary...${NC}"
    echo ""
    
    # Get current account counts from database
    local total_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1;" 2>/dev/null || echo "0")
    local active_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE suspended = 'False' AND is_active = 1;" 2>/dev/null || echo "0")
    local suspended_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE suspended = 'True' AND is_active = 1;" 2>/dev/null || echo "0")
    
    # Get license information if available
    local licensed_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(DISTINCT email) FROM licenses WHERE is_active = 1;" 2>/dev/null || echo "N/A")
    
    # Get storage information if available
    local total_storage=$(sqlite3 "$DATABASE_PATH" "SELECT ROUND(SUM(total_size_gb), 2) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history);" 2>/dev/null || echo "N/A")
    
    echo -e "${YELLOW}Account Overview:${NC}"
    echo "  Total Accounts: $total_accounts"
    echo "  Active Accounts: $active_accounts"
    echo "  Suspended Accounts: $suspended_accounts"
    echo "  Licensed Accounts: $licensed_accounts"
    echo ""
    echo -e "${YELLOW}Storage Overview:${NC}"
    echo "  Total Storage Used: ${total_storage} GB"
    echo ""
    
    # Show recent activity
    echo -e "${YELLOW}Recent Activity (Last 30 days):${NC}"
    local recent_suspensions=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM account_operations WHERE operation_type = 'suspend' AND timestamp > datetime('now', '-30 days');" 2>/dev/null || echo "0")
    local recent_restores=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM account_operations WHERE operation_type = 'restore' AND timestamp > datetime('now', '-30 days');" 2>/dev/null || echo "0")
    
    echo "  Accounts Suspended: $recent_suspensions"
    echo "  Accounts Restored: $recent_restores"
    echo ""
    
    # Calculate percentages
    if [[ $total_accounts -gt 0 ]]; then
        local active_percent=$(( (active_accounts * 100) / total_accounts ))
        local suspended_percent=$(( (suspended_accounts * 100) / total_accounts ))
        
        echo -e "${YELLOW}Distribution:${NC}"
        echo "  Active: ${active_percent}%"
        echo "  Suspended: ${suspended_percent}%"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

user_count_by_status() {
    clear
    echo -e "${GREEN}=== User Count by Status ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing user status distribution...${NC}"
    echo ""
    
    # Query accounts by various status fields
    echo -e "${YELLOW}By Suspension Status:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN suspended = 'True' THEN 'Suspended'
                WHEN suspended = 'False' THEN 'Active'
                ELSE 'Unknown'
            END as status,
            COUNT(*) as count
        FROM accounts 
        WHERE is_active = 1
        GROUP BY suspended
        ORDER BY count DESC;
    " 2>/dev/null || echo "No account data available"
    
    echo ""
    echo -e "${YELLOW}By Account Type:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN admin = 'True' THEN 'Admin'
                WHEN admin = 'False' THEN 'Regular User'
                ELSE 'Unknown'
            END as type,
            COUNT(*) as count
        FROM accounts 
        WHERE is_active = 1
        GROUP BY admin
        ORDER BY count DESC;
    " 2>/dev/null || echo "No account data available"
    
    echo ""
    echo -e "${YELLOW}By Two-Factor Authentication:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN two_factor = 'True' THEN '2FA Enabled'
                WHEN two_factor = 'False' THEN '2FA Disabled'
                ELSE 'Unknown'
            END as tfa_status,
            COUNT(*) as count
        FROM accounts 
        WHERE is_active = 1
        GROUP BY two_factor
        ORDER BY count DESC;
    " 2>/dev/null || echo "No 2FA data available"
    
    echo ""
    echo -e "${YELLOW}By Archive Status:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN archived = 'True' THEN 'Archived'
                WHEN archived = 'False' THEN 'Not Archived'
                ELSE 'Unknown'
            END as archive_status,
            COUNT(*) as count
        FROM accounts 
        WHERE is_active = 1
        GROUP BY archived
        ORDER BY count DESC;
    " 2>/dev/null || echo "No archive data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

organizational_unit_distribution() {
    clear
    echo -e "${GREEN}=== Organizational Unit Distribution ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing OU distribution...${NC}"
    echo ""
    
    echo -e "${YELLOW}Top 15 Organizational Units by User Count:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            org_unit_path,
            COUNT(*) as user_count,
            ROUND((COUNT(*) * 100.0 / (SELECT COUNT(*) FROM accounts WHERE is_active = 1)), 1) as percentage
        FROM accounts 
        WHERE is_active = 1 AND org_unit_path IS NOT NULL
        GROUP BY org_unit_path
        ORDER BY user_count DESC
        LIMIT 15;
    " 2>/dev/null || echo "No OU data available"
    
    echo ""
    echo -e "${YELLOW}OU Statistics Summary:${NC}"
    local total_ous=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(DISTINCT org_unit_path) FROM accounts WHERE is_active = 1 AND org_unit_path IS NOT NULL;" 2>/dev/null || echo "0")
    local users_with_ou=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1 AND org_unit_path IS NOT NULL;" 2>/dev/null || echo "0")
    local users_without_ou=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1 AND (org_unit_path IS NULL OR org_unit_path = '');" 2>/dev/null || echo "0")
    
    echo "  Total Organizational Units: $total_ous"
    echo "  Users with OU assigned: $users_with_ou"
    echo "  Users without OU: $users_without_ou"
    
    echo ""
    read -p "Press Enter to continue..."
}

account_creation_timeline() {
    clear
    echo -e "${GREEN}=== Account Creation Timeline ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing account creation patterns...${NC}"
    echo ""
    
    echo -e "${YELLOW}Accounts Created by Month (Last 12 months):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            strftime('%Y-%m', creation_time) as month,
            COUNT(*) as accounts_created
        FROM accounts 
        WHERE is_active = 1 
        AND creation_time >= datetime('now', '-12 months')
        GROUP BY strftime('%Y-%m', creation_time)
        ORDER BY month DESC;
    " 2>/dev/null || echo "No creation time data available"
    
    echo ""
    echo -e "${YELLOW}Accounts Created by Year:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            strftime('%Y', creation_time) as year,
            COUNT(*) as accounts_created
        FROM accounts 
        WHERE is_active = 1 
        AND creation_time IS NOT NULL
        GROUP BY strftime('%Y', creation_time)
        ORDER BY year DESC
        LIMIT 10;
    " 2>/dev/null || echo "No creation time data available"
    
    echo ""
    echo -e "${YELLOW}Recent Creation Activity (Last 30 days):${NC}"
    local recent_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1 AND creation_time >= datetime('now', '-30 days');" 2>/dev/null || echo "0")
    echo "  Accounts created in last 30 days: $recent_count"
    
    echo ""
    read -p "Press Enter to continue..."
}

storage_usage_statistics() {
    clear
    echo -e "${GREEN}=== Storage Usage Statistics ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing storage usage patterns...${NC}"
    echo ""
    
    # Get latest storage data
    echo -e "${YELLOW}Current Storage Summary:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Total Storage Used: ' || ROUND(SUM(total_size_gb), 2) || ' GB',
            'Average per User: ' || ROUND(AVG(total_size_gb), 2) || ' GB',
            'Median Storage: ' || ROUND(
                (SELECT total_size_gb FROM storage_size_history 
                 WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
                 ORDER BY total_size_gb 
                 LIMIT 1 OFFSET (SELECT COUNT(*)/2 FROM storage_size_history 
                                WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history))
                ), 2) || ' GB'
        FROM storage_size_history 
        WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history);
    " 2>/dev/null || echo "No storage data available"
    
    echo ""
    echo -e "${YELLOW}Storage Distribution:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN total_size_gb = 0 THEN '0 GB (Empty)'
                WHEN total_size_gb < 1 THEN '< 1 GB'
                WHEN total_size_gb < 5 THEN '1-5 GB'
                WHEN total_size_gb < 15 THEN '5-15 GB'
                WHEN total_size_gb < 30 THEN '15-30 GB (Approaching limit)'
                ELSE '30+ GB (Over standard quota)'
            END as storage_range,
            COUNT(*) as user_count,
            ROUND((COUNT(*) * 100.0 / (SELECT COUNT(*) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history))), 1) as percentage
        FROM storage_size_history 
        WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
        GROUP BY 
            CASE 
                WHEN total_size_gb = 0 THEN '0 GB (Empty)'
                WHEN total_size_gb < 1 THEN '< 1 GB'
                WHEN total_size_gb < 5 THEN '1-5 GB'
                WHEN total_size_gb < 15 THEN '5-15 GB'
                WHEN total_size_gb < 30 THEN '15-30 GB (Approaching limit)'
                ELSE '30+ GB (Over standard quota)'
            END
        ORDER BY MIN(total_size_gb);
    " 2>/dev/null || echo "No storage distribution data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

storage_growth_trends() {
    clear
    echo -e "${GREEN}=== Storage Growth Trends ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing storage growth patterns...${NC}"
    echo ""
    
    echo -e "${YELLOW}Monthly Storage Growth (Last 6 months):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            strftime('%Y-%m', scan_time) as month,
            ROUND(SUM(total_size_gb), 2) as total_storage_gb,
            COUNT(DISTINCT email) as users_scanned
        FROM storage_size_history 
        WHERE scan_time >= datetime('now', '-6 months')
        GROUP BY strftime('%Y-%m', scan_time)
        ORDER BY month DESC;
    " 2>/dev/null || echo "No historical storage data available"
    
    echo ""
    echo -e "${YELLOW}Storage Growth Rate Analysis:${NC}"
    
    # Calculate growth rate if we have historical data
    local current_total=$(sqlite3 "$DATABASE_PATH" "SELECT ROUND(SUM(total_size_gb), 2) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history);" 2>/dev/null || echo "0")
    local month_ago_total=$(sqlite3 "$DATABASE_PATH" "SELECT ROUND(SUM(total_size_gb), 2) FROM storage_size_history WHERE scan_time >= datetime('now', '-1 month') AND scan_time < datetime('now', '-25 days') ORDER BY scan_time DESC LIMIT 1;" 2>/dev/null || echo "0")
    
    if [[ "$current_total" != "0" && "$month_ago_total" != "0" ]]; then
        local growth=$(echo "scale=2; $current_total - $month_ago_total" | bc 2>/dev/null || echo "N/A")
        local growth_percent=$(echo "scale=1; ($growth / $month_ago_total) * 100" | bc 2>/dev/null || echo "N/A")
        
        echo "  Current Total: ${current_total} GB"
        echo "  Previous Month: ${month_ago_total} GB"
        echo "  Monthly Growth: ${growth} GB (${growth_percent}%)"
    else
        echo "  Insufficient historical data for growth calculation"
    fi
    
    echo ""
    echo -e "${YELLOW}Top Growing Users (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            ROUND(MAX(total_size_gb) - MIN(total_size_gb), 2) as growth_gb
        FROM storage_size_history 
        WHERE scan_time >= datetime('now', '-30 days')
        GROUP BY email
        HAVING growth_gb > 0
        ORDER BY growth_gb DESC
        LIMIT 10;
    " 2>/dev/null || echo "No storage growth data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

top_storage_users() {
    clear
    echo -e "${GREEN}=== Top Storage Users ===${NC}"
    echo ""
    
    echo -e "${CYAN}Identifying highest storage consumers...${NC}"
    echo ""
    
    echo -e "${YELLOW}Top 20 Storage Users (Current):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            ROUND(total_size_gb, 2) as storage_gb,
            CASE 
                WHEN total_size_gb > 30 THEN 'Over Quota'
                WHEN total_size_gb > 15 THEN 'High Usage'
                WHEN total_size_gb > 5 THEN 'Moderate Usage'
                ELSE 'Low Usage'
            END as usage_level
        FROM storage_size_history 
        WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
        ORDER BY total_size_gb DESC
        LIMIT 20;
    " 2>/dev/null || echo "No storage data available"
    
    echo ""
    echo -e "${YELLOW}Storage Usage Categories:${NC}"
    local over_quota=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history) AND total_size_gb > 30;" 2>/dev/null || echo "0")
    local high_usage=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history) AND total_size_gb BETWEEN 15 AND 30;" 2>/dev/null || echo "0")
    local moderate_usage=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history) AND total_size_gb BETWEEN 5 AND 15;" 2>/dev/null || echo "0")
    local low_usage=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history) AND total_size_gb < 5;" 2>/dev/null || echo "0")
    
    echo "  Over Quota (30+ GB): $over_quota users"
    echo "  High Usage (15-30 GB): $high_usage users"
    echo "  Moderate Usage (5-15 GB): $moderate_usage users"
    echo "  Low Usage (< 5 GB): $low_usage users"
    
    echo ""
    read -p "Press Enter to continue..."
}

storage_distribution_analysis() {
    clear
    echo -e "${GREEN}=== Storage Distribution Analysis ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing storage distribution patterns...${NC}"
    echo ""
    
    echo -e "${YELLOW}Statistical Distribution:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Mean: ' || ROUND(AVG(total_size_gb), 2) || ' GB' as mean,
            'Median: ' || ROUND(
                (SELECT total_size_gb FROM storage_size_history 
                 WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
                 ORDER BY total_size_gb 
                 LIMIT 1 OFFSET (SELECT COUNT(*)/2 FROM storage_size_history 
                                WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history))
                ), 2) || ' GB' as median,
            'Min: ' || ROUND(MIN(total_size_gb), 2) || ' GB' as minimum,
            'Max: ' || ROUND(MAX(total_size_gb), 2) || ' GB' as maximum
        FROM storage_size_history 
        WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history);
    " 2>/dev/null || echo "No storage data available"
    
    echo ""
    echo -e "${YELLOW}Percentile Analysis:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            '90th Percentile: ' || ROUND(
                (SELECT total_size_gb FROM storage_size_history 
                 WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
                 ORDER BY total_size_gb 
                 LIMIT 1 OFFSET (SELECT COUNT()*90/100 FROM storage_size_history 
                                WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history))
                ), 2) || ' GB' as p90,
            '75th Percentile: ' || ROUND(
                (SELECT total_size_gb FROM storage_size_history 
                 WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
                 ORDER BY total_size_gb 
                 LIMIT 1 OFFSET (SELECT COUNT()*75/100 FROM storage_size_history 
                                WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history))
                ), 2) || ' GB' as p75,
            '25th Percentile: ' || ROUND(
                (SELECT total_size_gb FROM storage_size_history 
                 WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
                 ORDER BY total_size_gb 
                 LIMIT 1 OFFSET (SELECT COUNT()*25/100 FROM storage_size_history 
                                WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history))
                ), 2) || ' GB' as p25
        FROM storage_size_history 
        WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
        LIMIT 1;
    " 2>/dev/null || echo "No storage data available"
    
    echo ""
    echo -e "${YELLOW}Empty Accounts Analysis:${NC}"
    local empty_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history) AND total_size_gb = 0;" 2>/dev/null || echo "0")
    local total_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history);" 2>/dev/null || echo "0")
    
    if [[ $total_accounts -gt 0 ]]; then
        local empty_percent=$(( (empty_accounts * 100) / total_accounts ))
        echo "  Empty accounts (0 GB): $empty_accounts ($empty_percent%)"
        echo "  Accounts with data: $((total_accounts - empty_accounts)) ($((100 - empty_percent))%)"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Placeholder functions for remaining statistics options

login_activity_summary() {
    clear
    echo -e "${GREEN}=== Login Activity Summary ===${NC}"
    echo ""
    echo -e "${CYAN}Login Activity Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Last login timestamps"
    echo "â€¢ Login frequency patterns"
    echo "â€¢ Inactive account identification"
    echo "â€¢ Login location analysis"
    echo ""
    read -p "Press Enter to continue..."
}

email_usage_statistics() {
    clear
    echo -e "${GREEN}=== Email Usage Statistics ===${NC}"
    echo ""
    echo -e "${CYAN}Email Usage Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Email volume statistics"
    echo "â€¢ Mailbox size analysis"
    echo "â€¢ Forwarding rule analysis"
    echo "â€¢ Email activity patterns"
    echo ""
    read -p "Press Enter to continue..."
}

drive_activity_patterns() {
    clear
    echo -e "${GREEN}=== Drive Activity Patterns ===${NC}"
    echo ""
    echo -e "${CYAN}Drive Activity Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ File access patterns"
    echo "â€¢ Sharing activity analysis"
    echo "â€¢ Drive usage trends"
    echo "â€¢ Collaboration metrics"
    echo ""
    read -p "Press Enter to continue..."
}

group_membership_analytics() {
    clear
    echo -e "${GREEN}=== Group Membership Analytics ===${NC}"
    echo ""
    echo -e "${CYAN}Group Analytics - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Group size distribution"
    echo "â€¢ Membership overlap analysis"
    echo "â€¢ Group activity metrics"
    echo "â€¢ Access pattern analysis"
    echo ""
    read -p "Press Enter to continue..."
}

account_stage_distribution() {
    clear
    echo -e "${GREEN}=== Account Stage Distribution ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing account lifecycle stages...${NC}"
    echo ""
    
    # Get current stage distribution
    echo -e "${YELLOW}Current Lifecycle Stage Distribution:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(stage, 'Active') as stage,
            COUNT(*) as count,
            ROUND((COUNT(*) * 100.0 / (SELECT COUNT(*) FROM accounts WHERE is_active = 1)), 1) as percentage
        FROM accounts 
        WHERE is_active = 1
        GROUP BY stage
        ORDER BY count DESC;
    " 2>/dev/null || echo "No stage data available"
    
    echo ""
    echo -e "${YELLOW}Stage History Summary (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            new_stage,
            COUNT(*) as transitions
        FROM stage_history 
        WHERE changed_at >= datetime('now', '-30 days')
        GROUP BY new_stage
        ORDER BY transitions DESC;
    " 2>/dev/null || echo "No recent stage history available"
    
    echo ""
    read -p "Press Enter to continue..."
}

average_account_tenure() {
    clear
    echo -e "${GREEN}=== Average Account Tenure ===${NC}"
    echo ""
    echo -e "${CYAN}Account Tenure Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Average account age"
    echo "â€¢ Tenure by department"
    echo "â€¢ Account lifecycle timing"
    echo "â€¢ Retention analysis"
    echo ""
    read -p "Press Enter to continue..."
}

suspension_deletion_stats() {
    clear
    echo -e "${GREEN}=== Suspension & Deletion Statistics ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing suspension and deletion patterns...${NC}"
    echo ""
    
    echo -e "${YELLOW}Recent Operations (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            operation_type,
            COUNT(*) as count
        FROM account_operations 
        WHERE timestamp >= datetime('now', '-30 days')
        GROUP BY operation_type
        ORDER BY count DESC;
    " 2>/dev/null || echo "No recent operations data available"
    
    echo ""
    echo -e "${YELLOW}Operations by Month (Last 6 months):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            strftime('%Y-%m', timestamp) as month,
            operation_type,
            COUNT(*) as count
        FROM account_operations 
        WHERE timestamp >= datetime('now', '-6 months')
        GROUP BY strftime('%Y-%m', timestamp), operation_type
        ORDER BY month DESC, count DESC;
    " 2>/dev/null || echo "No historical operations data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

account_health_metrics() {
    clear
    echo -e "${GREEN}=== Account Health Metrics ===${NC}"
    echo ""
    echo -e "${CYAN}Account Health Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Security compliance metrics"
    echo "â€¢ Account activity health"
    echo "â€¢ Risk assessment scores"
    echo "â€¢ Health trend analysis"
    echo ""
    read -p "Press Enter to continue..."
}

generate_statistical_report() {
    clear
    echo -e "${GREEN}=== Generate Statistical Report ===${NC}"
    echo ""
    
    # Database-driven statistical report generation
    local report_file="$LOG_DIR/statistical_report_$(date +%Y%m%d_%H%M%S).txt"
    local csv_file="$LOG_DIR/statistical_report_$(date +%Y%m%d_%H%M%S).csv"
    
    echo "Generating comprehensive statistical report..."
    
    {
        echo "GWOMBAT Statistical Report"
        echo "Generated: $(date)"
        echo "Domain: $DOMAIN"
        echo "================================"
        echo ""
        
        # User Statistics
        echo "USER STATISTICS:"
        if command -v gam >/dev/null 2>&1; then
            local total_users=$(gam print users | wc -l)
            local suspended_users=$(gam print users suspended | wc -l)
            local active_users=$((total_users - suspended_users))
            echo "Total Users: $total_users"
            echo "Active Users: $active_users"
            echo "Suspended Users: $suspended_users"
        fi
        echo ""
        
        # Group Statistics
        echo "GROUP STATISTICS:"
        if command -v gam >/dev/null 2>&1; then
            local total_groups=$(gam print groups | wc -l)
            echo "Total Groups: $total_groups"
        fi
        echo ""
        
        # Database Statistics
        echo "DATABASE STATISTICS:"
        if [[ -f "$DB_FILE" ]]; then
            local db_size=$(du -h "$DB_FILE" | cut -f1)
            echo "Database Size: $db_size"
            
            # Account operations count
            local account_ops=$(sqlite3 "$DB_FILE" "SELECT COUNT(*) FROM account_operations;" 2>/dev/null || echo "0")
            echo "Account Operations Logged: $account_ops"
            
            # Recent activity count (last 30 days)
            local recent_activity=$(sqlite3 "$DB_FILE" "SELECT COUNT(*) FROM account_operations WHERE created_at >= datetime('now', '-30 days');" 2>/dev/null || echo "0")
            echo "Recent Activity (30 days): $recent_activity"
        fi
        echo ""
        
        # System Information
        echo "SYSTEM INFORMATION:"
        echo "OS: $(uname -s)"
        echo "Hostname: $(hostname)"
        echo "Disk Usage: $(df -h . | tail -1 | awk '{print $5}' | sed 's/%//')"
        echo "Report Location: $report_file"
        
    } | tee "$report_file"
    
    # Generate CSV format
    {
        echo "Metric,Value,Date"
        echo "Total Users,$(gam print users 2>/dev/null | wc -l),$(date)"
        echo "Active Users,$(gam print users | grep -v suspended | wc -l 2>/dev/null),$(date)"
        echo "Total Groups,$(gam print groups 2>/dev/null | wc -l),$(date)"
        echo "Database Size (bytes),$(stat -f%z "$DB_FILE" 2>/dev/null || echo 0),$(date)"
    } > "$csv_file"
    
    echo ""
    echo -e "${GREEN}Report generated successfully!${NC}"
    echo "Text format: $report_file"
    echo "CSV format: $csv_file"
    echo ""
    read -p "Press Enter to continue..."
}

export_statistics_csv() {
    clear
    echo -e "${GREEN}=== Export Statistics to CSV ===${NC}"
    echo ""
    echo -e "${CYAN}CSV Export Functionality - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Export all statistics to CSV"
    echo "â€¢ Custom field selection"
    echo "â€¢ Data filtering options"
    echo "â€¢ Automated exports"
    echo ""
    read -p "Press Enter to continue..."
}

historical_trend_analysis() {
    clear
    echo -e "${GREEN}=== Historical Trend Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Trend Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Long-term trend visualization"
    echo "â€¢ Predictive analytics"
    echo "â€¢ Seasonal pattern detection"
    echo "â€¢ Growth projections"
    echo ""
    read -p "Press Enter to continue..."
}

custom_query_builder() {
    clear
    echo -e "${GREEN}=== Custom Query Builder ===${NC}"
    echo ""
    echo -e "${CYAN}Query Builder - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Visual query construction"
    echo "â€¢ Advanced filtering options"
    echo "â€¢ Custom report generation"
    echo "â€¢ Save and reuse queries"
    echo ""
    read -p "Press Enter to continue..."
}

account_lifecycle_reports_menu() {
    render_menu "reports_monitoring"
}

# Account Lifecycle Reports Functions

current_stage_distribution() {
    clear
    echo -e "${GREEN}=== Current Stage Distribution ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing current lifecycle stages...${NC}"
    echo ""
    
    echo -e "${YELLOW}Current Stage Distribution:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(stage, 'Active') as stage,
            COUNT(*) as count,
            ROUND((COUNT(*) * 100.0 / (SELECT COUNT(*) FROM accounts WHERE is_active = 1)), 1) as percentage
        FROM accounts 
        WHERE is_active = 1
        GROUP BY stage
        ORDER BY count DESC;
    " 2>/dev/null || echo "No stage data available"
    
    echo ""
    echo -e "${YELLOW}Stage Summary Statistics:${NC}"
    local total_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1;" 2>/dev/null || echo "0")
    local staged_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1 AND stage IS NOT NULL;" 2>/dev/null || echo "0")
    local active_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1 AND (stage IS NULL OR stage = 'Active');" 2>/dev/null || echo "0")
    
    echo "  Total Accounts: $total_accounts"
    echo "  In Lifecycle Process: $staged_accounts"
    echo "  Active (No Stage): $active_accounts"
    
    if [[ $total_accounts -gt 0 ]]; then
        local staged_percent=$(( (staged_accounts * 100) / total_accounts ))
        echo "  Percentage in Lifecycle: ${staged_percent}%"
    fi
    
    echo ""
    echo -e "${YELLOW}Stage Definitions:${NC}"
    echo "  â€¢ Stage 1: Recently Suspended"
    echo "  â€¢ Stage 2: Pending Deletion"
    echo "  â€¢ Stage 3: Sharing Analysis"
    echo "  â€¢ Stage 4: Final Decision"
    echo "  â€¢ Stage 5: Ready for Deletion"
    echo ""
    
    read -p "Press Enter to continue..."
}

stage_transition_history() {
    clear
    echo -e "${GREEN}=== Stage Transition History ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing stage transition patterns...${NC}"
    echo ""
    
    echo -e "${YELLOW}Recent Stage Transitions (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            old_stage,
            new_stage,
            changed_at,
            changed_by
        FROM stage_history 
        WHERE changed_at >= datetime('now', '-30 days')
        ORDER BY changed_at DESC
        LIMIT 20;
    " 2>/dev/null || echo "No recent stage history available"
    
    echo ""
    echo -e "${YELLOW}Transition Summary (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            old_stage || ' â†’ ' || new_stage as transition,
            COUNT(*) as count
        FROM stage_history 
        WHERE changed_at >= datetime('now', '-30 days')
        GROUP BY old_stage, new_stage
        ORDER BY count DESC;
    " 2>/dev/null || echo "No transition data available"
    
    echo ""
    echo -e "${YELLOW}Most Active Transitioners:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            COUNT(*) as transitions
        FROM stage_history 
        WHERE changed_at >= datetime('now', '-30 days')
        GROUP BY email
        ORDER BY transitions DESC
        LIMIT 10;
    " 2>/dev/null || echo "No transition data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

time_in_stage_analysis() {
    clear
    echo -e "${GREEN}=== Time in Stage Analysis ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing time spent in each lifecycle stage...${NC}"
    echo ""
    
    echo -e "${YELLOW}Average Time in Each Stage:${NC}"
    sqlite3 "$DATABASE_PATH" "
        WITH stage_durations AS (
            SELECT 
                email,
                new_stage,
                changed_at,
                LEAD(changed_at) OVER (PARTITION BY email ORDER BY changed_at) as next_change,
                CASE 
                    WHEN LEAD(changed_at) OVER (PARTITION BY email ORDER BY changed_at) IS NULL 
                    THEN datetime('now')
                    ELSE LEAD(changed_at) OVER (PARTITION BY email ORDER BY changed_at)
                END as end_time
            FROM stage_history
        )
        SELECT 
            new_stage as stage,
            COUNT(*) as account_count,
            ROUND(AVG(julianday(end_time) - julianday(changed_at)), 1) as avg_days
        FROM stage_durations
        WHERE new_stage IS NOT NULL
        GROUP BY new_stage
        ORDER BY avg_days DESC;
    " 2>/dev/null || echo "No stage duration data available"
    
    echo ""
    echo -e "${YELLOW}Current Stage Ages (Accounts currently in stages):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            a.stage,
            a.email,
            ROUND(julianday('now') - julianday(sh.changed_at), 1) as days_in_stage
        FROM accounts a
        JOIN stage_history sh ON a.email = sh.email
        WHERE a.stage IS NOT NULL 
        AND sh.new_stage = a.stage
        AND sh.changed_at = (
            SELECT MAX(changed_at) 
            FROM stage_history sh2 
            WHERE sh2.email = a.email
        )
        ORDER BY days_in_stage DESC
        LIMIT 15;
    " 2>/dev/null || echo "No current stage data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

stage_progression_trends() {
    clear
    echo -e "${GREEN}=== Stage Progression Trends ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing progression patterns through lifecycle stages...${NC}"
    echo ""
    
    echo -e "${YELLOW}Monthly Stage Transitions (Last 6 months):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            strftime('%Y-%m', changed_at) as month,
            new_stage,
            COUNT(*) as transitions
        FROM stage_history 
        WHERE changed_at >= datetime('now', '-6 months')
        GROUP BY strftime('%Y-%m', changed_at), new_stage
        ORDER BY month DESC, transitions DESC;
    " 2>/dev/null || echo "No historical stage data available"
    
    echo ""
    echo -e "${YELLOW}Progression Success Rates:${NC}"
    sqlite3 "$DATABASE_PATH" "
        WITH progression_analysis AS (
            SELECT 
                email,
                new_stage,
                changed_at,
                LEAD(new_stage) OVER (PARTITION BY email ORDER BY changed_at) as next_stage
            FROM stage_history
        )
        SELECT 
            new_stage as from_stage,
            next_stage as to_stage,
            COUNT(*) as transitions
        FROM progression_analysis
        WHERE next_stage IS NOT NULL
        GROUP BY new_stage, next_stage
        ORDER BY from_stage, transitions DESC;
    " 2>/dev/null || echo "No progression data available"
    
    echo ""
    echo -e "${YELLOW}Completion Rates (to Stage 5):${NC}"
    local stage1_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(DISTINCT email) FROM stage_history WHERE new_stage = 'Stage 1';" 2>/dev/null || echo "0")
    local stage5_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(DISTINCT email) FROM stage_history WHERE new_stage = 'Stage 5';" 2>/dev/null || echo "0")
    
    if [[ $stage1_count -gt 0 ]]; then
        local completion_rate=$(( (stage5_count * 100) / stage1_count ))
        echo "  Accounts entering Stage 1: $stage1_count"
        echo "  Accounts reaching Stage 5: $stage5_count"
        echo "  Completion Rate: ${completion_rate}%"
    else
        echo "  No lifecycle progression data available"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

suspension_timeline_reports() {
    clear
    echo -e "${GREEN}=== Suspension Timeline Reports ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing suspension patterns and timelines...${NC}"
    echo ""
    
    echo -e "${YELLOW}Recent Suspensions (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            operation_type,
            timestamp,
            reason
        FROM account_operations 
        WHERE operation_type = 'suspend' 
        AND timestamp >= datetime('now', '-30 days')
        ORDER BY timestamp DESC
        LIMIT 15;
    " 2>/dev/null || echo "No recent suspension data available"
    
    echo ""
    echo -e "${YELLOW}Suspension Timeline Analysis:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            strftime('%Y-%m', timestamp) as month,
            COUNT(*) as suspensions
        FROM account_operations 
        WHERE operation_type = 'suspend'
        AND timestamp >= datetime('now', '-12 months')
        GROUP BY strftime('%Y-%m', timestamp)
        ORDER BY month DESC;
    " 2>/dev/null || echo "No historical suspension data available"
    
    echo ""
    echo -e "${YELLOW}Time from Suspension to Deletion:${NC}"
    sqlite3 "$DATABASE_PATH" "
        WITH suspension_deletion AS (
            SELECT 
                s.email,
                s.timestamp as suspended_at,
                d.timestamp as deleted_at,
                ROUND(julianday(d.timestamp) - julianday(s.timestamp), 1) as days_to_deletion
            FROM account_operations s
            JOIN account_operations d ON s.email = d.email
            WHERE s.operation_type = 'suspend'
            AND d.operation_type = 'delete'
            AND d.timestamp > s.timestamp
        )
        SELECT 
            ROUND(AVG(days_to_deletion), 1) as avg_days,
            MIN(days_to_deletion) as min_days,
            MAX(days_to_deletion) as max_days,
            COUNT(*) as completed_cycles
        FROM suspension_deletion;
    " 2>/dev/null || echo "No suspension-deletion cycle data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

deletion_timeline_analysis() {
    clear
    echo -e "${GREEN}=== Deletion Timeline Analysis ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing account deletion patterns...${NC}"
    echo ""
    
    echo -e "${YELLOW}Recent Deletions (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            timestamp,
            reason
        FROM account_operations 
        WHERE operation_type = 'delete' 
        AND timestamp >= datetime('now', '-30 days')
        ORDER BY timestamp DESC
        LIMIT 15;
    " 2>/dev/null || echo "No recent deletion data available"
    
    echo ""
    echo -e "${YELLOW}Deletion Timeline Trends:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            strftime('%Y-%m', timestamp) as month,
            COUNT(*) as deletions
        FROM account_operations 
        WHERE operation_type = 'delete'
        AND timestamp >= datetime('now', '-12 months')
        GROUP BY strftime('%Y-%m', timestamp)
        ORDER BY month DESC;
    " 2>/dev/null || echo "No historical deletion data available"
    
    echo ""
    echo -e "${YELLOW}Deletion Reasons Analysis:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(reason, 'No reason specified') as deletion_reason,
            COUNT(*) as count
        FROM account_operations 
        WHERE operation_type = 'delete'
        AND timestamp >= datetime('now', '-6 months')
        GROUP BY reason
        ORDER BY count DESC;
    " 2>/dev/null || echo "No deletion reason data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

account_recovery_patterns() {
    clear
    echo -e "${GREEN}=== Account Recovery Patterns ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing account restoration and recovery...${NC}"
    echo ""
    
    echo -e "${YELLOW}Recent Restorations (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            timestamp,
            reason
        FROM account_operations 
        WHERE operation_type = 'restore' 
        AND timestamp >= datetime('now', '-30 days')
        ORDER BY timestamp DESC
        LIMIT 15;
    " 2>/dev/null || echo "No recent restoration data available"
    
    echo ""
    echo -e "${YELLOW}Recovery Success Rates:${NC}"
    local total_suspended=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM account_operations WHERE operation_type = 'suspend' AND timestamp >= datetime('now', '-6 months');" 2>/dev/null || echo "0")
    local total_restored=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM account_operations WHERE operation_type = 'restore' AND timestamp >= datetime('now', '-6 months');" 2>/dev/null || echo "0")
    
    if [[ $total_suspended -gt 0 ]]; then
        local recovery_rate=$(( (total_restored * 100) / total_suspended ))
        echo "  Accounts Suspended (6 months): $total_suspended"
        echo "  Accounts Restored (6 months): $total_restored"
        echo "  Recovery Rate: ${recovery_rate}%"
    else
        echo "  No suspension/restoration data available"
    fi
    
    echo ""
    echo -e "${YELLOW}Time to Recovery Analysis:${NC}"
    sqlite3 "$DATABASE_PATH" "
        WITH recovery_times AS (
            SELECT 
                s.email,
                s.timestamp as suspended_at,
                r.timestamp as restored_at,
                ROUND(julianday(r.timestamp) - julianday(s.timestamp), 1) as days_to_recovery
            FROM account_operations s
            JOIN account_operations r ON s.email = r.email
            WHERE s.operation_type = 'suspend'
            AND r.operation_type = 'restore'
            AND r.timestamp > s.timestamp
            AND s.timestamp >= datetime('now', '-6 months')
        )
        SELECT 
            ROUND(AVG(days_to_recovery), 1) as avg_days,
            MIN(days_to_recovery) as min_days,
            MAX(days_to_recovery) as max_days,
            COUNT(*) as recovery_cycles
        FROM recovery_times;
    " 2>/dev/null || echo "No recovery time data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

rapid_progression_detection() {
    clear
    echo -e "${GREEN}=== Rapid Progression Detection ===${NC}"
    echo ""
    
    echo -e "${CYAN}Identifying accounts with rapid lifecycle progression...${NC}"
    echo ""
    
    echo -e "${YELLOW}Rapid Stage Progressions (< 7 days between stages):${NC}"
    sqlite3 "$DATABASE_PATH" "
        WITH rapid_progressions AS (
            SELECT 
                sh1.email,
                sh1.new_stage as from_stage,
                sh2.new_stage as to_stage,
                sh1.changed_at as start_time,
                sh2.changed_at as end_time,
                ROUND(julianday(sh2.changed_at) - julianday(sh1.changed_at), 1) as days
            FROM stage_history sh1
            JOIN stage_history sh2 ON sh1.email = sh2.email
            WHERE sh2.changed_at > sh1.changed_at
            AND julianday(sh2.changed_at) - julianday(sh1.changed_at) < 7
            AND sh1.changed_at >= datetime('now', '-30 days')
        )
        SELECT 
            email,
            from_stage,
            to_stage,
            days,
            start_time
        FROM rapid_progressions
        ORDER BY days ASC
        LIMIT 20;
    " 2>/dev/null || echo "No rapid progression data available"
    
    echo ""
    echo -e "${YELLOW}Same-Day Multiple Transitions:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            DATE(changed_at) as transition_date,
            COUNT(*) as transitions
        FROM stage_history 
        WHERE changed_at >= datetime('now', '-30 days')
        GROUP BY email, DATE(changed_at)
        HAVING COUNT(*) > 1
        ORDER BY transitions DESC, transition_date DESC
        LIMIT 15;
    " 2>/dev/null || echo "No same-day transition data available"
    
    echo ""
    echo -e "${YELLOW}Accounts with Unusual Patterns:${NC}"
    echo "â€¢ Multiple stage changes in single day"
    echo "â€¢ Rapid progression (< 7 days between stages)"
    echo "â€¢ Backwards progression (higher to lower stage)"
    echo ""
    
    read -p "Press Enter to continue..."
}

account_age_distribution() {
    clear
    echo -e "${GREEN}=== Account Age Distribution ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing account creation and age patterns...${NC}"
    echo ""
    
    echo -e "${YELLOW}Account Age Distribution:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN julianday('now') - julianday(creation_time) < 30 THEN '< 1 month'
                WHEN julianday('now') - julianday(creation_time) < 90 THEN '1-3 months'
                WHEN julianday('now') - julianday(creation_time) < 180 THEN '3-6 months'
                WHEN julianday('now') - julianday(creation_time) < 365 THEN '6-12 months'
                WHEN julianday('now') - julianday(creation_time) < 730 THEN '1-2 years'
                WHEN julianday('now') - julianday(creation_time) < 1095 THEN '2-3 years'
                ELSE '3+ years'
            END as age_range,
            COUNT(*) as account_count,
            ROUND((COUNT(*) * 100.0 / (SELECT COUNT(*) FROM accounts WHERE is_active = 1 AND creation_time IS NOT NULL)), 1) as percentage
        FROM accounts 
        WHERE is_active = 1 AND creation_time IS NOT NULL
        GROUP BY 
            CASE 
                WHEN julianday('now') - julianday(creation_time) < 30 THEN '< 1 month'
                WHEN julianday('now') - julianday(creation_time) < 90 THEN '1-3 months'
                WHEN julianday('now') - julianday(creation_time) < 180 THEN '3-6 months'
                WHEN julianday('now') - julianday(creation_time) < 365 THEN '6-12 months'
                WHEN julianday('now') - julianday(creation_time) < 730 THEN '1-2 years'
                WHEN julianday('now') - julianday(creation_time) < 1095 THEN '2-3 years'
                ELSE '3+ years'
            END
        ORDER BY MIN(julianday('now') - julianday(creation_time));
    " 2>/dev/null || echo "No account age data available"
    
    echo ""
    echo -e "${YELLOW}Age Statistics:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Average Age: ' || ROUND(AVG(julianday('now') - julianday(creation_time)), 1) || ' days' as avg_age,
            'Oldest Account: ' || ROUND(MAX(julianday('now') - julianday(creation_time)), 1) || ' days' as oldest,
            'Newest Account: ' || ROUND(MIN(julianday('now') - julianday(creation_time)), 1) || ' days' as newest
        FROM accounts 
        WHERE is_active = 1 AND creation_time IS NOT NULL;
    " 2>/dev/null || echo "No account age statistics available"
    
    echo ""
    read -p "Press Enter to continue..."
}

# Placeholder functions for remaining lifecycle report options

average_tenure_by_stage() {
    clear
    echo -e "${GREEN}=== Average Tenure by Stage ===${NC}"
    echo ""
    echo -e "${CYAN}Tenure by Stage Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Average time spent in each lifecycle stage"
    echo "â€¢ Tenure analysis by account type"
    echo "â€¢ Stage-specific retention metrics"
    echo "â€¢ Performance benchmarking"
    echo ""
    read -p "Press Enter to continue..."
}

department_lifecycle_patterns() {
    clear
    echo -e "${GREEN}=== Department Lifecycle Patterns ===${NC}"
    echo ""
    echo -e "${CYAN}Department Pattern Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Lifecycle patterns by organizational unit"
    echo "â€¢ Department-specific progression rates"
    echo "â€¢ OU-based lifecycle performance"
    echo "â€¢ Cross-department comparisons"
    echo ""
    read -p "Press Enter to continue..."
}

tenure_trend_analysis() {
    clear
    echo -e "${GREEN}=== Tenure Trend Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Tenure Trend Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Historical tenure trend analysis"
    echo "â€¢ Seasonal patterns in account lifecycle"
    echo "â€¢ Long-term lifecycle predictions"
    echo "â€¢ Trend visualization and reporting"
    echo ""
    read -p "Press Enter to continue..."
}

lifecycle_compliance_report() {
    clear
    echo -e "${GREEN}=== Lifecycle Compliance Report ===${NC}"
    echo ""
    echo -e "${CYAN}Compliance Reporting - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Policy compliance verification"
    echo "â€¢ SLA adherence monitoring"
    echo "â€¢ Lifecycle audit trails"
    echo "â€¢ Compliance violation detection"
    echo ""
    read -p "Press Enter to continue..."
}

exception_anomaly_detection() {
    clear
    echo -e "${GREEN}=== Exception & Anomaly Detection ===${NC}"
    echo ""
    echo -e "${CYAN}Anomaly Detection - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Unusual lifecycle patterns"
    echo "â€¢ Policy violation detection"
    echo "â€¢ Automated anomaly alerts"
    echo "â€¢ Exception case analysis"
    echo ""
    read -p "Press Enter to continue..."
}

lifecycle_performance_metrics() {
    clear
    echo -e "${GREEN}=== Lifecycle Performance Metrics ===${NC}"
    echo ""
    echo -e "${CYAN}Performance Metrics - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Lifecycle efficiency metrics"
    echo "â€¢ Performance benchmarking"
    echo "â€¢ Process optimization insights"
    echo "â€¢ KPI tracking and reporting"
    echo ""
    read -p "Press Enter to continue..."
}

sla_compliance_analysis() {
    clear
    echo -e "${GREEN}=== SLA Compliance Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}SLA Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Service level agreement tracking"
    echo "â€¢ Compliance rate analysis"
    echo "â€¢ SLA violation detection"
    echo "â€¢ Performance against targets"
    echo ""
    read -p "Press Enter to continue..."
}

predicted_deletions() {
    clear
    echo -e "${GREEN}=== Predicted Deletions ===${NC}"
    echo ""
    echo -e "${CYAN}Deletion Prediction - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Machine learning-based predictions"
    echo "â€¢ Account deletion forecasting"
    echo "â€¢ Risk factor analysis"
    echo "â€¢ Proactive intervention suggestions"
    echo ""
    read -p "Press Enter to continue..."
}

lifecycle_forecasting() {
    clear
    echo -e "${GREEN}=== Lifecycle Forecasting ===${NC}"
    echo ""
    echo -e "${CYAN}Lifecycle Forecasting - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Future lifecycle trend predictions"
    echo "â€¢ Capacity planning insights"
    echo "â€¢ Resource allocation forecasting"
    echo "â€¢ Long-term lifecycle modeling"
    echo ""
    read -p "Press Enter to continue..."
}

at_risk_account_identification() {
    clear
    echo -e "${GREEN}=== At-Risk Account Identification ===${NC}"
    echo ""
    echo -e "${CYAN}Risk Identification - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Risk factor scoring"
    echo "â€¢ Early warning indicators"
    echo "â€¢ Proactive intervention alerts"
    echo "â€¢ Risk mitigation strategies"
    echo ""
    read -p "Press Enter to continue..."
}

lifecycle_health_score() {
    clear
    echo -e "${GREEN}=== Lifecycle Health Score ===${NC}"
    echo ""
    echo -e "${CYAN}Health Scoring - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Overall lifecycle health metrics"
    echo "â€¢ Health score calculation"
    echo "â€¢ Trend analysis and monitoring"
    echo "â€¢ Health improvement recommendations"
    echo ""
    read -p "Press Enter to continue..."
}

export_account_data_menu() {
    render_menu "account_list_management"
}

# Export Functions

export_all_active_accounts() {
    clear
    echo -e "${GREEN}=== Export All Active Accounts ===${NC}"
    echo ""
    
    local export_file="accounts_active_$(date +%Y%m%d_%H%M%S).csv"
    local export_path="$BACKUP_DIR/exports/$export_file"
    
    echo -e "${CYAN}Exporting all active accounts to CSV...${NC}"
    echo ""
    
    # Ensure export directory exists
    mkdir -p "$(dirname "$export_path")"
    
    # Export active accounts
    sqlite3 "$DATABASE_PATH" "
        .mode csv
        .headers on
        .output '$export_path'
        SELECT 
            email,
            full_name,
            suspended,
            admin,
            two_factor,
            org_unit_path,
            creation_time,
            last_login_time,
            archived,
            agreed_to_terms,
            change_password_next_login
        FROM accounts 
        WHERE is_active = 1 AND suspended = 'False'
        ORDER BY email;
    " 2>/dev/null
    
    if [[ $? -eq 0 && -f "$export_path" ]]; then
        local record_count=$(tail -n +2 "$export_path" | wc -l)
        echo -e "${GREEN}âœ“ Export completed successfully${NC}"
        echo "  File: $export_path"
        echo "  Records: $record_count active accounts"
        echo "  Format: CSV with headers"
    else
        echo -e "${RED}âœ— Export failed${NC}"
        echo "Check database connectivity and permissions"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

export_suspended_accounts() {
    clear
    echo -e "${GREEN}=== Export Suspended Accounts ===${NC}"
    echo ""
    
    local export_file="accounts_suspended_$(date +%Y%m%d_%H%M%S).csv"
    local export_path="$BACKUP_DIR/exports/$export_file"
    
    echo -e "${CYAN}Exporting suspended accounts to CSV...${NC}"
    echo ""
    
    # Ensure export directory exists
    mkdir -p "$(dirname "$export_path")"
    
    # Export suspended accounts with additional suspension details
    sqlite3 "$DATABASE_PATH" "
        .mode csv
        .headers on
        .output '$export_path'
        SELECT 
            a.email,
            a.full_name,
            a.org_unit_path,
            a.creation_time,
            a.last_login_time,
            COALESCE(a.stage, 'Not Staged') as current_stage,
            ao.timestamp as last_operation_time,
            ao.reason as suspension_reason
        FROM accounts a
        LEFT JOIN account_operations ao ON a.email = ao.email 
            AND ao.operation_type = 'suspend'
            AND ao.timestamp = (
                SELECT MAX(timestamp) 
                FROM account_operations ao2 
                WHERE ao2.email = a.email AND ao2.operation_type = 'suspend'
            )
        WHERE a.is_active = 1 AND a.suspended = 'True'
        ORDER BY ao.timestamp DESC, a.email;
    " 2>/dev/null
    
    if [[ $? -eq 0 && -f "$export_path" ]]; then
        local record_count=$(tail -n +2 "$export_path" | wc -l)
        echo -e "${GREEN}âœ“ Export completed successfully${NC}"
        echo "  File: $export_path"
        echo "  Records: $record_count suspended accounts"
        echo "  Includes: Suspension reasons and lifecycle stages"
    else
        echo -e "${RED}âœ— Export failed${NC}"
        echo "Check database connectivity and permissions"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

export_admin_accounts() {
    clear
    echo -e "${GREEN}=== Export Admin Accounts ===${NC}"
    echo ""
    
    local export_file="accounts_admin_$(date +%Y%m%d_%H%M%S).csv"
    local export_path="$BACKUP_DIR/exports/$export_file"
    
    echo -e "${CYAN}Exporting admin accounts to CSV...${NC}"
    echo ""
    
    # Ensure export directory exists
    mkdir -p "$(dirname "$export_path")"
    
    # Export admin accounts with security details
    sqlite3 "$DATABASE_PATH" "
        .mode csv
        .headers on
        .output '$export_path'
        SELECT 
            email,
            full_name,
            suspended,
            admin,
            super_admin,
            two_factor,
            org_unit_path,
            creation_time,
            last_login_time,
            agreed_to_terms,
            change_password_next_login
        FROM accounts 
        WHERE is_active = 1 AND admin = 'True'
        ORDER BY super_admin DESC, email;
    " 2>/dev/null
    
    if [[ $? -eq 0 && -f "$export_path" ]]; then
        local record_count=$(tail -n +2 "$export_path" | wc -l)
        echo -e "${GREEN}âœ“ Export completed successfully${NC}"
        echo "  File: $export_path"
        echo "  Records: $record_count admin accounts"
        echo "  Includes: Admin privileges and security settings"
        echo ""
        echo -e "${YELLOW}Security Note: This file contains sensitive administrative data${NC}"
    else
        echo -e "${RED}âœ— Export failed${NC}"
        echo "Check database connectivity and permissions"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

export_accounts_with_2fa() {
    clear
    echo -e "${GREEN}=== Export Accounts with 2FA ===${NC}"
    echo ""
    
    local export_file="accounts_2fa_$(date +%Y%m%d_%H%M%S).csv"
    local export_path="$BACKUP_DIR/exports/$export_file"
    
    echo -e "${CYAN}Exporting accounts with 2FA enabled...${NC}"
    echo ""
    
    # Ensure export directory exists
    mkdir -p "$(dirname "$export_path")"
    
    # Export accounts with 2FA
    sqlite3 "$DATABASE_PATH" "
        .mode csv
        .headers on
        .output '$export_path'
        SELECT 
            email,
            full_name,
            suspended,
            admin,
            two_factor,
            org_unit_path,
            creation_time,
            last_login_time
        FROM accounts 
        WHERE is_active = 1 AND two_factor = 'True'
        ORDER BY admin DESC, email;
    " 2>/dev/null
    
    if [[ $? -eq 0 && -f "$export_path" ]]; then
        local record_count=$(tail -n +2 "$export_path" | wc -l)
        local total_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1;" 2>/dev/null || echo "0")
        
        if [[ $total_accounts -gt 0 ]]; then
            local adoption_rate=$(( (record_count * 100) / total_accounts ))
            echo -e "${GREEN}âœ“ Export completed successfully${NC}"
            echo "  File: $export_path"
            echo "  Records: $record_count accounts with 2FA"
            echo "  2FA Adoption Rate: ${adoption_rate}%"
        else
            echo -e "${GREEN}âœ“ Export completed successfully${NC}"
            echo "  File: $export_path"
            echo "  Records: $record_count accounts with 2FA"
        fi
    else
        echo -e "${RED}âœ— Export failed${NC}"
        echo "Check database connectivity and permissions"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

export_by_organizational_unit() {
    clear
    echo -e "${GREEN}=== Export by Organizational Unit ===${NC}"
    echo ""
    
    echo -e "${CYAN}Available Organizational Units:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            org_unit_path,
            COUNT(*) as user_count
        FROM accounts 
        WHERE is_active = 1 AND org_unit_path IS NOT NULL
        GROUP BY org_unit_path
        ORDER BY user_count DESC
        LIMIT 15;
    " 2>/dev/null
    
    echo ""
    read -p "Enter Organizational Unit path (or 'all' for all OUs): " ou_filter
    
    if [[ -z "$ou_filter" ]]; then
        echo -e "${RED}OU path required${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    local export_file="accounts_ou_$(echo "$ou_filter" | tr '/' '_' | tr ' ' '_')_$(date +%Y%m%d_%H%M%S).csv"
    local export_path="$BACKUP_DIR/exports/$export_file"
    
    echo ""
    echo -e "${CYAN}Exporting accounts for OU: $ou_filter${NC}"
    
    # Ensure export directory exists
    mkdir -p "$(dirname "$export_path")"
    
    if [[ "$ou_filter" == "all" ]]; then
        # Export all accounts grouped by OU
        sqlite3 "$DATABASE_PATH" "
            .mode csv
            .headers on
            .output '$export_path'
            SELECT 
                org_unit_path,
                email,
                full_name,
                suspended,
                admin,
                two_factor,
                creation_time,
                last_login_time
            FROM accounts 
            WHERE is_active = 1
            ORDER BY org_unit_path, email;
        " 2>/dev/null
    else
        # Export specific OU
        sqlite3 "$DATABASE_PATH" "
            .mode csv
            .headers on
            .output '$export_path'
            SELECT 
                email,
                full_name,
                suspended,
                admin,
                two_factor,
                org_unit_path,
                creation_time,
                last_login_time
            FROM accounts 
            WHERE is_active = 1 AND org_unit_path = '$ou_filter'
            ORDER BY email;
        " 2>/dev/null
    fi
    
    if [[ $? -eq 0 && -f "$export_path" ]]; then
        local record_count=$(tail -n +2 "$export_path" | wc -l)
        echo -e "${GREEN}âœ“ Export completed successfully${NC}"
        echo "  File: $export_path"
        echo "  Records: $record_count accounts"
        echo "  Filter: $ou_filter"
    else
        echo -e "${RED}âœ— Export failed${NC}"
        echo "Check OU path and database connectivity"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

export_by_creation_date() {
    clear
    echo -e "${GREEN}=== Export by Creation Date Range ===${NC}"
    echo ""
    
    echo "Enter date range for account creation:"
    read -p "Start date (YYYY-MM-DD): " start_date
    read -p "End date (YYYY-MM-DD): " end_date
    
    if [[ -z "$start_date" || -z "$end_date" ]]; then
        echo -e "${RED}Both start and end dates required${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    local export_file="accounts_created_${start_date}_to_${end_date}_$(date +%Y%m%d_%H%M%S).csv"
    local export_path="$BACKUP_DIR/exports/$export_file"
    
    echo ""
    echo -e "${CYAN}Exporting accounts created between $start_date and $end_date${NC}"
    
    # Ensure export directory exists
    mkdir -p "$(dirname "$export_path")"
    
    # Export accounts by creation date range
    sqlite3 "$DATABASE_PATH" "
        .mode csv
        .headers on
        .output '$export_path'
        SELECT 
            email,
            full_name,
            suspended,
            admin,
            two_factor,
            org_unit_path,
            creation_time,
            last_login_time,
            DATE(creation_time) as creation_date
        FROM accounts 
        WHERE is_active = 1 
        AND DATE(creation_time) BETWEEN '$start_date' AND '$end_date'
        ORDER BY creation_time DESC, email;
    " 2>/dev/null
    
    if [[ $? -eq 0 && -f "$export_path" ]]; then
        local record_count=$(tail -n +2 "$export_path" | wc -l)
        echo -e "${GREEN}âœ“ Export completed successfully${NC}"
        echo "  File: $export_path"
        echo "  Records: $record_count accounts"
        echo "  Date Range: $start_date to $end_date"
    else
        echo -e "${RED}âœ— Export failed${NC}"
        echo "Check date format (YYYY-MM-DD) and database connectivity"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

export_by_storage_usage() {
    clear
    echo -e "${GREEN}=== Export by Storage Usage ===${NC}"
    echo ""
    
    echo "Storage usage categories:"
    echo "1. High usage (>15 GB)"
    echo "2. Medium usage (5-15 GB)"
    echo "3. Low usage (<5 GB)"
    echo "4. Empty accounts (0 GB)"
    echo "5. Custom range"
    echo ""
    read -p "Select category (1-5): " storage_category
    
    local filter_condition=""
    local category_name=""
    
    case $storage_category in
        1)
            filter_condition="total_size_gb > 15"
            category_name="high_usage"
            ;;
        2)
            filter_condition="total_size_gb BETWEEN 5 AND 15"
            category_name="medium_usage"
            ;;
        3)
            filter_condition="total_size_gb < 5 AND total_size_gb > 0"
            category_name="low_usage"
            ;;
        4)
            filter_condition="total_size_gb = 0"
            category_name="empty_accounts"
            ;;
        5)
            read -p "Enter minimum GB: " min_gb
            read -p "Enter maximum GB: " max_gb
            if [[ -n "$min_gb" && -n "$max_gb" ]]; then
                filter_condition="total_size_gb BETWEEN $min_gb AND $max_gb"
                category_name="custom_${min_gb}_to_${max_gb}GB"
            else
                echo -e "${RED}Both minimum and maximum values required${NC}"
                read -p "Press Enter to continue..."
                return 1
            fi
            ;;
        *)
            echo -e "${RED}Invalid selection${NC}"
            read -p "Press Enter to continue..."
            return 1
            ;;
    esac
    
    local export_file="accounts_storage_${category_name}_$(date +%Y%m%d_%H%M%S).csv"
    local export_path="$BACKUP_DIR/exports/$export_file"
    
    echo ""
    echo -e "${CYAN}Exporting accounts by storage usage...${NC}"
    
    # Ensure export directory exists
    mkdir -p "$(dirname "$export_path")"
    
    # Export accounts with storage data
    sqlite3 "$DATABASE_PATH" "
        .mode csv
        .headers on
        .output '$export_path'
        SELECT 
            a.email,
            a.full_name,
            a.suspended,
            a.org_unit_path,
            s.total_size_gb,
            s.scan_time as last_storage_scan
        FROM accounts a
        JOIN storage_size_history s ON a.email = s.email
        WHERE a.is_active = 1 
        AND s.scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
        AND $filter_condition
        ORDER BY s.total_size_gb DESC, a.email;
    " 2>/dev/null
    
    if [[ $? -eq 0 && -f "$export_path" ]]; then
        local record_count=$(tail -n +2 "$export_path" | wc -l)
        echo -e "${GREEN}âœ“ Export completed successfully${NC}"
        echo "  File: $export_path"
        echo "  Records: $record_count accounts"
        echo "  Category: $category_name"
        echo "  Includes: Current storage usage data"
    else
        echo -e "${RED}âœ— Export failed${NC}"
        echo "Check storage data availability and database connectivity"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

export_by_lifecycle_stage() {
    clear
    echo -e "${GREEN}=== Export by Lifecycle Stage ===${NC}"
    echo ""
    
    echo -e "${CYAN}Available Lifecycle Stages:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(stage, 'Active') as stage,
            COUNT(*) as account_count
        FROM accounts 
        WHERE is_active = 1
        GROUP BY stage
        ORDER BY account_count DESC;
    " 2>/dev/null
    
    echo ""
    read -p "Enter lifecycle stage (or 'all' for all stages): " stage_filter
    
    if [[ -z "$stage_filter" ]]; then
        echo -e "${RED}Stage required${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    local export_file="accounts_stage_$(echo "$stage_filter" | tr ' ' '_')_$(date +%Y%m%d_%H%M%S).csv"
    local export_path="$BACKUP_DIR/exports/$export_file"
    
    echo ""
    echo -e "${CYAN}Exporting accounts in stage: $stage_filter${NC}"
    
    # Ensure export directory exists
    mkdir -p "$(dirname "$export_path")"
    
    if [[ "$stage_filter" == "all" ]]; then
        # Export all accounts with stage information
        sqlite3 "$DATABASE_PATH" "
            .mode csv
            .headers on
            .output '$export_path'
            SELECT 
                email,
                full_name,
                suspended,
                COALESCE(stage, 'Active') as current_stage,
                org_unit_path,
                creation_time,
                last_login_time
            FROM accounts 
            WHERE is_active = 1
            ORDER BY stage, email;
        " 2>/dev/null
    else
        # Export specific stage
        local stage_condition=""
        if [[ "$stage_filter" == "Active" ]]; then
            stage_condition="stage IS NULL OR stage = 'Active'"
        else
            stage_condition="stage = '$stage_filter'"
        fi
        
        sqlite3 "$DATABASE_PATH" "
            .mode csv
            .headers on
            .output '$export_path'
            SELECT 
                a.email,
                a.full_name,
                a.suspended,
                COALESCE(a.stage, 'Active') as current_stage,
                a.org_unit_path,
                sh.changed_at as stage_entered_date,
                sh.changed_by as stage_changed_by
            FROM accounts a
            LEFT JOIN stage_history sh ON a.email = sh.email 
                AND sh.new_stage = a.stage
                AND sh.changed_at = (
                    SELECT MAX(changed_at) 
                    FROM stage_history sh2 
                    WHERE sh2.email = a.email
                )
            WHERE a.is_active = 1 AND ($stage_condition)
            ORDER BY sh.changed_at DESC, a.email;
        " 2>/dev/null
    fi
    
    if [[ $? -eq 0 && -f "$export_path" ]]; then
        local record_count=$(tail -n +2 "$export_path" | wc -l)
        echo -e "${GREEN}âœ“ Export completed successfully${NC}"
        echo "  File: $export_path"
        echo "  Records: $record_count accounts"
        echo "  Stage Filter: $stage_filter"
        echo "  Includes: Stage transition history"
    else
        echo -e "${RED}âœ— Export failed${NC}"
        echo "Check stage name and database connectivity"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Placeholder functions for remaining export options

custom_field_selection_export() {
    clear
    echo -e "${GREEN}=== Custom Field Selection Export ===${NC}"
    echo ""
    echo -e "${CYAN}Custom Field Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Interactive field selection"
    echo "â€¢ Custom column ordering"
    echo "â€¢ Field validation and formatting"
    echo "â€¢ Preview before export"
    echo ""
    read -p "Press Enter to continue..."
}

export_with_custom_filters() {
    clear
    echo -e "${GREEN}=== Export with Custom Filters ===${NC}"
    echo ""
    echo -e "${CYAN}Custom Filter Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Advanced filter builder"
    echo "â€¢ Multiple condition support"
    echo "â€¢ Filter templates and saving"
    echo "â€¢ Complex query construction"
    echo ""
    read -p "Press Enter to continue..."
}

export_search_results() {
    clear
    echo -e "${GREEN}=== Export Search Results ===${NC}"
    echo ""
    echo -e "${CYAN}Search Results Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Export from search queries"
    echo "â€¢ Multiple search criteria"
    echo "â€¢ Result filtering and sorting"
    echo "â€¢ Search history export"
    echo ""
    read -p "Press Enter to continue..."
}

export_statistics_summary() {
    clear
    echo -e "${GREEN}=== Export Statistics Summary ===${NC}"
    echo ""
    echo -e "${CYAN}Statistics Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Statistical summaries export"
    echo "â€¢ Chart and graph data"
    echo "â€¢ Trend analysis data"
    echo "â€¢ Dashboard data export"
    echo ""
    read -p "Press Enter to continue..."
}

export_storage_history_data() {
    clear
    echo -e "${GREEN}=== Export Storage History Data ===${NC}"
    echo ""
    echo -e "${CYAN}Storage History Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Historical storage data"
    echo "â€¢ Growth trend analysis"
    echo "â€¢ Storage pattern exports"
    echo "â€¢ Time-series data"
    echo ""
    read -p "Press Enter to continue..."
}

export_lifecycle_history() {
    clear
    echo -e "${GREEN}=== Export Lifecycle History ===${NC}"
    echo ""
    echo -e "${CYAN}Lifecycle History Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Complete lifecycle timelines"
    echo "â€¢ Stage transition history"
    echo "â€¢ Operation audit trails"
    echo "â€¢ Compliance reporting data"
    echo ""
    read -p "Press Enter to continue..."
}

export_security_compliance_data() {
    clear
    echo -e "${GREEN}=== Export Security Compliance Data ===${NC}"
    echo ""
    echo -e "${CYAN}Security Compliance Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Security policy compliance"
    echo "â€¢ Audit trail exports"
    echo "â€¢ Risk assessment data"
    echo "â€¢ Compliance violation reports"
    echo ""
    read -p "Press Enter to continue..."
}

export_contact_information() {
    clear
    echo -e "${GREEN}=== Export Contact Information ===${NC}"
    echo ""
    echo -e "${CYAN}Contact Information Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Contact details and profiles"
    echo "â€¢ Communication preferences"
    echo "â€¢ Directory information"
    echo "â€¢ Contact history and notes"
    echo ""
    read -p "Press Enter to continue..."
}

export_for_bulk_operations() {
    clear
    echo -e "${GREEN}=== Export for Bulk Operations ===${NC}"
    echo ""
    echo -e "${CYAN}Bulk Operations Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ CSV templates for bulk operations"
    echo "â€¢ Operation-specific formats"
    echo "â€¢ Validation templates"
    echo "â€¢ Batch processing formats"
    echo ""
    read -p "Press Enter to continue..."
}

export_stage_management_lists() {
    clear
    echo -e "${GREEN}=== Export Stage Management Lists ===${NC}"
    echo ""
    echo -e "${CYAN}Stage Management Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Stage-specific account lists"
    echo "â€¢ Management workflow exports"
    echo "â€¢ Action item lists"
    echo "â€¢ Progress tracking data"
    echo ""
    read -p "Press Enter to continue..."
}

export_analytics_data() {
    clear
    echo -e "${GREEN}=== Export Analytics Data ===${NC}"
    echo ""
    echo -e "${CYAN}Analytics Data Export - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Raw analytics data"
    echo "â€¢ Metrics and KPIs"
    echo "â€¢ Performance indicators"
    echo "â€¢ Business intelligence data"
    echo ""
    read -p "Press Enter to continue..."
}

export_complete_database_dump() {
    clear
    echo -e "${GREEN}=== Export Complete Database Dump ===${NC}"
    echo ""
    
    echo -e "${YELLOW}âš ï¸  WARNING: Complete Database Export âš ï¸${NC}"
    echo ""
    echo "This will export the entire GWOMBAT database including:"
    echo "â€¢ All account information"
    echo "â€¢ Historical data and audit trails"
    echo "â€¢ Security and compliance data"
    echo "â€¢ System configuration"
    echo ""
    echo -e "${RED}This file will contain sensitive data!${NC}"
    echo ""
    
    read -p "Type 'CONFIRM' to proceed with complete export: " confirm_export
    
    if [[ "$confirm_export" == "CONFIRM" ]]; then
        local export_file="gwombat_complete_dump_$(date +%Y%m%d_%H%M%S).sql"
        local export_path="$BACKUP_DIR/exports/$export_file"
        
        echo ""
        echo -e "${CYAN}Creating complete database dump...${NC}"
        
        # Ensure export directory exists
        mkdir -p "$(dirname "$export_path")"
        
        # Create SQL dump
        sqlite3 "$DATABASE_PATH" ".dump" > "$export_path" 2>/dev/null
        
        if [[ $? -eq 0 && -f "$export_path" ]]; then
            local file_size=$(du -h "$export_path" | cut -f1)
            echo -e "${GREEN}âœ“ Complete database dump created${NC}"
            echo "  File: $export_path"
            echo "  Size: $file_size"
            echo "  Format: SQL dump"
            echo ""
            echo -e "${YELLOW}Security Reminder: Secure this file appropriately${NC}"
        else
            echo -e "${RED}âœ— Database dump failed${NC}"
            echo "Check database connectivity and disk space"
        fi
    else
        echo -e "${GREEN}Export cancelled${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# User & Group Management Menu  
# User & Group Management Menu - SQLite-driven dynamic menu
user_group_management_menu() {
    render_menu "user_group_management"
}

# Group Operations Menu (extracted from buried shared drive menu)
group_operations_menu() {
    render_menu "user_group_management"
}

# Source database functions
source "${SCRIPTPATH}/shared-utilities/database_functions.sh" 2>/dev/null || {
    echo -e "${YELLOW}Warning: Database functions not available. Some features may be limited.${NC}"
}

# Source export functions
source "${SCRIPTPATH}/shared-utilities/export_functions.sh" 2>/dev/null || {
    echo -e "${YELLOW}Warning: Export functions not available. CSV export features may be limited.${NC}"
}

# List Management Menu
list_management_menu() {
    render_menu "account_list_management"
}

# CSV Import/Export Operations Menu - Comprehensive CSV data management
csv_operations_menu() {
    render_menu "account_list_management"
}

# Database Operations Menu - Comprehensive database maintenance and operations
database_operations_menu() {
    render_menu "system_administration"
}

# File Operations Menu - SQLite-driven dynamic menu
# File Operations Menu - SQLite-driven dynamic menu
file_operations_menu() {
    render_menu "file_drive_operations"
}

# Implementation functions for file operations menu items

file_listing_search() {
    echo -e "${CYAN}ðŸ“‹ File Listing & Search${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    echo "File listing and search options:"
    echo ""
    echo "1. List directory contents (local filesystem)"
    echo "2. Search for files/folders by name (Google Drive)"
    echo "3. Export file listing to CSV or Google Sheets"
    echo "4. Get file/folder details by ID"
    echo ""
    read -p "Select option (1-4): " list_choice
    echo ""
    
    case $list_choice in
        1)
            # List directory contents
            read -p "Enter directory path (default: current directory): " dir_path
            dir_path=${dir_path:-.}
            
            if [[ -d "$dir_path" ]]; then
                echo "Listing contents of $dir_path:"
                echo ""
                ls -la "$dir_path" | head -50
                
                echo ""
                read -p "Export this listing? (1=CSV, 2=Google Sheets, n=No): " export_choice
                
                if [[ "$export_choice" == "1" ]]; then
                    local csv_file="/tmp/file_listing_$(date +%Y%m%d_%H%M%S).csv"
                    echo "Filename,Type,Size,Modified,Permissions,Owner" > "$csv_file"
                    ls -la "$dir_path" | tail -n +2 | awk '{print $9","$1","$5","$6" "$7" "$8","$1","$3}' >> "$csv_file"
                    echo -e "${GREEN}Exported to: $csv_file${NC}"
                elif [[ "$export_choice" == "2" ]]; then
                    echo -e "${YELLOW}Google Sheets export will be implemented soon${NC}"
                fi
            else
                echo -e "${RED}Directory not found${NC}"
            fi
            ;;
        2)
            # Search for files/folders in Google Drive
            read -p "Enter search term: " search_term
            if [[ -n "$search_term" ]]; then
                echo "Searching Google Drive for '$search_term'..."
                $GAM user "$ADMIN_USER" show filelist query "name contains '$search_term'" fields "id,name,mimeType,size,createdTime,modifiedTime,owners" 2>/dev/null | head -20
                
                echo ""
                read -p "Get details for a specific file? Enter file ID (or press Enter to skip): " file_id
                if [[ -n "$file_id" ]]; then
                    manage_files_folders "$file_id"
                fi
            else
                echo "Search term required"
            fi
            ;;
        3)
            # Export file listing
            echo "Export file listing options:"
            echo "1. Export current directory listing to CSV"
            echo "2. Export Google Drive file list to CSV"
            echo "3. Export to Google Sheets"
            read -p "Select export option (1-3): " export_option
            
            case $export_option in
                1)
                    local csv_file="/tmp/directory_listing_$(date +%Y%m%d_%H%M%S).csv"
                    echo "Filename,Type,Size,Modified" > "$csv_file"
                    find . -maxdepth 1 -type f -exec stat -c "%n,file,%s,%y" {} \; >> "$csv_file"
                    find . -maxdepth 1 -type d -exec stat -c "%n,directory,%s,%y" {} \; >> "$csv_file"
                    echo -e "${GREEN}Exported to: $csv_file${NC}"
                    ;;
                2)
                    local csv_file="/tmp/drive_listing_$(date +%Y%m%d_%H%M%S).csv"
                    echo "Exporting Google Drive files..."
                    $GAM user "$ADMIN_USER" print filelist fields "id,name,mimeType,size,createdTime,modifiedTime,owners" > "$csv_file" 2>/dev/null
                    echo -e "${GREEN}Exported to: $csv_file${NC}"
                    ;;
                3)
                    echo -e "${YELLOW}Google Sheets export will be implemented soon${NC}"
                    ;;
            esac
            ;;
        4)
            # Get file/folder details by ID
            read -p "Enter Google Drive file/folder ID: " file_id
            if [[ -n "$file_id" ]]; then
                echo "Getting file details..."
                $GAM user "$ADMIN_USER" show fileinfo "$file_id" 2>/dev/null
                
                echo ""
                manage_files_folders "$file_id"
            else
                echo "File ID required"
            fi
            ;;
        *)
            echo "Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

manage_files_folders() {
    local file_id="$1"
    
    echo -e "${CYAN}ðŸ“ Manage Files & Folders${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    if [[ -z "$file_id" ]]; then
        # Step 1: File/Folder Selection
        echo "Step 1: File/Folder Selection"
        echo "1. Manually provide Google Drive file/folder ID"
        echo "2. Search for file/folder by name"
        echo "3. Select multiple files from CSV (Bulk action)"
        echo ""
        read -p "Select option (1-3): " selection_choice
        
        case $selection_choice in
            1)
                read -p "Enter file/folder ID: " file_id
                ;;
            2)
                read -p "Enter search term: " search_term
                if [[ -n "$search_term" ]]; then
                    echo "Searching..."
                    $GAM user "$ADMIN_USER" show filelist query "name contains '$search_term'" fields "id,name" 2>/dev/null | head -10
                    read -p "Enter file ID from above: " file_id
                fi
                ;;
            3)
                read -p "Enter CSV file path: " csv_path
                if [[ -f "$csv_path" ]]; then
                    echo "Processing bulk operation from CSV..."
                    # Implement bulk processing
                    echo -e "${YELLOW}Bulk processing will be implemented soon${NC}"
                    return
                else
                    echo -e "${RED}CSV file not found${NC}"
                    return
                fi
                ;;
        esac
    fi
    
    if [[ -n "$file_id" ]]; then
        # Step 2: Actions
        echo ""
        echo "Step 2: Select Action"
        echo "1. Rename"
        echo "2. Change ownership"
        echo "3. Change sharing settings"
        echo "4. Add/remove labels"
        echo "5. Move to new location"
        echo ""
        read -p "Select action (1-5): " action_choice
        
        case $action_choice in
            1)
                read -p "Enter new name: " new_name
                if [[ -n "$new_name" ]]; then
                    echo "Renaming file..."
                    $GAM user "$ADMIN_USER" update drivefile "$file_id" name "$new_name" 2>/dev/null
                    echo -e "${GREEN}File renamed successfully${NC}"
                fi
                ;;
            2)
                read -p "Enter new owner email: " new_owner
                if [[ -n "$new_owner" ]]; then
                    echo "Changing ownership..."
                    $GAM user "$ADMIN_USER" add drivefileacl "$file_id" user "$new_owner" role owner 2>/dev/null
                    echo -e "${GREEN}Ownership changed successfully${NC}"
                fi
                ;;
            3)
                echo "Sharing options:"
                echo "1. Add user permission"
                echo "2. Remove user permission"
                echo "3. Make public"
                echo "4. Make private"
                read -p "Select sharing option (1-4): " share_option
                
                case $share_option in
                    1)
                        read -p "Enter user email: " user_email
                        read -p "Enter role (reader/writer/commenter): " role
                        $GAM user "$ADMIN_USER" add drivefileacl "$file_id" user "$user_email" role "$role" 2>/dev/null
                        echo -e "${GREEN}Permission added${NC}"
                        ;;
                    2)
                        read -p "Enter user email to remove: " user_email
                        $GAM user "$ADMIN_USER" delete drivefileacl "$file_id" "$user_email" 2>/dev/null
                        echo -e "${GREEN}Permission removed${NC}"
                        ;;
                    3)
                        $GAM user "$ADMIN_USER" add drivefileacl "$file_id" anyone role reader 2>/dev/null
                        echo -e "${GREEN}File made public${NC}"
                        ;;
                    4)
                        $GAM user "$ADMIN_USER" delete drivefileacl "$file_id" anyone 2>/dev/null
                        echo -e "${GREEN}File made private${NC}"
                        ;;
                esac
                ;;
            4)
                echo -e "${YELLOW}Label management will be implemented soon${NC}"
                ;;
            5)
                read -p "Enter new parent folder ID: " new_parent
                if [[ -n "$new_parent" ]]; then
                    echo "Moving file..."
                    $GAM user "$ADMIN_USER" update drivefile "$file_id" parentid "$new_parent" 2>/dev/null
                    echo -e "${GREEN}File moved successfully${NC}"
                fi
                ;;
        esac
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

batch_file_operations() {
    echo -e "${CYAN}ðŸ“¦ Batch File Operations${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    echo "Process multiple files from a CSV file"
    echo ""
    echo "CSV format should include:"
    echo "- Column 1: File ID"
    echo "- Column 2: Action (rename/move/share/delete)"
    echo "- Column 3: Action parameter (new name, new location, etc.)"
    echo ""
    
    read -p "Enter CSV file path: " csv_path
    
    if [[ -f "$csv_path" ]]; then
        echo "Processing CSV file..."
        local processed=0
        local failed=0
        
        while IFS=',' read -r file_id action parameter; do
            [[ "$file_id" == "file_id" ]] && continue  # Skip header
            
            case $action in
                "rename")
                    if $GAM user "$ADMIN_USER" update drivefile "$file_id" name "$parameter" 2>/dev/null; then
                        ((processed++))
                    else
                        ((failed++))
                    fi
                    ;;
                "move")
                    if $GAM user "$ADMIN_USER" update drivefile "$file_id" parentid "$parameter" 2>/dev/null; then
                        ((processed++))
                    else
                        ((failed++))
                    fi
                    ;;
                "share")
                    if $GAM user "$ADMIN_USER" add drivefileacl "$file_id" user "$parameter" role reader 2>/dev/null; then
                        ((processed++))
                    else
                        ((failed++))
                    fi
                    ;;
                "delete")
                    if $GAM user "$ADMIN_USER" delete drivefile "$file_id" 2>/dev/null; then
                        ((processed++))
                    else
                        ((failed++))
                    fi
                    ;;
                *)
                    echo "Unknown action: $action"
                    ((failed++))
                    ;;
            esac
        done < "$csv_path"
        
        echo ""
        echo -e "${GREEN}Processed: $processed files${NC}"
        echo -e "${RED}Failed: $failed files${NC}"
    else
        echo -e "${RED}CSV file not found${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}
# Menu functions for consolidated operations

# File & Drive Operations Function Dispatcher
file_drive_operations_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        "file_operations_menu") file_operations_menu ;;
        "shared_drive_menu") shared_drive_menu ;;
        "backup_operations_menu") 
            echo -e "${CYAN}Backup Operations - Coming Soon${NC}"
            echo "This feature will include:"
            echo "â€¢ File backup and restore"
            echo "â€¢ Automated backup scheduling"
            echo "â€¢ Backup verification and integrity checks"
            echo "â€¢ Cloud storage integration"
            read -p "Press Enter to continue..."
            ;;
        "drive_cleanup_menu")
            echo -e "${CYAN}Drive Cleanup Operations - Coming Soon${NC}"
            echo "This feature will include:"
            echo "â€¢ Remove duplicate files"
            echo "â€¢ Clean up temporary files"
            echo "â€¢ Organize folder structures"
            echo "â€¢ Archive old files"
            read -p "Press Enter to continue..."
            ;;
        "permission_management_menu") permission_management_menu ;;
        "export_data_menu") export_data_menu ;;
        *)
            echo -e "${RED}Unknown file operations function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# File & Drive Operations Menu - SQLite-driven implementation
# File management, shared drives, and permission operations interface
# Uses database-driven menu items from file_drive_operations section
file_drive_operations_menu() {
    render_menu "file_drive_operations"
}

# Permission Management Function Dispatcher
permission_management_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # File Permissions (1-5)
        "check_file_permissions"|"modify_file_permissions"|"grant_file_access"|"revoke_file_access"|"transfer_file_ownership")
            echo -e "${CYAN}File Permission Operation: $function_name${NC}"
            echo "This feature will provide comprehensive file permission management."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ View and modify individual file permissions"
            echo "â€¢ Grant and revoke access for users and groups"
            echo "â€¢ Transfer file ownership"
            echo "â€¢ Audit file access patterns"
            read -p "Press Enter to continue..."
            ;;
        
        # Folder Permissions (6-9)
        "check_folder_permissions"|"modify_folder_permissions"|"grant_folder_access"|"revoke_folder_access")
            echo -e "${CYAN}Folder Permission Operation: $function_name${NC}"
            echo "This feature will provide comprehensive folder permission management."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Recursive folder permission viewing and modification"
            echo "â€¢ Bulk access management for users and groups"
            echo "â€¢ Inheritance control and permission propagation"
            read -p "Press Enter to continue..."
            ;;
        
        # Drive Permissions (10-13)
        "check_drive_permissions"|"modify_drive_permissions"|"grant_drive_access"|"revoke_drive_access")
            echo -e "${CYAN}Shared Drive Permission Operation: $function_name${NC}"
            echo "This feature will provide shared drive permission management."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Shared drive role management (viewer, editor, manager)"
            echo "â€¢ Add and remove users from shared drives"
            echo "â€¢ Permission auditing and reporting"
            read -p "Press Enter to continue..."
            ;;
        
        # Security Operations (14-17)
        "audit_permissions"|"detect_public_files"|"security_scan"|"compliance_check")
            echo -e "${CYAN}Security Operation: $function_name${NC}"
            echo "This feature will provide advanced security and compliance capabilities."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Comprehensive permission auditing"
            echo "â€¢ Public file detection and remediation"
            echo "â€¢ Security vulnerability scanning"
            echo "â€¢ Compliance policy checking"
            read -p "Press Enter to continue..."
            ;;
        
        # Batch Operations (18-20)
        "batch_permission_changes"|"bulk_ownership_transfer"|"export_permissions_report")
            echo -e "${CYAN}Batch Operation: $function_name${NC}"
            echo "This feature will provide bulk permission management capabilities."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Batch permission changes across multiple files/folders"
            echo "â€¢ Bulk ownership transfers"
            echo "â€¢ Comprehensive permission reporting and export"
            read -p "Press Enter to continue..."
            ;;
        
        *)
            echo -e "${RED}Unknown permission management function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Permission Management Menu - SQLite-driven implementation
# File and folder permission management and security operations interface
# Uses database-driven menu items from permission_management section
permission_management_menu() {
    render_menu "analysis_discovery"
}
# File Permission Functions

view_file_permissions() {
    clear
    echo -e "${GREEN}=== View File Permissions ===${NC}"
    echo ""
    
    read -p "Enter file ID or file name: " file_input
    
    if [[ -z "$file_input" ]]; then
        echo -e "${RED}File input required${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo -e "${CYAN}Retrieving permissions for: $file_input${NC}"
    echo ""
    
    # Use GAM to show file permissions
    $GAM_PATH info drive "$file_input" permissions 2>/dev/null || {
        echo -e "${RED}Failed to retrieve file permissions. Check file ID/name.${NC}"
        read -p "Press Enter to continue..."
        return 1
    }
    
    echo ""
    read -p "Press Enter to continue..."
}

list_shared_files_by_user() {
    clear
    echo -e "${GREEN}=== List Shared Files by User ===${NC}"
    echo ""
    
    read -p "Enter user email: " user_email
    
    if [[ -z "$user_email" ]]; then
        echo -e "${RED}User email required${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo -e "${CYAN}Finding files shared by: $user_email${NC}"
    echo ""
    
    # List files owned by user with sharing information
    $GAM_PATH user "$user_email" print files fields id,name,permissions,shared,webViewLink \
        query "sharedWithMe=false" 2>/dev/null || {
        echo -e "${RED}Failed to retrieve shared files for user${NC}"
        read -p "Press Enter to continue..."
        return 1
    }
    
    echo ""
    read -p "Press Enter to continue..."
}

manage_file_sharing() {
    clear
    echo -e "${GREEN}=== Manage File Sharing ===${NC}"
    echo ""
    
    echo -e "${YELLOW}File Sharing Operations:${NC}"
    echo "1. Add sharing permission"
    echo "2. Remove sharing permission" 
    echo "3. Update permission level"
    echo "4. List current permissions"
    echo ""
    read -p "Select operation (1-4): " share_op
    
    case $share_op in
        1)
            read -p "Enter file ID: " file_id
            read -p "Enter user/group email: " recipient
            read -p "Enter role (reader/writer/owner): " role
            
            if [[ -n "$file_id" && -n "$recipient" && -n "$role" ]]; then
                echo -e "${CYAN}Adding $role permission for $recipient...${NC}"
                $GAM_PATH add permission "$file_id" user "$recipient" role "$role" 2>/dev/null && {
                    echo -e "${GREEN}âœ“ Permission added successfully${NC}"
                } || {
                    echo -e "${RED}âœ— Failed to add permission${NC}"
                }
            else
                echo -e "${RED}All fields required${NC}"
            fi
            ;;
        2)
            read -p "Enter file ID: " file_id
            read -p "Enter permission ID or user email: " perm_target
            
            if [[ -n "$file_id" && -n "$perm_target" ]]; then
                echo -e "${CYAN}Removing permission...${NC}"
                $GAM_PATH remove permission "$file_id" "$perm_target" 2>/dev/null && {
                    echo -e "${GREEN}âœ“ Permission removed successfully${NC}"
                } || {
                    echo -e "${RED}âœ— Failed to remove permission${NC}"
                }
            else
                echo -e "${RED}File ID and permission target required${NC}"
            fi
            ;;
        3)
            read -p "Enter file ID: " file_id
            read -p "Enter permission ID or user email: " perm_target
            read -p "Enter new role (reader/writer/owner): " new_role
            
            if [[ -n "$file_id" && -n "$perm_target" && -n "$new_role" ]]; then
                echo -e "${CYAN}Updating permission to $new_role...${NC}"
                $GAM_PATH update permission "$file_id" "$perm_target" role "$new_role" 2>/dev/null && {
                    echo -e "${GREEN}âœ“ Permission updated successfully${NC}"
                } || {
                    echo -e "${RED}âœ— Failed to update permission${NC}"
                }
            else
                echo -e "${RED}All fields required${NC}"
            fi
            ;;
        4)
            read -p "Enter file ID: " file_id
            if [[ -n "$file_id" ]]; then
                $GAM_PATH info drive "$file_id" permissions
            else
                echo -e "${RED}File ID required${NC}"
            fi
            ;;
        *)
            echo -e "${RED}Invalid option${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

remove_external_sharing() {
    clear
    echo -e "${GREEN}=== Remove External Sharing ===${NC}"
    echo ""
    
    read -p "Enter user email to audit: " user_email
    
    if [[ -z "$user_email" ]]; then
        echo -e "${RED}User email required${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo -e "${CYAN}Scanning for externally shared files...${NC}"
    echo ""
    
    # Find files shared outside domain
    domain=$(echo "$ADMIN_USER" | cut -d'@' -f2)
    
    echo -e "${YELLOW}Files shared outside $domain domain:${NC}"
    
    # This would require more complex GAM logic to identify external shares
    $GAM_PATH user "$user_email" print files fields id,name,permissions \
        query "sharedWithMe=false" 2>/dev/null | \
        grep -v "@$domain" | head -20
    
    echo ""
    echo -e "${YELLOW}Note: Use 'manage_file_sharing' to remove specific external permissions${NC}"
    echo ""
    read -p "Press Enter to continue..."
}

file_sharing_reports() {
    clear
    echo -e "${GREEN}=== File Sharing Reports ===${NC}"
    echo ""
    
    echo -e "${YELLOW}Available Reports:${NC}"
    echo "1. Files shared outside domain"
    echo "2. Most shared files"
    echo "3. Users with most shared files"
    echo "4. Public access files"
    echo ""
    read -p "Select report (1-4): " report_type
    
    case $report_type in
        1)
            echo -e "${CYAN}Generating external sharing report...${NC}"
            echo "This would scan all users for files shared outside the domain"
            ;;
        2)
            echo -e "${CYAN}Generating most shared files report...${NC}"
            echo "This would identify files with the most sharing permissions"
            ;;
        3)
            echo -e "${CYAN}Generating user sharing summary...${NC}"
            echo "This would rank users by number of shared files"
            ;;
        4)
            echo -e "${CYAN}Generating public access report...${NC}"
            echo "This would find files with public/anyone access"
            ;;
        *)
            echo -e "${RED}Invalid report type${NC}"
            ;;
    esac
    
    echo ""
    echo -e "${YELLOW}Note: Full reporting functionality coming soon${NC}"
    echo ""
    read -p "Press Enter to continue..."
}

# Folder Permission Functions (placeholder implementations)

view_folder_permissions() {
    clear
    echo -e "${GREEN}=== View Folder Permissions ===${NC}"
    echo ""
    echo -e "${CYAN}Folder Permission Viewer - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ View folder-level permissions"
    echo "â€¢ Recursive permission analysis"  
    echo "â€¢ Inherited permissions tracking"
    echo "â€¢ Folder access rights summary"
    echo ""
    read -p "Press Enter to continue..."
}

manage_folder_access() {
    clear
    echo -e "${GREEN}=== Manage Folder Access ===${NC}"
    echo ""
    echo -e "${CYAN}Folder Access Management - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Add/remove folder permissions"
    echo "â€¢ Bulk folder access updates"
    echo "â€¢ Folder sharing controls"
    echo "â€¢ Access level modifications"
    echo ""
    read -p "Press Enter to continue..."
}

bulk_folder_permission_updates() {
    clear
    echo -e "${GREEN}=== Bulk Folder Permission Updates ===${NC}"
    echo ""
    echo -e "${CYAN}Bulk Folder Operations - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Mass folder permission changes"
    echo "â€¢ CSV-based permission updates"
    echo "â€¢ Recursive permission application"
    echo "â€¢ Batch access modifications"
    echo ""
    read -p "Press Enter to continue..."
}

inherit_permission_management() {
    clear
    echo -e "${GREEN}=== Inherit Permission Management ===${NC}"
    echo ""
    echo -e "${CYAN}Permission Inheritance - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Inheritance chain analysis"
    echo "â€¢ Permission propagation control"
    echo "â€¢ Override management"
    echo "â€¢ Inheritance troubleshooting"
    echo ""
    read -p "Press Enter to continue..."
}

# Drive Permission Functions (placeholder implementations)

audit_drive_access_rights() {
    clear
    echo -e "${GREEN}=== Audit Drive Access Rights ===${NC}"
    echo ""
    echo -e "${CYAN}Drive Access Audit - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Comprehensive drive access review"
    echo "â€¢ Permission matrix generation"
    echo "â€¢ Access rights verification"
    echo "â€¢ Compliance checking"
    echo ""
    read -p "Press Enter to continue..."
}

user_drive_access_summary() {
    clear
    echo -e "${GREEN}=== User Drive Access Summary ===${NC}"
    echo ""
    echo -e "${CYAN}User Access Summary - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Per-user access summary"
    echo "â€¢ Drive access mapping"
    echo "â€¢ Permission level breakdown"
    echo "â€¢ Access history tracking"
    echo ""
    read -p "Press Enter to continue..."
}

identify_overprivileged_access() {
    clear
    echo -e "${GREEN}=== Identify Over-Privileged Access ===${NC}"
    echo ""
    echo -e "${CYAN}Over-Privilege Detection - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Excessive permission detection"
    echo "â€¢ Role appropriateness analysis"
    echo "â€¢ Access level recommendations"
    echo "â€¢ Security risk assessment"
    echo ""
    read -p "Press Enter to continue..."
}

generate_permission_reports() {
    clear
    echo -e "${GREEN}=== Generate Permission Reports ===${NC}"
    echo ""
    echo -e "${CYAN}Permission Reporting - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Comprehensive permission reports"
    echo "â€¢ Export to various formats"
    echo "â€¢ Scheduled report generation"
    echo "â€¢ Custom report templates"
    echo ""
    read -p "Press Enter to continue..."
}

# Security Operation Functions (placeholder implementations)

security_policy_enforcement() {
    clear
    echo -e "${GREEN}=== Security Policy Enforcement ===${NC}"
    echo ""
    echo -e "${CYAN}Policy Enforcement - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Automated policy compliance"
    echo "â€¢ Policy violation remediation"
    echo "â€¢ Custom security rules"
    echo "â€¢ Continuous monitoring"
    echo ""
    read -p "Press Enter to continue..."
}

find_public_access_files() {
    clear
    echo -e "${GREEN}=== Find Files with Public Access ===${NC}"
    echo ""
    
    echo -e "${CYAN}Scanning for publicly accessible files...${NC}"
    echo ""
    
    # This would scan for files with 'anyone' permissions
    echo -e "${YELLOW}Files with public access:${NC}"
    
    # GAM command to find public files (simplified version)
    $GAM_PATH all users print files fields id,name,permissions \
        query "visibility='anyoneCanFind' or visibility='anyoneWithLink'" 2>/dev/null | head -20 || {
        echo -e "${YELLOW}No public files found or access denied${NC}"
    }
    
    echo ""
    echo -e "${YELLOW}Note: Review these files for security compliance${NC}"
    echo ""
    read -p "Press Enter to continue..."
}

permission_violation_detection() {
    clear
    echo -e "${GREEN}=== Permission Violation Detection ===${NC}"
    echo ""
    echo -e "${CYAN}Violation Detection - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Policy violation scanning"
    echo "â€¢ Anomaly detection"
    echo "â€¢ Compliance checking"
    echo "â€¢ Automated alerting"
    echo ""
    read -p "Press Enter to continue..."
}

emergency_access_revocation() {
    clear
    echo -e "${GREEN}=== Emergency Access Revocation ===${NC}"
    echo ""
    echo -e "${RED}âš ï¸  WARNING: Emergency Access Revocation âš ï¸${NC}"
    echo ""
    echo "This feature allows immediate revocation of user access rights"
    echo ""
    
    read -p "Enter user email for emergency revocation: " emergency_user
    
    if [[ -n "$emergency_user" ]]; then
        echo ""
        echo -e "${YELLOW}This would revoke access for: $emergency_user${NC}"
        echo "â€¢ Remove all file permissions"
        echo "â€¢ Revoke shared drive access"
        echo "â€¢ Disable external sharing"
        echo ""
        read -p "Type 'CONFIRM' to proceed (or Enter to cancel): " confirm
        
        if [[ "$confirm" == "CONFIRM" ]]; then
            echo -e "${CYAN}Emergency revocation initiated...${NC}"
            echo -e "${YELLOW}Note: Full implementation coming soon${NC}"
        else
            echo -e "${GREEN}Operation cancelled${NC}"
        fi
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Batch Operation Functions (placeholder implementations)

bulk_permission_changes() {
    clear
    echo -e "${GREEN}=== Bulk Permission Changes ===${NC}"
    echo ""
    echo -e "${CYAN}Bulk Permission Operations - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ CSV-based permission updates"
    echo "â€¢ Mass permission modifications"
    echo "â€¢ Batch access control"
    echo "â€¢ Progress tracking"
    echo ""
    read -p "Press Enter to continue..."
}

transfer_file_ownership() {
    clear
    echo -e "${GREEN}=== Transfer File Ownership ===${NC}"
    echo ""
    
    read -p "Enter current owner email: " current_owner
    read -p "Enter new owner email: " new_owner
    
    if [[ -n "$current_owner" && -n "$new_owner" ]]; then
        echo ""
        echo -e "${CYAN}Transferring ownership from $current_owner to $new_owner${NC}"
        echo ""
        echo -e "${YELLOW}This operation will:${NC}"
        echo "â€¢ Transfer all file ownership"
        echo "â€¢ Update permission structures"
        echo "â€¢ Maintain access continuity"
        echo ""
        echo -e "${YELLOW}Note: Full implementation coming soon${NC}"
    else
        echo -e "${RED}Both owner emails required${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

cleanup_broken_permissions() {
    clear
    echo -e "${GREEN}=== Clean Up Broken Permissions ===${NC}"
    echo ""
    echo -e "${CYAN}Permission Cleanup - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Detect orphaned permissions"
    echo "â€¢ Remove invalid access entries"
    echo "â€¢ Fix broken inheritance"
    echo "â€¢ Optimize permission structures"
    echo ""
    read -p "Press Enter to continue..."
}

# NOTE: File Discovery functionality has been moved to shared-utilities/standalone-file-analysis-tools.sh
# This was done because the file analysis tools were focused on local filesystem
# analysis rather than Google Drive/Shared Drive management (GWOMBAT's core purpose)

# Placeholder function for removed File Discovery
file_discovery_removed_notice() {
    clear
    echo -e "${YELLOW}ðŸ“„ File Discovery Tools - Relocated${NC}"
    echo ""
    echo -e "${CYAN}The comprehensive file discovery and analysis tools have been moved${NC}"
    echo -e "${CYAN}to a separate standalone script for general filesystem analysis.${NC}"
    echo ""
    echo -e "${WHITE}Moved tools included:${NC}"
    echo "â€¢ âš¡ Fast Duplicate Scan (5 methods)"
    echo "â€¢ ðŸ“ Directory Structure Analysis (5 analysis types)"
    echo "â€¢ ðŸ·ï¸ File Type Distribution (5 analysis methods)"
    echo "â€¢ ðŸ” Duplicate File Finder (4 detection methods)"
    echo "â€¢ ðŸ“Š Similarity Analysis (5 analysis types)"
    echo "â€¢ ðŸ§¹ Duplicate Cleanup Assistant (5 cleanup operations)"
    echo "â€¢ ðŸ“‹ Duplicate Report Generator (5 report types)"
    echo "â€¢ ðŸ“ File Operations Suite (10 tools, 50+ operations)"
    echo "â€¢ ðŸ›¡ï¸ File Security Scanner (5 security scans)"
    echo ""
    echo -e "${GREEN}Available in: ${WHITE}./shared-utilities/standalone-file-analysis-tools.sh${NC}"
    echo ""
    echo -e "${YELLOW}Reason for move:${NC}"
    echo "These tools focus on local filesystem analysis rather than"
    echo "Google Drive/Shared Drive management (GWOMBAT's core purpose)."
    echo ""
    echo -e "${YELLOW}Future tasks not yet implemented:${NC}"
    echo "â€¢ File Age Analysis â€¢ File Size Patterns â€¢ File Dependency Mapping"
    echo "â€¢ Orphaned File Detection â€¢ Temporary File Cleanup â€¢ Hidden File Discovery"
    echo "â€¢ File Inventory Generator â€¢ Custom Discovery Rules"
    echo ""
    read -p "Press Enter to return to Analysis & Discovery menu..."
}

file_discovery_menu() {
    render_menu "analysis_discovery"
}

# Fast Duplicate Scan Menu - Optimized for speed and efficiency
fast_duplicate_scan_menu() {
    render_menu "analysis_discovery"
}

# Directory Structure Analysis Menu - Comprehensive directory analysis and visualization
directory_structure_analysis_menu() {
    render_menu "analysis_discovery"
}

# File Type Distribution Menu - Comprehensive file type analysis and categorization
file_type_distribution_menu() {
    render_menu "analysis_discovery"
}

# Analysis & Discovery Menu - SQLite-driven implementation
# Provides comprehensive analysis tools and data discovery operations
# Uses database-driven menu items from analysis_discovery section
analysis_discovery_menu() {
    render_menu "analysis_discovery"
}

# System Diagnostics Menu
system_diagnostics_menu() {
    render_menu "system_administration"
}

# Account Analysis Menu
account_analysis_menu() {
    render_menu "account_analysis"
}

# Account Analysis Functions

search_accounts_by_criteria() {
    clear
    echo -e "${GREEN}=== Search Accounts by Criteria ===${NC}"
    echo ""
    
    echo -e "${CYAN}Advanced account search with multiple criteria${NC}"
    echo ""
    echo -e "${YELLOW}Search Options:${NC}"
    echo "1. Search by email pattern"
    echo "2. Search by name"
    echo "3. Search by organizational unit"
    echo "4. Search by account status"
    echo "5. Search by creation date"
    echo "6. Search by storage usage"
    echo "7. Multi-criteria search"
    echo ""
    read -p "Select search type (1-7): " search_type
    
    case $search_type in
        1)
            read -p "Enter email pattern (use % for wildcards): " email_pattern
            if [[ -n "$email_pattern" ]]; then
                echo -e "${CYAN}Searching accounts with email pattern: $email_pattern${NC}"
                echo ""
                sqlite3 "$DATABASE_PATH" "
                    SELECT 
                        email,
                        full_name,
                        suspended,
                        admin,
                        org_unit_path,
                        creation_time
                    FROM accounts 
                    WHERE is_active = 1 AND email LIKE '$email_pattern'
                    ORDER BY email
                    LIMIT 50;
                " 2>/dev/null || echo "No matching accounts found"
            fi
            ;;
        2)
            read -p "Enter name to search for: " name_pattern
            if [[ -n "$name_pattern" ]]; then
                echo -e "${CYAN}Searching accounts with name: $name_pattern${NC}"
                echo ""
                sqlite3 "$DATABASE_PATH" "
                    SELECT 
                        email,
                        full_name,
                        suspended,
                        admin,
                        org_unit_path
                    FROM accounts 
                    WHERE is_active = 1 AND full_name LIKE '%$name_pattern%'
                    ORDER BY full_name
                    LIMIT 50;
                " 2>/dev/null || echo "No matching accounts found"
            fi
            ;;
        3)
            read -p "Enter organizational unit path: " ou_path
            if [[ -n "$ou_path" ]]; then
                echo -e "${CYAN}Searching accounts in OU: $ou_path${NC}"
                echo ""
                sqlite3 "$DATABASE_PATH" "
                    SELECT 
                        email,
                        full_name,
                        suspended,
                        admin,
                        org_unit_path,
                        creation_time
                    FROM accounts 
                    WHERE is_active = 1 AND org_unit_path LIKE '%$ou_path%'
                    ORDER BY org_unit_path, email
                    LIMIT 50;
                " 2>/dev/null || echo "No matching accounts found"
            fi
            ;;
        4)
            echo "Account status options:"
            echo "1. Active accounts"
            echo "2. Suspended accounts"
            echo "3. Admin accounts"
            echo "4. Accounts with 2FA"
            read -p "Select status (1-4): " status_option
            
            case $status_option in
                1)
                    echo -e "${CYAN}Active accounts:${NC}"
                    sqlite3 "$DATABASE_PATH" "
                        SELECT email, full_name, org_unit_path, creation_time
                        FROM accounts 
                        WHERE is_active = 1 AND suspended = 'False'
                        ORDER BY email LIMIT 50;
                    " 2>/dev/null
                    ;;
                2)
                    echo -e "${CYAN}Suspended accounts:${NC}"
                    sqlite3 "$DATABASE_PATH" "
                        SELECT email, full_name, org_unit_path, stage
                        FROM accounts 
                        WHERE is_active = 1 AND suspended = 'True'
                        ORDER BY email LIMIT 50;
                    " 2>/dev/null
                    ;;
                3)
                    echo -e "${CYAN}Admin accounts:${NC}"
                    sqlite3 "$DATABASE_PATH" "
                        SELECT email, full_name, admin, super_admin, two_factor
                        FROM accounts 
                        WHERE is_active = 1 AND admin = 'True'
                        ORDER BY super_admin DESC, email;
                    " 2>/dev/null
                    ;;
                4)
                    echo -e "${CYAN}Accounts with 2FA:${NC}"
                    sqlite3 "$DATABASE_PATH" "
                        SELECT email, full_name, admin, org_unit_path
                        FROM accounts 
                        WHERE is_active = 1 AND two_factor = 'True'
                        ORDER BY admin DESC, email LIMIT 50;
                    " 2>/dev/null
                    ;;
            esac
            ;;
        5)
            read -p "Enter start date (YYYY-MM-DD): " start_date
            read -p "Enter end date (YYYY-MM-DD): " end_date
            if [[ -n "$start_date" && -n "$end_date" ]]; then
                echo -e "${CYAN}Accounts created between $start_date and $end_date:${NC}"
                echo ""
                sqlite3 "$DATABASE_PATH" "
                    SELECT 
                        email,
                        full_name,
                        creation_time,
                        org_unit_path,
                        suspended
                    FROM accounts 
                    WHERE is_active = 1 
                    AND DATE(creation_time) BETWEEN '$start_date' AND '$end_date'
                    ORDER BY creation_time DESC
                    LIMIT 50;
                " 2>/dev/null || echo "No accounts found in date range"
            fi
            ;;
        6)
            echo "Storage usage ranges:"
            echo "1. High usage (>15 GB)"
            echo "2. Medium usage (5-15 GB)" 
            echo "3. Low usage (<5 GB)"
            echo "4. Custom range"
            read -p "Select range (1-4): " storage_range
            
            local storage_condition=""
            case $storage_range in
                1) storage_condition="s.total_size_gb > 15" ;;
                2) storage_condition="s.total_size_gb BETWEEN 5 AND 15" ;;
                3) storage_condition="s.total_size_gb < 5" ;;
                4)
                    read -p "Enter minimum GB: " min_gb
                    read -p "Enter maximum GB: " max_gb
                    if [[ -n "$min_gb" && -n "$max_gb" ]]; then
                        storage_condition="s.total_size_gb BETWEEN $min_gb AND $max_gb"
                    fi
                    ;;
            esac
            
            if [[ -n "$storage_condition" ]]; then
                echo -e "${CYAN}Accounts by storage usage:${NC}"
                echo ""
                sqlite3 "$DATABASE_PATH" "
                    SELECT 
                        a.email,
                        a.full_name,
                        s.total_size_gb,
                        a.org_unit_path,
                        a.suspended
                    FROM accounts a
                    JOIN storage_size_history s ON a.email = s.email
                    WHERE a.is_active = 1 
                    AND s.scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
                    AND $storage_condition
                    ORDER BY s.total_size_gb DESC
                    LIMIT 50;
                " 2>/dev/null || echo "No accounts found with specified storage usage"
            fi
            ;;
        7)
            echo -e "${CYAN}Multi-criteria Search - Coming Soon${NC}"
            echo "This will allow combining multiple search criteria"
            ;;
        *)
            echo -e "${RED}Invalid search type${NC}"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

account_profile_analysis() {
    clear
    echo -e "${GREEN}=== Account Profile Analysis ===${NC}"
    echo ""
    
    read -p "Enter email address to analyze: " analyze_email
    
    if [[ -z "$analyze_email" ]]; then
        echo -e "${RED}Email address required${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo -e "${CYAN}Analyzing account: $analyze_email${NC}"
    echo ""
    
    # Basic account information
    echo -e "${YELLOW}Basic Information:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Email: ' || email,
            'Name: ' || COALESCE(full_name, 'Not available'),
            'Status: ' || CASE WHEN suspended = 'True' THEN 'Suspended' ELSE 'Active' END,
            'Admin: ' || CASE WHEN admin = 'True' THEN 'Yes' ELSE 'No' END,
            'Super Admin: ' || CASE WHEN super_admin = 'True' THEN 'Yes' ELSE 'No' END,
            '2FA: ' || CASE WHEN two_factor = 'True' THEN 'Enabled' ELSE 'Disabled' END,
            'OU: ' || COALESCE(org_unit_path, 'Not set'),
            'Created: ' || COALESCE(creation_time, 'Unknown'),
            'Last Login: ' || COALESCE(last_login_time, 'Never')
        FROM accounts 
        WHERE email = '$analyze_email' AND is_active = 1;
    " 2>/dev/null || echo "Account not found"
    
    echo ""
    
    # Storage information
    echo -e "${YELLOW}Storage Information:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Current Storage: ' || COALESCE(ROUND(total_size_gb, 2), 0) || ' GB',
            'Last Scan: ' || COALESCE(scan_time, 'Never scanned')
        FROM storage_size_history 
        WHERE email = '$analyze_email' 
        AND scan_time = (SELECT MAX(scan_time) FROM storage_size_history WHERE email = '$analyze_email');
    " 2>/dev/null || echo "No storage data available"
    
    echo ""
    
    # Lifecycle information
    echo -e "${YELLOW}Lifecycle Information:${NC}"
    local current_stage=$(sqlite3 "$DATABASE_PATH" "SELECT COALESCE(stage, 'Active') FROM accounts WHERE email = '$analyze_email';" 2>/dev/null)
    echo "Current Stage: $current_stage"
    
    if [[ "$current_stage" != "Active" && "$current_stage" != "" ]]; then
        echo -e "${YELLOW}Recent Stage Changes:${NC}"
        sqlite3 "$DATABASE_PATH" "
            SELECT 
                'Date: ' || changed_at || ', Stage: ' || new_stage || ', By: ' || COALESCE(changed_by, 'System')
            FROM stage_history 
            WHERE email = '$analyze_email'
            ORDER BY changed_at DESC 
            LIMIT 5;
        " 2>/dev/null || echo "No stage history available"
    fi
    
    echo ""
    
    # Recent operations
    echo -e "${YELLOW}Recent Operations (Last 30 days):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Date: ' || timestamp || ', Operation: ' || operation_type || ', Reason: ' || COALESCE(reason, 'Not specified')
        FROM account_operations 
        WHERE email = '$analyze_email' 
        AND timestamp >= datetime('now', '-30 days')
        ORDER BY timestamp DESC 
        LIMIT 5;
    " 2>/dev/null || echo "No recent operations"
    
    echo ""
    read -p "Press Enter to continue..."
}

department_analysis() {
    clear
    echo -e "${GREEN}=== Department Analysis ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing accounts by organizational units...${NC}"
    echo ""
    
    echo -e "${YELLOW}Top 15 Departments by User Count:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(org_unit_path, 'No OU Assigned') as department,
            COUNT(*) as total_users,
            SUM(CASE WHEN suspended = 'False' THEN 1 ELSE 0 END) as active_users,
            SUM(CASE WHEN suspended = 'True' THEN 1 ELSE 0 END) as suspended_users,
            SUM(CASE WHEN admin = 'True' THEN 1 ELSE 0 END) as admin_users,
            ROUND(AVG(CASE WHEN suspended = 'False' THEN 1 ELSE 0 END) * 100, 1) as active_percentage
        FROM accounts 
        WHERE is_active = 1
        GROUP BY org_unit_path
        ORDER BY total_users DESC
        LIMIT 15;
    " 2>/dev/null || echo "No department data available"
    
    echo ""
    echo -e "${YELLOW}Department Security Analysis:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(org_unit_path, 'No OU Assigned') as department,
            COUNT(*) as total_users,
            SUM(CASE WHEN two_factor = 'True' THEN 1 ELSE 0 END) as users_with_2fa,
            ROUND((SUM(CASE WHEN two_factor = 'True' THEN 1 ELSE 0 END) * 100.0) / COUNT(*), 1) as tfa_adoption_rate
        FROM accounts 
        WHERE is_active = 1
        GROUP BY org_unit_path
        HAVING COUNT(*) >= 5
        ORDER BY tfa_adoption_rate DESC
        LIMIT 10;
    " 2>/dev/null || echo "No security data available"
    
    echo ""
    read -p "Enter specific OU path to analyze (or press Enter to skip): " specific_ou
    
    if [[ -n "$specific_ou" ]]; then
        echo ""
        echo -e "${CYAN}Detailed analysis for: $specific_ou${NC}"
        echo ""
        
        sqlite3 "$DATABASE_PATH" "
            SELECT 
                email,
                full_name,
                suspended,
                admin,
                two_factor,
                creation_time
            FROM accounts 
            WHERE is_active = 1 AND org_unit_path = '$specific_ou'
            ORDER BY admin DESC, email
            LIMIT 20;
        " 2>/dev/null || echo "No accounts found in specified OU"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

email_pattern_analysis() {
    clear
    echo -e "${GREEN}=== Email Pattern Analysis ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing email address patterns...${NC}"
    echo ""
    
    echo -e "${YELLOW}Most Common Email Domains:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            SUBSTR(email, INSTR(email, '@') + 1) as domain,
            COUNT(*) as account_count
        FROM accounts 
        WHERE is_active = 1
        GROUP BY SUBSTR(email, INSTR(email, '@') + 1)
        ORDER BY account_count DESC
        LIMIT 10;
    " 2>/dev/null || echo "No email data available"
    
    echo ""
    echo -e "${YELLOW}Email Naming Patterns:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN email LIKE '%.%@%' THEN 'firstname.lastname format'
                WHEN email LIKE '%_%@%' THEN 'firstname_lastname format'
                WHEN email LIKE '%[0-9]%@%' THEN 'Contains numbers'
                ELSE 'Other format'
            END as pattern_type,
            COUNT(*) as count
        FROM accounts 
        WHERE is_active = 1
        GROUP BY 
            CASE 
                WHEN email LIKE '%.%@%' THEN 'firstname.lastname format'
                WHEN email LIKE '%_%@%' THEN 'firstname_lastname format'
                WHEN email LIKE '%[0-9]%@%' THEN 'Contains numbers'
                ELSE 'Other format'
            END
        ORDER BY count DESC;
    " 2>/dev/null || echo "No pattern data available"
    
    echo ""
    echo -e "${YELLOW}Email Length Analysis:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN LENGTH(email) < 15 THEN 'Short (< 15 chars)'
                WHEN LENGTH(email) < 25 THEN 'Medium (15-25 chars)'
                WHEN LENGTH(email) < 35 THEN 'Long (25-35 chars)'
                ELSE 'Very Long (35+ chars)'
            END as length_category,
            COUNT(*) as count,
            ROUND(AVG(LENGTH(email)), 1) as avg_length
        FROM accounts 
        WHERE is_active = 1
        GROUP BY 
            CASE 
                WHEN LENGTH(email) < 15 THEN 'Short (< 15 chars)'
                WHEN LENGTH(email) < 25 THEN 'Medium (15-25 chars)'
                WHEN LENGTH(email) < 35 THEN 'Long (25-35 chars)'
                ELSE 'Very Long (35+ chars)'
            END
        ORDER BY avg_length;
    " 2>/dev/null || echo "No length data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

storage_usage_analysis_detailed() {
    clear
    echo -e "${GREEN}=== Detailed Storage Usage Analysis ===${NC}"
    echo ""
    
    echo -e "${CYAN}Comprehensive storage usage analysis...${NC}"
    echo ""
    
    echo -e "${YELLOW}Storage Distribution Summary:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Total Accounts Scanned: ' || COUNT(*),
            'Total Storage Used: ' || ROUND(SUM(total_size_gb), 2) || ' GB',
            'Average Storage per User: ' || ROUND(AVG(total_size_gb), 2) || ' GB',
            'Maximum Storage: ' || ROUND(MAX(total_size_gb), 2) || ' GB',
            'Minimum Storage: ' || ROUND(MIN(total_size_gb), 2) || ' GB'
        FROM storage_size_history 
        WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history);
    " 2>/dev/null || echo "No storage data available"
    
    echo ""
    echo -e "${YELLOW}Storage Usage Categories:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN total_size_gb = 0 THEN 'Empty (0 GB)'
                WHEN total_size_gb < 1 THEN 'Minimal (< 1 GB)'
                WHEN total_size_gb < 5 THEN 'Low (1-5 GB)'
                WHEN total_size_gb < 15 THEN 'Medium (5-15 GB)'
                WHEN total_size_gb < 30 THEN 'High (15-30 GB)'
                ELSE 'Very High (30+ GB)'
            END as usage_category,
            COUNT(*) as user_count,
            ROUND((COUNT(*) * 100.0) / (SELECT COUNT(*) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)), 1) as percentage,
            ROUND(SUM(total_size_gb), 2) as total_gb_in_category
        FROM storage_size_history 
        WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
        GROUP BY 
            CASE 
                WHEN total_size_gb = 0 THEN 'Empty (0 GB)'
                WHEN total_size_gb < 1 THEN 'Minimal (< 1 GB)'
                WHEN total_size_gb < 5 THEN 'Low (1-5 GB)'
                WHEN total_size_gb < 15 THEN 'Medium (5-15 GB)'
                WHEN total_size_gb < 30 THEN 'High (15-30 GB)'
                ELSE 'Very High (30+ GB)'
            END
        ORDER BY MIN(total_size_gb);
    " 2>/dev/null || echo "No storage distribution data available"
    
    echo ""
    echo -e "${YELLOW}Top 15 Storage Users:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            s.email,
            ROUND(s.total_size_gb, 2) as storage_gb,
            a.org_unit_path,
            a.suspended
        FROM storage_size_history s
        JOIN accounts a ON s.email = a.email
        WHERE s.scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
        AND a.is_active = 1
        ORDER BY s.total_size_gb DESC
        LIMIT 15;
    " 2>/dev/null || echo "No top users data available"
    
    echo ""
    echo -e "${YELLOW}Storage by Department (Top 10):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(a.org_unit_path, 'No OU Assigned') as department,
            COUNT(*) as user_count,
            ROUND(SUM(s.total_size_gb), 2) as total_storage_gb,
            ROUND(AVG(s.total_size_gb), 2) as avg_storage_per_user
        FROM storage_size_history s
        JOIN accounts a ON s.email = a.email
        WHERE s.scan_time = (SELECT MAX(scan_time) FROM storage_size_history)
        AND a.is_active = 1
        GROUP BY a.org_unit_path
        ORDER BY total_storage_gb DESC
        LIMIT 10;
    " 2>/dev/null || echo "No department storage data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

# Placeholder functions for remaining account analysis options

login_activity_analysis() {
    clear
    echo -e "${GREEN}=== Login Activity Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Login Activity Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Last login timestamp analysis"
    echo "â€¢ Login frequency patterns"
    echo "â€¢ Inactive account identification"
    echo "â€¢ Login geographic analysis"
    echo ""
    read -p "Press Enter to continue..."
}

account_activity_patterns() {
    clear
    echo -e "${GREEN}=== Account Activity Patterns ===${NC}"
    echo ""
    echo -e "${CYAN}Activity Pattern Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Usage pattern identification"
    echo "â€¢ Activity timeline analysis"
    echo "â€¢ Behavioral pattern detection"
    echo "â€¢ Activity correlation analysis"
    echo ""
    read -p "Press Enter to continue..."
}

drive_usage_analysis() {
    clear
    echo -e "${GREEN}=== Drive Usage Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Drive Usage Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Drive access pattern analysis"
    echo "â€¢ File sharing behavior"
    echo "â€¢ Collaboration metrics"
    echo "â€¢ Drive organization analysis"
    echo ""
    read -p "Press Enter to continue..."
}

security_profile_analysis() {
    clear
    echo -e "${GREEN}=== Security Profile Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Security Profile Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Comprehensive security assessment"
    echo "â€¢ Risk factor analysis"
    echo "â€¢ Security compliance scoring"
    echo "â€¢ Vulnerability identification"
    echo ""
    read -p "Press Enter to continue..."
}

tfa_adoption_analysis() {
    clear
    echo -e "${GREEN}=== 2FA Adoption Analysis ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing 2FA adoption across the organization...${NC}"
    echo ""
    
    echo -e "${YELLOW}Overall 2FA Statistics:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Total Accounts: ' || COUNT(*),
            'Accounts with 2FA: ' || SUM(CASE WHEN two_factor = 'True' THEN 1 ELSE 0 END),
            'Accounts without 2FA: ' || SUM(CASE WHEN two_factor = 'False' THEN 1 ELSE 0 END),
            'Adoption Rate: ' || ROUND((SUM(CASE WHEN two_factor = 'True' THEN 1 ELSE 0 END) * 100.0) / COUNT(*), 1) || '%'
        FROM accounts 
        WHERE is_active = 1;
    " 2>/dev/null || echo "No 2FA data available"
    
    echo ""
    echo -e "${YELLOW}2FA Adoption by Account Type:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            CASE 
                WHEN admin = 'True' AND super_admin = 'True' THEN 'Super Admin'
                WHEN admin = 'True' THEN 'Admin'
                ELSE 'Regular User'
            END as account_type,
            COUNT(*) as total_accounts,
            SUM(CASE WHEN two_factor = 'True' THEN 1 ELSE 0 END) as with_2fa,
            ROUND((SUM(CASE WHEN two_factor = 'True' THEN 1 ELSE 0 END) * 100.0) / COUNT(*), 1) as adoption_rate
        FROM accounts 
        WHERE is_active = 1
        GROUP BY 
            CASE 
                WHEN admin = 'True' AND super_admin = 'True' THEN 'Super Admin'
                WHEN admin = 'True' THEN 'Admin'
                ELSE 'Regular User'
            END
        ORDER BY adoption_rate DESC;
    " 2>/dev/null || echo "No account type data available"
    
    echo ""
    echo -e "${YELLOW}2FA Adoption by Department (Top 10):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(org_unit_path, 'No OU Assigned') as department,
            COUNT(*) as total_users,
            SUM(CASE WHEN two_factor = 'True' THEN 1 ELSE 0 END) as users_with_2fa,
            ROUND((SUM(CASE WHEN two_factor = 'True' THEN 1 ELSE 0 END) * 100.0) / COUNT(*), 1) as adoption_rate
        FROM accounts 
        WHERE is_active = 1
        GROUP BY org_unit_path
        HAVING COUNT(*) >= 3
        ORDER BY adoption_rate DESC
        LIMIT 10;
    " 2>/dev/null || echo "No department data available"
    
    echo ""
    echo -e "${YELLOW}Accounts Without 2FA (Security Risk):${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            full_name,
            admin,
            org_unit_path,
            last_login_time
        FROM accounts 
        WHERE is_active = 1 AND two_factor = 'False'
        ORDER BY admin DESC, last_login_time DESC
        LIMIT 15;
    " 2>/dev/null || echo "No accounts without 2FA found"
    
    echo ""
    read -p "Press Enter to continue..."
}

admin_access_analysis() {
    clear
    echo -e "${GREEN}=== Admin Access Analysis ===${NC}"
    echo ""
    
    echo -e "${CYAN}Analyzing administrative access and privileges...${NC}"
    echo ""
    
    echo -e "${YELLOW}Admin Account Summary:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            'Total Admin Accounts: ' || SUM(CASE WHEN admin = 'True' THEN 1 ELSE 0 END),
            'Super Admin Accounts: ' || SUM(CASE WHEN super_admin = 'True' THEN 1 ELSE 0 END),
            'Regular Admin Accounts: ' || SUM(CASE WHEN admin = 'True' AND super_admin = 'False' THEN 1 ELSE 0 END),
            'Admin Accounts with 2FA: ' || SUM(CASE WHEN admin = 'True' AND two_factor = 'True' THEN 1 ELSE 0 END),
            'Admin 2FA Rate: ' || ROUND((SUM(CASE WHEN admin = 'True' AND two_factor = 'True' THEN 1 ELSE 0 END) * 100.0) / SUM(CASE WHEN admin = 'True' THEN 1 ELSE 0 END), 1) || '%'
        FROM accounts 
        WHERE is_active = 1;
    " 2>/dev/null || echo "No admin data available"
    
    echo ""
    echo -e "${YELLOW}All Admin Accounts:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            email,
            full_name,
            CASE WHEN super_admin = 'True' THEN 'Super Admin' ELSE 'Admin' END as admin_type,
            two_factor,
            suspended,
            org_unit_path,
            last_login_time
        FROM accounts 
        WHERE is_active = 1 AND admin = 'True'
        ORDER BY super_admin DESC, two_factor DESC, email;
    " 2>/dev/null || echo "No admin accounts found"
    
    echo ""
    echo -e "${YELLOW}Admin Access Security Issues:${NC}"
    local admin_no_2fa=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1 AND admin = 'True' AND two_factor = 'False';" 2>/dev/null || echo "0")
    local suspended_admins=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1 AND admin = 'True' AND suspended = 'True';" 2>/dev/null || echo "0")
    
    echo "  Admin accounts without 2FA: $admin_no_2fa"
    echo "  Suspended admin accounts: $suspended_admins"
    
    if [[ $admin_no_2fa -gt 0 ]]; then
        echo ""
        echo -e "${RED}âš ï¸  Admin accounts without 2FA (SECURITY RISK):${NC}"
        sqlite3 "$DATABASE_PATH" "
            SELECT 
                email,
                full_name,
                CASE WHEN super_admin = 'True' THEN 'Super Admin' ELSE 'Admin' END as admin_type,
                last_login_time
            FROM accounts 
            WHERE is_active = 1 AND admin = 'True' AND two_factor = 'False'
            ORDER BY super_admin DESC, last_login_time DESC;
        " 2>/dev/null
    fi
    
    echo ""
    echo -e "${YELLOW}Admin Distribution by Department:${NC}"
    sqlite3 "$DATABASE_PATH" "
        SELECT 
            COALESCE(org_unit_path, 'No OU Assigned') as department,
            SUM(CASE WHEN admin = 'True' THEN 1 ELSE 0 END) as admin_count,
            SUM(CASE WHEN super_admin = 'True' THEN 1 ELSE 0 END) as super_admin_count,
            COUNT(*) as total_users
        FROM accounts 
        WHERE is_active = 1
        GROUP BY org_unit_path
        HAVING SUM(CASE WHEN admin = 'True' THEN 1 ELSE 0 END) > 0
        ORDER BY admin_count DESC;
    " 2>/dev/null || echo "No admin distribution data available"
    
    echo ""
    read -p "Press Enter to continue..."
}

# Additional placeholder functions for remaining analysis options

risk_assessment() {
    clear
    echo -e "${GREEN}=== Risk Assessment ===${NC}"
    echo ""
    echo -e "${CYAN}Risk Assessment - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Comprehensive risk scoring"
    echo "â€¢ Security vulnerability assessment"
    echo "â€¢ Compliance risk analysis"
    echo "â€¢ Risk mitigation recommendations"
    echo ""
    read -p "Press Enter to continue..."
}

account_lifecycle_analysis() {
    clear
    echo -e "${GREEN}=== Account Lifecycle Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Lifecycle Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Lifecycle stage progression analysis"
    echo "â€¢ Account journey mapping"
    echo "â€¢ Lifecycle efficiency metrics"
    echo "â€¢ Stage optimization insights"
    echo ""
    read -p "Press Enter to continue..."
}

account_age_analysis() {
    clear
    echo -e "${GREEN}=== Account Age Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Age Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Account age distribution"
    echo "â€¢ Age-based pattern analysis"
    echo "â€¢ Tenure correlation analysis"
    echo "â€¢ Age-related trends"
    echo ""
    read -p "Press Enter to continue..."
}

account_growth_analysis() {
    clear
    echo -e "${GREEN}=== Account Growth Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Growth Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Account creation trends"
    echo "â€¢ Growth rate analysis"
    echo "â€¢ Seasonal pattern detection"
    echo "â€¢ Growth forecasting"
    echo ""
    read -p "Press Enter to continue..."
}

account_health_scoring() {
    clear
    echo -e "${GREEN}=== Account Health Scoring ===${NC}"
    echo ""
    echo -e "${CYAN}Health Scoring - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Multi-factor health scoring"
    echo "â€¢ Health trend analysis"
    echo "â€¢ Risk factor weighting"
    echo "â€¢ Health improvement recommendations"
    echo ""
    read -p "Press Enter to continue..."
}

cross_department_comparison() {
    clear
    echo -e "${GREEN}=== Cross-Department Comparison ===${NC}"
    echo ""
    echo -e "${CYAN}Department Comparison - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Multi-department analytics"
    echo "â€¢ Comparative benchmarking"
    echo "â€¢ Performance ranking"
    echo "â€¢ Best practice identification"
    echo ""
    read -p "Press Enter to continue..."
}

trend_comparison() {
    clear
    echo -e "${GREEN}=== Trend Comparison ===${NC}"
    echo ""
    echo -e "${CYAN}Trend Comparison - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Historical trend analysis"
    echo "â€¢ Comparative trend visualization"
    echo "â€¢ Pattern recognition"
    echo "â€¢ Trend prediction"
    echo ""
    read -p "Press Enter to continue..."
}

anomaly_detection() {
    clear
    echo -e "${GREEN}=== Anomaly Detection ===${NC}"
    echo ""
    echo -e "${CYAN}Anomaly Detection - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Statistical anomaly detection"
    echo "â€¢ Behavioral anomaly identification"
    echo "â€¢ Pattern deviation analysis"
    echo "â€¢ Automated alert generation"
    echo ""
    read -p "Press Enter to continue..."
}

batch_account_analysis() {
    clear
    echo -e "${GREEN}=== Batch Account Analysis ===${NC}"
    echo ""
    echo -e "${CYAN}Batch Analysis - Coming Soon${NC}"
    echo ""
    echo "This feature will include:"
    echo "â€¢ Bulk account analysis"
    echo "â€¢ CSV input processing"
    echo "â€¢ Batch report generation"
    echo "â€¢ Mass analysis operations"
    echo ""
    read -p "Press Enter to continue..."
}

# System Administration Menu - SQLite-driven implementation
# Provides system configuration, maintenance, and administrative tools
# Uses database-driven menu items from system_administration section
system_administration_menu() {
    render_menu "system_administration"
}

# Stage 1: Recently Suspended Accounts
stage1_recently_suspended_menu() {
    while true; do
        clear
        echo -e "${GREEN}=== Stage 1: Recently Suspended Accounts ===${NC}"
        echo ""
        echo -e "${CYAN}These accounts have been suspended but no further processing has occurred.${NC}"
        echo -e "${CYAN}Use this stage to review and query recently suspended accounts.${NC}"
        echo ""
        echo "1. Query all suspended accounts (all OUs)"
        echo "2. Query suspended accounts by department/type"
        echo "3. Check account status and details"
        echo "4. View suspended account statistics"
        echo "5. Export suspended account list"
        echo ""
        echo "6. Return to lifecycle management menu"
        echo ""
        echo "p. Previous menu (lifecycle management)"
        echo "m. Main menu"
        echo "x. Exit"
        echo ""
        read -p "Select an option (1-6, p, m, x): " stage1_choice
        echo ""
        
        case $stage1_choice in
            1) query_all_suspended_users ;;
            2) query_users_by_filter ;;
            3) 
                read -p "Enter username to check: " username
                if [[ -n "$username" ]]; then
                    diagnose_account "$username"
                else
                    echo -e "${RED}Username cannot be empty${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            4)
                echo -e "${CYAN}Suspended Account Statistics:${NC}"
                query_all_suspended_users | tail -n +2 | wc -l | xargs echo "Total suspended accounts:"
                echo ""
                read -p "Press Enter to continue..."
                ;;
            5)
                echo -e "${CYAN}Exporting suspended account list...${NC}"
                local export_file="local-config/reports/suspended_accounts_$(date +%Y%m%d_%H%M%S).csv"
                mkdir -p reports
                query_all_suspended_users > "$export_file"
                echo -e "${GREEN}Exported to: $export_file${NC}"
                echo ""
                read -p "Press Enter to continue..."
                ;;
            6|p|P|m|M) return ;;
            x|X) exit 0 ;;
            *) 
                echo -e "${RED}Invalid option. Please select 1-6, p, m, or x.${NC}"
                read -p "Press Enter to continue..."
                ;;
        esac
    done
}

# Stage 2: Process Pending Deletion
stage2_pending_deletion_menu() {
    while true; do
        clear
        echo -e "${GREEN}=== Stage 2: Process Pending Deletion (Rename & Label) ===${NC}"
        echo ""
        echo -e "${CYAN}Move accounts to 'Pending Deletion' OU and process their files:${NC}"
        echo -e "${CYAN}â€¢ Rename user's last name with '(PENDING DELETION - CONTACT OIT)'${NC}"
        echo -e "${CYAN}â€¢ Rename and label all their files${NC}"
        echo -e "${CYAN}â€¢ Remove from all groups${NC}"
        echo ""
        echo "1. Process single user for pending deletion"
        echo "2. Process multiple users from file"
        echo "3. Process multiple users (manual entry)"
        echo "4. Remove pending deletion (reverse operation)"
        echo "5. Query users in Pending Deletion OU"
        echo "6. Dry-run mode (preview changes)"
        echo ""
        echo "7. Return to lifecycle management menu"
        echo ""
        echo "p. Previous menu (lifecycle management)"
        echo "m. Main menu"
        echo "x. Exit"
        echo ""
        read -p "Select an option (1-7, p, m, x): " stage2_choice
        echo ""
        
        case $stage2_choice in
            1)
                user=$(get_user_input)
                show_pending_summary "$user"
                if enhanced_confirm "mark for pending deletion" 1 "high"; then
                    create_backup "$user" "add_pending"
                    process_pending_user "$user"
                else
                    echo -e "${YELLOW}Operation cancelled.${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            2)
                file_path=$(load_users_from_file)
                user_count=$(wc -l < "$file_path")
                echo ""
                echo -e "${YELLOW}Found $user_count users in file.${NC}"
                echo "Each user will be marked for pending deletion."
                if enhanced_confirm "batch mark for pending deletion" "$user_count" "high"; then
                    process_pending_users_from_file "$file_path"
                else
                    echo -e "${YELLOW}Operation cancelled.${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            3)
                users_array=($(get_multiple_user_input))
                if [[ ${#users_array[@]} -gt 0 ]]; then
                    echo ""
                    echo "Processing ${#users_array[@]} users for pending deletion"
                    if enhanced_confirm "process ${#users_array[@]} manually entered users" "${#users_array[@]}" "high"; then
                        for user in "${users_array[@]}"; do
                            echo ""
                            echo -e "${CYAN}Processing: $user${NC}"
                            create_backup "$user" "add_pending"
                            process_pending_user "$user"
                            echo "----------------------------------------"
                        done
                        echo -e "${GREEN}Manual processing completed for ${#users_array[@]} users.${NC}"
                    else
                        echo -e "${YELLOW}Operation cancelled.${NC}"
                    fi
                else
                    echo -e "${YELLOW}No users entered. Returning to menu.${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            4)
                user=$(get_user_input)
                show_pending_removal_summary "$user"
                if enhanced_confirm "remove pending deletion" 1 "normal"; then
                    create_backup "$user" "remove_pending"
                    remove_pending_user "$user"
                else
                    echo -e "${YELLOW}Operation cancelled.${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            5) 
                query_pending_users
                echo ""
                read -p "Press Enter to continue..."
                ;;
            15) 
                DRY_RUN=true
                echo -e "${YELLOW}Dry-run mode enabled. No actual changes will be made.${NC}"
                echo ""
                user=$(get_user_input)
                show_pending_summary "$user"
                process_pending_user "$user"
                DRY_RUN=false
                echo ""
                read -p "Press Enter to continue..."
                ;;
            7|p|P|m|M) return ;;
            x|X) exit 0 ;;
            *)
                echo -e "${RED}Invalid option. Please select 1-7, p, m, or x.${NC}"
                read -p "Press Enter to continue..."
                ;;
        esac
    done
}

# Stage 3: File Sharing Analysis & Reports  
stage3_sharing_analysis_menu() {
    while true; do
        clear
        echo -e "${GREEN}=== Stage 3: File Sharing Analysis & Reports ===${NC}"
        echo ""
        echo -e "${CYAN}Analyze files shared by pending deletion accounts with active ${DOMAIN:-yourdomain.edu} users.${NC}"
        echo -e "${CYAN}Generate reports for active users about files they're receiving from suspended accounts.${NC}"
        echo ""
        echo "1. Analyze single user's file sharing"
        echo "2. Analyze multiple users (batch processing)"
        echo "3. Generate report for active user (what they're receiving)"
        echo "4. Update shared filenames with pending deletion labels"
        echo "5. Bulk analysis of all pending deletion users"
        echo "6. View analysis statistics"
        echo "7. Clean up analysis files"
        echo ""
        echo "p. Previous menu (Lifecycle Management)"
        echo "m. Main menu"
        echo "x. Exit"
        echo ""
        read -p "Select an option (1-7, p, m, x): " stage3_choice
        echo ""
        
        case $stage3_choice in
            1)
                read -p "Enter username (email): " username
                if [[ -n "$username" ]]; then
                    echo ""
                    echo "Analysis options:"
                    echo "1. Standard analysis"
                    echo "2. Analysis with pending deletion filename updates"
                    echo "3. Analysis without report generation"
                    read -p "Select analysis type (1-3): " analysis_type
                    
                    case $analysis_type in
                        1) analyze_user_file_sharing "$username" false false true ;;
                        2) analyze_user_file_sharing "$username" false true true ;;
                        3) analyze_user_file_sharing "$username" false false false ;;
                        *) analyze_user_file_sharing "$username" false false true ;;
                    esac
                    
                    echo ""
                    read -p "Press Enter to continue..."
                else
                    echo -e "${RED}Username cannot be empty${NC}"
                    read -p "Press Enter to continue..."
                fi
                ;;
            2)
                echo -e "${CYAN}Batch File Sharing Analysis${NC}"
                echo ""
                read -p "Enter path to file containing usernames (one per line): " user_file
                if [[ -f "$user_file" ]]; then
                    echo "Analysis options:"
                    echo "1. Standard analysis for all users"
                    echo "2. Analysis with pending deletion updates"
                    read -p "Select analysis type (1-2): " batch_type
                    
                    local pending_mode=false
                    [[ "$batch_type" == "2" ]] && pending_mode=true
                    
                    echo -e "${CYAN}Processing users from file...${NC}"
                    local total_users=$(wc -l < "$user_file")
                    local current_user=0
                    local success_count=0
                    local error_count=0
                    
                    while read -r username; do
                        [[ -z "$username" ]] && continue
                        ((current_user++))
                        echo ""
                        echo -e "${BLUE}=== Processing user $current_user of $total_users: $username ===${NC}"
                        
                        if analyze_user_file_sharing "$username" false "$pending_mode" true; then
                            ((success_count++))
                        else
                            ((error_count++))
                        fi
                    done < "$user_file"
                    
                    echo ""
                    echo -e "${GREEN}Batch analysis completed${NC}"
                    echo -e "${CYAN}Total users processed: $current_user${NC}"
                    echo -e "${GREEN}Successful analyses: $success_count${NC}"
                    echo -e "${RED}Failed analyses: $error_count${NC}"
                    read -p "Press Enter to continue..."
                else
                    echo -e "${RED}File not found: $user_file${NC}"
                    read -p "Press Enter to continue..."
                fi
                ;;
            3)
                read -p "Enter active user email to generate report for: " recipient_email
                if [[ -n "$recipient_email" ]]; then
                    generate_recipient_report "$recipient_email"
                    echo ""
                    read -p "Press Enter to continue..."
                else
                    echo -e "${RED}Email cannot be empty${NC}"
                    read -p "Press Enter to continue..."
                fi
                ;;
            4)
                read -p "Enter username to update filenames for: " username
                if [[ -n "$username" ]]; then
                    local active_shares_csv="listshared/${username}_active-shares.csv"
                    if [[ -f "$active_shares_csv" ]]; then
                        update_pending_deletion_filenames "$username" "$active_shares_csv"
                    else
                        echo -e "${RED}No active shares analysis found for $username${NC}"
                        echo -e "${CYAN}Please run file sharing analysis first${NC}"
                    fi
                    echo ""
                    read -p "Press Enter to continue..."
                else
                    echo -e "${RED}Username cannot be empty${NC}"
                    read -p "Press Enter to continue..."
                fi
                ;;
            5)
                echo -e "${CYAN}Bulk Analysis of All Pending Deletion Users${NC}"
                echo ""
                echo "This will analyze all users in pending deletion OU."
                read -p "Continue? (y/n): " confirm_bulk
                
                if [[ "$confirm_bulk" == "y" || "$confirm_bulk" == "Y" ]]; then
                    local pending_users=$(mktemp)
                    $GAM print users query "orgUnitPath:'/Suspended Accounts/Suspended - Pending Deletion'" fields primaryemail > "$pending_users" 2>/dev/null
                    
                    local total=$(tail -n +2 "$pending_users" | wc -l)
                    echo -e "${CYAN}Found $total pending deletion users to analyze${NC}"
                    
                    local processed=0
                    local success=0
                    
                    tail -n +2 "$pending_users" | while read -r email rest; do
                        ((processed++))
                        echo ""
                        echo -e "${BLUE}=== Processing $processed/$total: $email ===${NC}"
                        
                        if analyze_user_file_sharing "$email" false false true; then
                            ((success++))
                        fi
                    done
                    
                    rm -f "$pending_users"
                    echo -e "${GREEN}Bulk analysis completed${NC}"
                    read -p "Press Enter to continue..."
                else
                    echo -e "${YELLOW}Bulk analysis cancelled${NC}"
                    read -p "Press Enter to continue..."
                fi
                ;;
            6)
                echo -e "${CYAN}File Sharing Analysis Statistics${NC}"
                echo ""
                
                local user_analyses=$(ls listshared/*_all_files.csv 2>/dev/null | wc -l)
                local sharing_analyses=$(ls listshared/*_shared_files.csv 2>/dev/null | wc -l)
                local active_analyses=$(ls listshared/*_active-shares.csv 2>/dev/null | wc -l)
                local recipient_reports=$(ls local-config/reports/*_files_from_*.csv 2>/dev/null | wc -l)
                
                echo "Analysis Files:"
                echo "- User file analyses: $user_analyses"
                echo "- Sharing analyses: $sharing_analyses"  
                echo "- Active share analyses: $active_analyses"
                echo "- Recipient reports: $recipient_reports"
                
                if [[ $active_analyses -gt 0 ]]; then
                    echo ""
                    echo "Active Sharing Summary:"
                    local total_active_files=0
                    for file in listshared/*_active-shares.csv; do
                        if [[ -f "$file" ]]; then
                            local count=$(tail -n +2 "$file" | wc -l 2>/dev/null || echo "0")
                            total_active_files=$((total_active_files + count))
                        fi
                    done
                    echo "- Total files shared with active users: $total_active_files"
                fi
                
                echo ""
                read -p "Press Enter to continue..."
                ;;
            7)
                echo -e "${CYAN}Clean Up Analysis Files${NC}"
                echo ""
                echo "This will clean up temporary and cache files from analysis."
                echo "Analysis results and reports will be preserved."
                echo ""
                read -p "Continue? (y/n): " confirm_cleanup
                
                if [[ "$confirm_cleanup" == "y" || "$confirm_cleanup" == "Y" ]]; then
                    rm -rf listshared/temp/* listshared/cache/*
                    find listshared/ -name "*.tmp" -delete 2>/dev/null
                    find listshared/ -name "temp-*" -delete 2>/dev/null
                    echo -e "${GREEN}Cleanup completed${NC}"
                else
                    echo -e "${YELLOW}Cleanup cancelled${NC}"
                fi
                read -p "Press Enter to continue..."
                ;;
            p|P) return ;;
            m|M) return 99 ;;  # Special code to return to main menu
            x|X) exit 0 ;;
            *)
                echo -e "${RED}Invalid option. Please select 1-7, p, m, or x.${NC}"
                read -p "Press Enter to continue..."
                ;;
        esac
    done
}

# Stage 4: Final Decisions (Exit Row / Temporary Hold)
stage4_final_decisions_menu() {
    while true; do
        clear
        echo -e "${GREEN}=== Stage 4: Final Decisions (Exit Row / Temporary Hold) ===${NC}"
        echo ""
        echo -e "${CYAN}Make final decisions about pending deletion accounts:${NC}"
        echo -e "${YELLOW}â€¢ Move to 'Exit Row' â†’ Account will be deleted soon${NC}"
        echo -e "${YELLOW}â€¢ Move to 'Temporary Hold' â†’ Account gets more time${NC}"
        echo ""
        echo "1. Move user to Temporary Hold"
        echo "2. Move users from file to Temporary Hold"
        echo "3. Remove user from Temporary Hold (reactivate or continue deletion)"
        echo "4. Query users in Temporary Hold OU"
        echo "5. Query users in Exit Row OU"
        echo "6. Move user to Exit Row (prepare for deletion)"
        echo ""
        echo "7. Return to main menu"
        echo "m. Main menu"
        echo "x. Exit"
        echo ""
        read -p "Select an option (1-7, m, x): " stage4_choice
        echo ""
        
        case $stage4_choice in
            1)
                user=$(get_user_input)
                show_summary "$user"
                if enhanced_confirm "move to temporary hold" 1 "normal"; then
                    create_backup "$user" "add_gwombat_hold"
                    process_user "$user"
                else
                    echo -e "${YELLOW}Operation cancelled.${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            2)
                file_path=$(load_users_from_file)
                user_count=$(wc -l < "$file_path")
                echo ""
                echo -e "${YELLOW}Found $user_count users in file.${NC}"
                echo "Each user will be moved to temporary hold."
                if enhanced_confirm "batch move to temporary hold" "$user_count" "batch"; then
                    process_users_from_file "$file_path"
                else
                    echo -e "${YELLOW}Operation cancelled.${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            3)
                user=$(get_user_input)
                show_removal_summary "$user"
                if enhanced_confirm "remove from temporary hold" 1 "normal"; then
                    create_backup "$user" "remove_gwombat_hold"
                    remove_gwombat_hold_user "$user"
                else
                    echo -e "${YELLOW}Operation cancelled.${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            4) 
                query_gwombat_hold_users
                echo ""
                read -p "Press Enter to continue..."
                ;;
            5)
                echo -e "${CYAN}Querying users in Exit Row OU...${NC}"
                $GAM print users query "orgUnitPath:'/Suspended Accounts/Suspended - Exit Row'" fields primaryemail,givenname,familyname,suspended,lastlogintime
                echo ""
                read -p "Press Enter to continue..."
                ;;
            15)
                read -p "Enter username to move to Exit Row: " username
                if [[ -n "$username" ]]; then
                    echo -e "${RED}WARNING: Moving to Exit Row means this account will be deleted soon!${NC}"
                    if enhanced_confirm "move $username to Exit Row" 1 "high"; then
                        move_user_to_ou "$username" "/Suspended Accounts/Suspended - Exit Row"
                        echo -e "${GREEN}$username moved to Exit Row${NC}"
                        log_info "Moved $username to Exit Row OU"
                    else
                        echo -e "${YELLOW}Operation cancelled.${NC}"
                    fi
                else
                    echo -e "${RED}Username cannot be empty${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            16) return ;;
            m|M) return ;;
            x|X) exit 0 ;;
            *)
                echo -e "${RED}Invalid option. Please select 1-7, m, or x.${NC}"
                read -p "Press Enter to continue..."
                ;;
        esac
    done
}

# Stage 5: Account Deletion Operations
stage5_deletion_operations_menu() {
    while true; do
        clear
        echo -e "${RED}=== Stage 5: Account Deletion Operations ===${NC}"
        echo ""
        echo -e "${YELLOW}âš ï¸  DANGER ZONE: These operations are irreversible!${NC}"
        echo -e "${CYAN}Manage accounts that are ready for final deletion.${NC}"
        echo ""
        echo "1. List accounts ready for deletion (Exit Row)"
        echo "2. Collect orphaned files before deletion"
        echo "3. License management for deletion candidates"
        echo "4. Generate pre-deletion audit report"
        echo "5. View deletion-related statistics"
        echo ""
        echo "6. Return to main menu"
        echo "m. Main menu"
        echo "x. Exit"
        echo ""
        read -p "Select an option (1-6, m, x): " stage5_choice
        echo ""
        
        case $stage5_choice in
            1)
                echo -e "${CYAN}Accounts in Exit Row (ready for deletion):${NC}"
                $GAM print users query "orgUnitPath:'/Suspended Accounts/Suspended - Exit Row'" fields primaryemail,givenname,familyname,suspended,lastlogintime,creationtime
                echo ""
                read -p "Press Enter to continue..."
                ;;
            2)
                orphaned_file_collection_menu
                ;;
            3)
                license_management_menu
                ;;
            4)
                read -p "Enter username for pre-deletion audit: " username
                if [[ -n "$username" ]]; then
                    echo -e "${CYAN}Generating pre-deletion audit for $username...${NC}"
                    
                    local audit_file="local-config/reports/${username}_pre_deletion_audit_$(date +%Y%m%d_%H%M%S).txt"
                    mkdir -p reports
                    
                    {
                        echo "=== PRE-DELETION AUDIT REPORT ==="
                        echo "User: $username"
                        echo "Generated: $(date)"
                        echo ""
                        
                        echo "=== USER INFORMATION ==="
                        $GAM info user "$username" 2>/dev/null || echo "User not found"
                        echo ""
                        
                        echo "=== FILE SHARING STATUS ==="
                        if [[ -f "listshared/${username}_active-shares.csv" ]]; then
                            local shared_files=$(tail -n +2 "listshared/${username}_active-shares.csv" | wc -l)
                            echo "Files still shared with active users: $shared_files"
                            
                            if [[ $shared_files -gt 0 ]]; then
                                echo ""
                                echo "WARNING: User still has files shared with active users!"
                                echo "Recipients:"
                                tail -n +2 "listshared/${username}_active-shares.csv" | cut -d, -f8 | sort -u
                            fi
                        else
                            echo "No file sharing analysis found - run Stage 3 analysis first"
                        fi
                        
                        echo ""
                        echo "=== GROUP MEMBERSHIPS ==="
                        $GAM info user "$username" | grep -A 10 "Groups:" 2>/dev/null || echo "No group information available"
                        
                    } > "$audit_file"
                    
                    echo -e "${GREEN}Pre-deletion audit saved to: $audit_file${NC}"
                    
                    # Show summary
                    echo ""
                    echo -e "${CYAN}Audit Summary:${NC}"
                    grep -E "Files still shared|WARNING|User not found" "$audit_file" || echo "No warnings found"
                else
                    echo -e "${RED}Username cannot be empty${NC}"
                fi
                echo ""
                read -p "Press Enter to continue..."
                ;;
            5)
                echo -e "${CYAN}Deletion-Related Statistics:${NC}"
                echo ""
                
                local exit_row_count=$($GAM print users query "orgUnitPath:'/Suspended Accounts/Suspended - Exit Row'" fields primaryemail 2>/dev/null | tail -n +2 | wc -l)
                local temphold_count=$(query_gwombat_hold_users | tail -n +2 | wc -l)
                local pending_count=$(query_pending_users | tail -n +2 | wc -l)
                
                echo "Accounts by stage:"
                echo "- Exit Row (ready for deletion): $exit_row_count"
                echo "- Temporary Hold: $temphold_count"
                echo "- Pending Deletion: $pending_count"
                echo ""
                
                # Check for potential issues
                local shared_files_count=0
                for file in listshared/*_active-shares.csv; do
                    if [[ -f "$file" ]]; then
                        local count=$(tail -n +2 "$file" | wc -l 2>/dev/null || echo "0")
                        shared_files_count=$((shared_files_count + count))
                    fi
                done
                
                echo "Potential issues:"
                echo "- Files still shared with active users: $shared_files_count"
                
                if [[ $shared_files_count -gt 0 ]]; then
                    echo -e "${YELLOW}âš ï¸  Warning: Some accounts still have active file shares!${NC}"
                fi
                
                echo ""
                read -p "Press Enter to continue..."
                ;;
            15) return ;;
            m|M) return ;;
            x|X) exit 0 ;;
            *)
                echo -e "${RED}Invalid option. Please select 1-6, m, or x.${NC}"
                read -p "Press Enter to continue..."
                ;;
        esac
    done
}

# SCuBA Compliance Management Menu
scuba_compliance_menu() {
    render_menu "scuba_compliance"
}

# Check if first time setup is needed
check_first_time_setup() {
    # Check if .env file exists
    if [[ ! -f "./local-config/.env" ]] && [[ ! -f "./.env" ]]; then
        return 0  # First time setup needed
    fi
    
    # Check if setup was completed
    local env_file=""
    if [[ -f "./local-config/.env" ]]; then
        env_file="./local-config/.env"
    elif [[ -f "./.env" ]]; then
        env_file="./.env"
    fi
    
    if [[ -n "$env_file" ]] && grep -q "SETUP_COMPLETED=.*true" "$env_file"; then
        return 1  # Setup already completed
    fi
    
    return 0  # Setup needed
}

# Menu navigation functions
show_menu_index() {
    clear
    # Use database-driven index
    show_menu_database_index
    
    echo ""
    read -p "Press Enter to return to main menu..."
}
search_menu_options() {
    while true; do
        clear
        echo -e "${BLUE}=== GWOMBAT Menu Search ===${NC}"
        echo ""
        echo -e "${CYAN}Search for menu options by keyword${NC}"
        echo -e "${GRAY}Type 'back' or 'exit' to return to main menu${NC}"
        echo ""
        
        # Check if stdin is a terminal to handle piped input correctly
        if [[ -t 0 ]]; then
            read -p "Enter search term: " search_term
        else
            # For non-interactive mode (piped input), read available input
            read search_term || break
        fi
        
        # Handle exit commands
        if [[ "$search_term" == "back" || "$search_term" == "b" || "$search_term" == "exit" ]]; then
            break
        fi
        
        # Validate input
        if [[ -z "$search_term" ]]; then
            echo -e "${RED}Please enter a search term${NC}"
            # Only wait for Enter in interactive mode
            if [[ -t 0 ]]; then
                read -p "Press Enter to continue..."
            else
                # In non-interactive mode, exit to prevent infinite loop
                echo "Exiting search due to empty input in non-interactive mode"
                break
            fi
            continue
        fi
        
        echo ""
        
        # Use database-driven search
        search_menu_database "$search_term"
        
        echo ""
        echo -e "${GRAY}Tip: Use short keywords for better results${NC}"
        echo ""
        
        # Only prompt for another search in interactive mode
        if [[ -t 0 ]]; then
            echo ""
            read -p "Search again? (Enter = yes, 'exit' or 'back' = return to menu): " next_action
            
            if [[ "$next_action" == "exit" || "$next_action" == "back" || "$next_action" == "b" ]]; then
                break
            fi
        else
            # In non-interactive mode, do one search and exit
            break
        fi
    done
}
# Main script execution
main() {
    # Check for first-time setup BEFORE verifying domains
    if check_first_time_setup; then
        echo -e "${BLUE}=== GWOMBAT First-Time Setup ===${NC}"
        echo ""
        echo "Welcome to GWOMBAT! It looks like this is your first time running the application."
        echo ""
        echo "Would you like to run the setup wizard to configure GWOMBAT for your environment?"
        echo "The wizard will help you configure:"
        echo ""
        echo "â€¢ Google Workspace domain and admin settings"
        echo "â€¢ GAM (Google Apps Manager) configuration"
        echo "â€¢ Organizational unit structure"
        echo "â€¢ Python environment and dependencies"
        echo "â€¢ Optional tools (GYB, rclone, restic)"
        echo "â€¢ Initial system scans and statistics"
        echo ""
        echo "1. Yes - Run setup wizard (recommended)"
        echo "2. No - Continue to main menu"
        echo "3. Exit"
        echo ""
        
        while true; do
            read -p "Select option (1-3): " setup_choice
            case "$setup_choice" in
                1)
                    echo ""
                    echo -e "${CYAN}Starting setup wizard...${NC}"
                    if [[ -x "./shared-utilities/setup_wizard.sh" ]]; then
                        ./shared-utilities/setup_wizard.sh
                        # After setup wizard completes, continue to main menu
                        break
                    else
                        echo -e "${RED}Setup wizard not found. Continuing to main menu.${NC}"
                        break
                    fi
                    ;;
                2)
                    echo ""
                    echo -e "${YELLOW}Skipping setup wizard. You can run it later with: ./shared-utilities/setup_wizard.sh${NC}"
                    echo ""
                    break
                    ;;
                3)
                    echo "Goodbye!"
                    exit 0
                    ;;
                *)
                    echo -e "${RED}Invalid choice. Please select 1, 2, or 3.${NC}"
                    ;;
            esac
        done
        
        echo ""
        read -p "Press Enter to continue to GWOMBAT main menu..."
    fi
    
    # Initialize databases if they don't exist
    if [[ ! -f "local-config/gwombat.db" ]] || [[ ! -f "shared-config/menu.db" ]]; then
        echo ""
        initialize_database
        echo ""
    fi
    
    # After setup is complete, verify GAM domain matches configuration
    # Only do this if .env exists (not first-time setup)
    if [[ -f ".env" ]]; then
        echo -e "${BLUE}=== GWOMBAT Security Verification ===${NC}"
        if ! verify_gam_domain; then
            echo ""
            echo -e "${RED}âŒ CRITICAL SECURITY ERROR: Cannot proceed with domain mismatch${NC}"
            echo ""
            echo "This prevents potential data operations on the wrong domain."
            echo ""
            echo -e "${CYAN}Would you like me to help you fix this now?${NC}"
            echo ""
            echo "1. ðŸ”§ Reconfigure GAM for domain: ${DOMAIN}"
            echo "2. ðŸ“ Update .env to match GAM domain"
            echo "3. ðŸšª Exit to fix manually"
            echo ""
            
            local fix_choice
            while true; do
                read -p "Select option (1-3): " fix_choice
                case "$fix_choice" in
                    1)
                        echo ""
                        echo -e "${CYAN}Starting GAM reconfiguration for domain: ${DOMAIN}${NC}"
                        echo ""
                        
                        # Source config manager functions
                        if [[ -f "$SHARED_UTILITIES_PATH/config_manager.sh" ]]; then
                            source "$SHARED_UTILITIES_PATH/config_manager.sh"
                            configure_gam_for_domain
                        else
                            echo -e "${YELLOW}Configuration manager not found. Manual GAM setup required:${NC}"
                            echo ""
                            echo "1. Run: ${WHITE}$GAM oauth create${NC}"
                            echo "2. Follow the authentication prompts"
                            echo "3. Verify with: ${WHITE}$GAM info domain${NC}"
                            echo "4. Restart GWOMBAT"
                            echo ""
                            read -p "Press Enter when GAM is configured..."
                        fi
                        
                        # Verify the fix worked
                        echo ""
                        echo -e "${CYAN}Testing GAM configuration...${NC}"
                        if verify_gam_domain; then
                            echo -e "${GREEN}âœ“ GAM domain configuration fixed!${NC}"
                            echo ""
                            break
                        else
                            echo -e "${RED}Domain mismatch still exists. Please check GAM configuration.${NC}"
                            echo ""
                            exit 1
                        fi
                        ;;
                    2)
                        echo ""
                        echo -e "${CYAN}Updating .env file to match GAM domain...${NC}"
                        
                        # Get GAM domain
                        local gam_domain=$(${GAM:-gam} info domain 2>/dev/null | grep -E "^Primary Domain|^Domain:" | head -1 | awk '{print $NF}' | tr -d ' ')
                        if [[ -n "$gam_domain" ]]; then
                            # Create backup
                            cp .env ".env.backup.$(date +%Y%m%d_%H%M%S)"
                            
                            # Update domain
                            sed -i.tmp "s/^DOMAIN=.*/DOMAIN=\"$gam_domain\"/" .env && rm .env.tmp
                            
                            echo -e "${GREEN}âœ“ Updated DOMAIN in .env to: $gam_domain${NC}"
                            echo -e "${YELLOW}âš ï¸  Please restart GWOMBAT to apply changes${NC}"
                            echo ""
                            exit 0
                        else
                            echo -e "${RED}Could not detect GAM domain. Please configure GAM first.${NC}"
                            exit 1
                        fi
                        ;;
                    3)
                        echo ""
                        echo "Exiting for manual configuration."
                        echo ""
                        exit 1
                        ;;
                    *)
                        echo -e "${RED}Invalid option. Please select 1, 2, or 3.${NC}"
                        ;;
                esac
            done
        fi
        echo ""
    fi
    
    # Run dependency check on startup
    if ! check_dependencies; then
        echo -e "${RED}Dependency check failed. Please install missing dependencies before continuing.${NC}"
        echo -e "${YELLOW}Press Enter to continue anyway, or Ctrl+C to exit...${NC}"
        read -r
    fi
    
    # First display with stats
    show_main_menu_with_stats
    choice=$MENU_CHOICE
    
    while true; do
        case $choice in
            [1-9])
                # Use database-driven navigation for numeric choices
                local section_name
                section_name=$(sqlite3 "$MENU_DB_FILE" "
                    SELECT name 
                    FROM menu_sections 
                    WHERE section_order = '$choice' AND is_active = 1;
                " 2>/dev/null)
                
                if [[ -n "$section_name" ]]; then
                    main_menu_function_dispatcher "$section_name"
                else
                    echo -e "${RED}Invalid option${NC}"
                    read -p "Press Enter to continue..."
                fi
                ;;
            98|s|S)
                # Search Menu Options
                search_menu_options
                ;;
            97|i|I)
                # Menu Index (Alphabetical)
                show_menu_index
                ;;
            x|X)
                echo -e "${BLUE}Goodbye!${NC}"
                log_info "Session ended by user"
                echo "=== SESSION END: $(date) ===" >> "$LOG_FILE"
                generate_daily_report
                exit 0
                ;;
            97)
                # Menu Index (Alphabetical)
                show_menu_index
                ;;
            98)
                # Search Menu Options
                search_menu_options
                ;;
            99)
                # Configuration Management
                if [[ -x "$SHARED_UTILITIES_PATH/config_manager.sh" ]]; then
                    source "$SHARED_UTILITIES_PATH/config_manager.sh"
                    show_config_menu
                else
                    configuration_menu
                fi
                ;;
            *)
                echo -e "${RED}Invalid choice. Please select 1-10, c, s, i, or x.${NC}"
                read -p "Press Enter to continue..."
                ;;
        esac
        
        # Reload menu without stats for subsequent displays
        show_main_menu_no_stats
        choice=$MENU_CHOICE
    done
}

# Shared Drive Function Dispatcher
shared_drive_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Drive Operations (1-4)
        "list_shared_drives"|"create_shared_drive"|"modify_shared_drive"|"delete_shared_drive")
            echo -e "${CYAN}Shared Drive Operation: $function_name${NC}"
            echo "This feature will provide comprehensive shared drive management."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Complete shared drive inventory and listing"
            echo "â€¢ Drive creation with templates and initial settings"
            echo "â€¢ Drive modification and configuration management"
            echo "â€¢ Safe drive deletion with confirmation and backup"
            read -p "Press Enter to continue..."
            ;;
        
        # Member Management (5-8)
        "list_drive_members"|"add_drive_members"|"remove_drive_members"|"change_member_roles")
            echo -e "${CYAN}Drive Member Management: $function_name${NC}"
            echo "This feature will provide comprehensive member management for shared drives."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ View all members and their roles in specific drives"
            echo "â€¢ Add users and groups with appropriate permissions"
            echo "â€¢ Remove members safely with access verification"
            echo "â€¢ Change member roles (viewer, editor, manager, content manager)"
            read -p "Press Enter to continue..."
            ;;
        
        # Drive Administration (9-12)
        "set_drive_restrictions"|"backup_drive_settings"|"restore_drive_settings"|"audit_drive_access")
            echo -e "${CYAN}Drive Administration: $function_name${NC}"
            echo "This feature will provide advanced drive administration capabilities."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Configure sharing restrictions and access controls"
            echo "â€¢ Backup and restore drive configurations"
            echo "â€¢ Comprehensive access auditing and reporting"
            echo "â€¢ Security policy enforcement"
            read -p "Press Enter to continue..."
            ;;
        
        # Bulk Operations (13-15)
        "bulk_member_changes"|"mass_drive_creation"|"bulk_permission_sync")
            echo -e "${CYAN}Bulk Drive Operations: $function_name${NC}"
            echo "This feature will provide bulk operations for efficient drive management."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Apply member changes across multiple drives simultaneously"
            echo "â€¢ Create multiple drives from templates or CSV imports"
            echo "â€¢ Synchronize permissions and settings across drive collections"
            read -p "Press Enter to continue..."
            ;;
        
        # Reports & Analytics (16-20)
        "drive_usage_report"|"member_access_report"|"drive_security_scan"|"compliance_audit"|"export_drive_inventory")
            echo -e "${CYAN}Drive Analytics & Reporting: $function_name${NC}"
            echo "This feature will provide comprehensive analytics and reporting for shared drives."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Storage usage and activity pattern analysis"
            echo "â€¢ Member access patterns and collaboration metrics"
            echo "â€¢ Security vulnerability scanning and compliance checking"
            echo "â€¢ Complete drive inventory export and documentation"
            read -p "Press Enter to continue..."
            ;;
        
        *)
            echo -e "${RED}Unknown shared drive function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Shared Drive Management Menu - SQLite-driven implementation
# Shared drive operations and team collaboration management interface
# Uses database-driven menu items from shared_drives section
shared_drive_menu() {
    render_menu "file_drive_operations"
}
# Shared Drive Management Functions
list_all_shared_drives() {
    echo -e "${CYAN}All Shared Drives${NC}"
    echo ""
    echo -e "${YELLOW}Retrieving shared drives...${NC}"
    echo ""
    
    # Get all shared drives using proper GAM7 syntax
    echo "Command: $GAM_PATH print shareddrives"
    shared_drives=$($GAM_PATH print shareddrives 2>/dev/null)
    
    if [[ $? -eq 0 ]] && [[ -n "$shared_drives" ]]; then
        echo "$shared_drives" | head -1  # Header
        echo "$shared_drives" | tail -n +2 | sort
        
        count=$(echo "$shared_drives" | tail -n +2 | wc -l)
        echo ""
        echo -e "${YELLOW}Total shared drives: $count${NC}"
    else
        echo -e "${YELLOW}No shared drives found or error retrieving data.${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

create_new_shared_drive() {
    echo -e "${CYAN}Create New Shared Drive${NC}"
    echo ""
    
    read -p "Enter shared drive name: " drive_name
    
    if [[ -z "$drive_name" ]]; then
        echo -e "${RED}Error: Drive name cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Admin Management Options:${NC}"
    echo "1. Admin managed (recommended for organization control)"
    echo "2. User managed (standard team drive)"
    echo ""
    read -p "Select option (1-2): " admin_option
    
    local admin_managed=""
    case $admin_option in
        1) admin_managed="adminmanagedrestrictions true" ;;
        2) admin_managed="" ;;
        *) 
            echo -e "${YELLOW}Using default (user managed)${NC}"
            admin_managed=""
            ;;
    esac
    
    echo ""
    echo -e "${YELLOW}Creating shared drive: $drive_name${NC}"
    echo "Command: $GAM_PATH create shareddrive \"$drive_name\" $admin_managed"
    echo ""
    
    # Create the shared drive with proper GAM7 syntax
    local drive_output=$($GAM_PATH create shareddrive "$drive_name" $admin_managed 2>&1)
    local exit_code=$?
    
    if [[ $exit_code -eq 0 ]]; then
        # Extract drive ID from GAM output (format: User,name,id)
        local new_drive_id=$(echo "$drive_output" | tail -1 | cut -d',' -f3 | tr -d '"')
        
        if [[ -n "$new_drive_id" && "$new_drive_id" != "id" ]]; then
            echo -e "${GREEN}âœ“ Shared drive created successfully!${NC}"
            echo "Drive ID: $new_drive_id"
            echo "Drive Name: $drive_name"
            
            # Wait for drive to be ready (GAM requirement: 30+ seconds for updates)
            echo ""
            echo -e "${YELLOW}Waiting for drive to be ready for member operations...${NC}"
            echo "âš  Note: New shared drives require 30+ seconds before member operations can be performed."
            
            # Ask if user wants to add initial members
            echo ""
            read -p "Would you like to add initial members now? (y/N): " add_members
            
            if [[ "$add_members" =~ ^[Yy]$ ]]; then
                echo ""
                echo "Waiting 35 seconds for drive to be ready..."
                sleep 35
                
                echo ""
                echo "Adding initial members..."
                echo "Enter email addresses (one per line, empty line to finish):"
                
                local members=()
                while true; do
                    read -p "> " member_email
                    if [[ -z "$member_email" ]]; then
                        break
                    fi
                    members+=("$member_email")
                done
                
                if [[ ${#members[@]} -gt 0 ]]; then
                    echo ""
                    echo -e "${YELLOW}Adding members to shared drive...${NC}"
                    
                    for member in "${members[@]}"; do
                        echo -n "Adding $member... "
                        if $GAM_PATH update shareddrive "$new_drive_id" add member "$member" 2>/dev/null; then
                            echo -e "${GREEN}âœ“${NC}"
                        else
                            echo -e "${RED}âœ—${NC}"
                        fi
                    done
                fi
            fi
            
            echo ""
            echo -e "${GREEN}Shared drive setup completed!${NC}"
        else
            echo -e "${RED}Error: Could not extract drive ID from GAM output${NC}"
            echo "GAM output: $drive_output"
        fi
    else
        echo -e "${RED}Error creating shared drive${NC}"
        echo "GAM output: $drive_output"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

search_shared_drives() {
    echo -e "${CYAN}Search Shared Drives${NC}"
    echo ""
    
    read -p "Enter search term (name or partial name): " search_term
    
    if [[ -z "$search_term" ]]; then
        echo -e "${RED}Error: Search term cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Searching for drives containing: '$search_term'${NC}"
    echo ""
    
    # Search shared drives
    echo "Command: $GAM_PATH print shareddrives query \"name contains '$search_term'\""
    local search_results=$($GAM_PATH print shareddrives query "name contains '$search_term'" 2>/dev/null)
    
    if [[ $? -eq 0 ]] && [[ -n "$search_results" ]]; then
        echo "$search_results" | head -1  # Header
        echo "$search_results" | tail -n +2 | sort
        
        local count=$(echo "$search_results" | tail -n +2 | wc -l)
        echo ""
        echo -e "${YELLOW}Found $count matching drive(s)${NC}"
    else
        echo -e "${YELLOW}No drives found matching: '$search_term'${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

view_drive_details() {
    echo -e "${CYAN}View Drive Details${NC}"
    echo ""
    
    read -p "Enter shared drive ID or name: " drive_identifier
    
    if [[ -z "$drive_identifier" ]]; then
        echo -e "${RED}Error: Drive identifier cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Retrieving drive details for: $drive_identifier${NC}"
    echo ""
    
    # Get drive information
    echo "Command: $GAM_PATH info shareddrive \"$drive_identifier\""
    local drive_info=$($GAM_PATH info shareddrive "$drive_identifier" 2>/dev/null)
    
    if [[ $? -eq 0 ]] && [[ -n "$drive_info" ]]; then
        echo -e "${GREEN}Drive Information:${NC}"
        echo "$drive_info"
        
        echo ""
        echo -e "${CYAN}Drive Members:${NC}"
        echo "Command: $GAM_PATH print shareddrive \"$drive_identifier\" members"
        local members_info=$($GAM_PATH print shareddrive "$drive_identifier" members 2>/dev/null)
        
        if [[ $? -eq 0 ]] && [[ -n "$members_info" ]]; then
            echo "$members_info"
        else
            echo "No member information available or error retrieving data."
        fi
    else
        echo -e "${RED}Drive not found or error retrieving information.${NC}"
        echo "Please check the drive ID/name and try again."
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

add_drive_members() {
    echo -e "${CYAN}Add Members to Shared Drive${NC}"
    echo ""
    
    read -p "Enter shared drive ID or name: " drive_identifier
    
    if [[ -z "$drive_identifier" ]]; then
        echo -e "${RED}Error: Drive identifier cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo "Enter email addresses to add (one per line, empty line to finish):"
    
    local members=()
    while true; do
        read -p "> " member_email
        if [[ -z "$member_email" ]]; then
            break
        fi
        members+=("$member_email")
    done
    
    if [[ ${#members[@]} -eq 0 ]]; then
        echo -e "${YELLOW}No email addresses entered.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Role Selection:${NC}"
    echo "1. Member (can view and edit)"
    echo "2. Organizer (can manage members and settings)"
    echo ""
    read -p "Select role (1-2): " role_choice
    
    local role=""
    case $role_choice in
        1) role="member" ;;
        2) role="organizer" ;;
        *) 
            echo -e "${YELLOW}Using default role: member${NC}"
            role="member"
            ;;
    esac
    
    echo ""
    echo -e "${YELLOW}Adding members to drive: $drive_identifier${NC}"
    echo "Role: $role"
    echo ""
    
    local success_count=0
    local fail_count=0
    
    for member in "${members[@]}"; do
        echo -n "Adding $member as $role... "
        if $GAM_PATH update shareddrive "$drive_identifier" add "$role" "$member" 2>/dev/null; then
            echo -e "${GREEN}âœ“${NC}"
            ((success_count++))
        else
            echo -e "${RED}âœ—${NC}"
            ((fail_count++))
        fi
    done
    
    echo ""
    echo -e "${GREEN}Member addition completed:${NC}"
    echo "  Successful: $success_count"
    echo "  Failed: $fail_count"
    
    if [[ $fail_count -gt 0 ]]; then
        echo ""
        echo -e "${YELLOW}Note: Failures may be due to:${NC}"
        echo "â€¢ Invalid email addresses"
        echo "â€¢ Users already have access"
        echo "â€¢ Insufficient permissions"
        echo "â€¢ Drive not found"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

remove_drive_members() {
    echo -e "${CYAN}Remove Members from Shared Drive${NC}"
    echo ""
    
    read -p "Enter shared drive ID or name: " drive_identifier
    
    if [[ -z "$drive_identifier" ]]; then
        echo -e "${RED}Error: Drive identifier cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo "Enter email addresses to remove (one per line, empty line to finish):"
    
    local members=()
    while true; do
        read -p "> " member_email
        if [[ -z "$member_email" ]]; then
            break
        fi
        members+=("$member_email")
    done
    
    if [[ ${#members[@]} -eq 0 ]]; then
        echo -e "${YELLOW}No email addresses entered.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Removing members from drive: $drive_identifier${NC}"
    echo ""
    
    local success_count=0
    local fail_count=0
    
    for member in "${members[@]}"; do
        echo -n "Removing $member... "
        if $GAM_PATH update shareddrive "$drive_identifier" remove "$member" 2>/dev/null; then
            echo -e "${GREEN}âœ“${NC}"
            ((success_count++))
        else
            echo -e "${RED}âœ—${NC}"
            ((fail_count++))
        fi
    done
    
    echo ""
    echo -e "${GREEN}Member removal completed:${NC}"
    echo "  Successful: $success_count"
    echo "  Failed: $fail_count"
    
    echo ""
    read -p "Press Enter to continue..."
}

update_member_permissions() {
    echo -e "${CYAN}Update Member Permissions${NC}"
    echo ""
    
    read -p "Enter shared drive ID or name: " drive_identifier
    read -p "Enter member email address: " member_email
    
    if [[ -z "$drive_identifier" || -z "$member_email" ]]; then
        echo -e "${RED}Error: Both drive identifier and email are required.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}New Role Selection:${NC}"
    echo "1. Member (can view and edit)"
    echo "2. Organizer (can manage members and settings)"
    echo ""
    read -p "Select new role (1-2): " role_choice
    
    local role=""
    case $role_choice in
        1) role="member" ;;
        2) role="organizer" ;;
        *) 
            echo -e "${RED}Invalid role selection.${NC}"
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    echo ""
    echo -e "${YELLOW}Updating permissions for: $member_email${NC}"
    echo "Drive: $drive_identifier"
    echo "New Role: $role"
    echo ""
    
    # Remove and re-add with new role (GAM7 approach)
    echo "Removing current access..."
    $GAM_PATH update shareddrive "$drive_identifier" remove "$member_email" 2>/dev/null
    
    echo "Adding with new role..."
    if $GAM_PATH update shareddrive "$drive_identifier" add "$role" "$member_email" 2>/dev/null; then
        echo -e "${GREEN}âœ“ Successfully updated permissions for $member_email${NC}"
    else
        echo -e "${RED}âœ— Failed to update permissions for $member_email${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

list_drive_members() {
    echo -e "${CYAN}List Drive Members${NC}"
    echo ""
    
    read -p "Enter shared drive ID or name: " drive_identifier
    
    if [[ -z "$drive_identifier" ]]; then
        echo -e "${RED}Error: Drive identifier cannot be empty.${NC}"
        read -p "Press Enter to continue..."
        return
    fi
    
    echo ""
    echo -e "${YELLOW}Members of drive: $drive_identifier${NC}"
    echo ""
    
    # Get drive members
    echo "Command: $GAM_PATH print shareddrive \"$drive_identifier\" members"
    local members_info=$($GAM_PATH print shareddrive "$drive_identifier" members 2>/dev/null)
    
    if [[ $? -eq 0 ]] && [[ -n "$members_info" ]]; then
        echo "$members_info"
        
        local member_count=$(echo "$members_info" | tail -n +2 | wc -l)
        echo ""
        echo -e "${YELLOW}Total members: $member_count${NC}"
    else
        echo -e "${YELLOW}No members found or error retrieving data.${NC}"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Placeholder functions for remaining menu items
transfer_drive_ownership() {
    echo -e "${YELLOW}Transfer Shared Drive Management - Coming Soon${NC}"
    echo "This feature will allow transferring management roles of shared drives."
    read -p "Press Enter to continue..."
}

update_drive_settings() {
    echo -e "${YELLOW}Update Drive Settings - Coming Soon${NC}"
    echo "This feature will allow updating drive settings and restrictions."
    read -p "Press Enter to continue..."
}

archive_restore_drive() {
    echo -e "${YELLOW}Archive/Restore Shared Drive - Coming Soon${NC}"
    echo "This feature will allow archiving and restoring shared drives."
    read -p "Press Enter to continue..."
}

delete_shared_drive() {
    echo -e "${YELLOW}Delete Shared Drive - Coming Soon${NC}"
    echo "This feature will allow safe deletion of shared drives with confirmation."
    read -p "Press Enter to continue..."
}

bulk_member_operations() {
    echo -e "${YELLOW}Bulk Member Operations - Coming Soon${NC}"
    echo "This feature will allow bulk operations on drive members."
    read -p "Press Enter to continue..."
}

drive_migration_tools() {
    echo -e "${YELLOW}Drive Migration Tools - Coming Soon${NC}"
    echo "This feature will provide tools for migrating drives and content."
    read -p "Press Enter to continue..."
}

permission_audit_report() {
    echo -e "${YELLOW}Permission Audit Report - Coming Soon${NC}"
    echo "This feature will generate comprehensive permission audit reports."
    read -p "Press Enter to continue..."
}

drive_usage_statistics() {
    echo -e "${YELLOW}Drive Usage Statistics - Coming Soon${NC}"
    echo "This feature will show drive usage and activity statistics."
    read -p "Press Enter to continue..."
}

member_activity_report() {
    echo -e "${YELLOW}Member Activity Report - Coming Soon${NC}"
    echo "This feature will show member activity across shared drives."
    read -p "Press Enter to continue..."
}

export_drive_data() {
    echo -e "${YELLOW}Export Drive Data - Coming Soon${NC}"
    echo "This feature will allow exporting drive data and member lists."
    read -p "Press Enter to continue..."
}

# Analyze Shared Drives for Inactive Members
analyze_inactive_shared_drives() {
    while true; do
        clear
        echo -e "${CYAN}ðŸ” Shared Drive Inactive Member Analysis${NC}"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "Identify shared drives with no active users for cleanup and optimization"
        echo ""
        echo -e "${YELLOW}Analysis Options:${NC}"
        echo "1. ðŸŽ¯ Quick Analysis - No Active Users"
        echo "2. ðŸ“Š Detailed Analysis - Inactive vs Active Members"
        echo "3. ðŸ§¹ Comprehensive Report - All Drive Status"
        echo "4. ðŸ”„ Custom Analysis - Filter by Criteria"
        echo "5. ðŸ“‹ Generate Cleanup Recommendations"
        echo ""
        echo "b. Back to Shared Drive Management"
        echo ""
        read -p "Select analysis type (1-5, b): " analysis_choice
        echo ""
        
        case $analysis_choice in
            1)
                quick_inactive_drive_analysis
                ;;
            2)
                detailed_inactive_drive_analysis
                ;;
            3)
                comprehensive_drive_status_report
                ;;
            4)
                custom_drive_analysis
                ;;
            5)
                generate_drive_cleanup_recommendations
                ;;
            b|B)
                return
                ;;
            *)
                echo -e "${RED}Invalid option. Please select 1-5 or b.${NC}"
                read -p "Press Enter to continue..."
                ;;
        esac
    done
}

# Quick Analysis - Find drives with no active users
quick_inactive_drive_analysis() {
    echo -e "${BLUE}ðŸŽ¯ Quick Analysis - Shared Drives with No Active Users${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    # Check dependencies
    if ! command -v "$GAM" >/dev/null 2>&1; then
        echo -e "${RED}GAM not found. Please ensure GAM is installed and configured.${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo "Scanning all shared drives for active membership..."
    echo ""
    
    # Create temporary files for analysis
    local drive_list="/tmp/shared_drives_$$.txt"
    local inactive_drives="/tmp/inactive_drives_$$.txt"
    local analysis_report="/tmp/drive_analysis_$(date +%Y%m%d_%H%M%S).txt"
    
    # Get all shared drives
    echo "Step 1: Retrieving all shared drives..."
    if ! $GAM print shareddrives > "$drive_list" 2>/dev/null; then
        echo -e "${RED}Failed to retrieve shared drives. Check GAM configuration.${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    local total_drives=$(tail -n +2 "$drive_list" | wc -l)
    echo "Found $total_drives shared drives to analyze"
    echo ""
    
    # Initialize report
    {
        echo "=== GWOMBAT Shared Drive Inactive Analysis Report ==="
        echo "Generated: $(date)"
        echo "Domain: ${DOMAIN:-Unknown}"
        echo "Total Drives Analyzed: $total_drives"
        echo ""
        echo "=== DRIVES WITH NO ACTIVE USERS ==="
        echo ""
    } > "$analysis_report"
    
    local inactive_count=0
    local processed=0
    
    # Analyze each drive
    echo -e "${CYAN}Analyzing shared drives for active members...${NC}"
    echo ""
    
    tail -n +2 "$drive_list" | while IFS=',' read -r drive_id drive_name creator_email creation_time rest; do
        ((processed++))
        
        # Clean up drive name (remove quotes and commas)
        drive_name=$(echo "$drive_name" | sed 's/^"//; s/"$//; s/""/"/')
        
        # Show progress
        echo -n "[$processed/$total_drives] Analyzing: "
        if [[ ${#drive_name} -gt 50 ]]; then
            echo "${drive_name:0:47}..."
        else
            echo "$drive_name"
        fi
        
        # Get drive members
        local members_file="/tmp/drive_members_${drive_id}_$$.txt"
        if $GAM print shareddrivemembers shareddrive "$drive_id" > "$members_file" 2>/dev/null; then
            
            # Count active users (exclude suspended and external)
            local active_members=0
            local total_members=0
            local suspended_members=0
            local external_members=0
            
            if [[ -s "$members_file" ]]; then
                # Skip header line and analyze members
                tail -n +2 "$members_file" | while IFS=',' read -r member_email member_role member_type rest; do
                    ((total_members++))
                    
                    # Clean up email
                    member_email=$(echo "$member_email" | sed 's/^"//; s/"$//')
                    
                    # Check if member is from our domain
                    if [[ "$member_email" == *"@${DOMAIN:-yourdomain.edu}" ]]; then
                        # Check if user is active (not suspended)
                        if $GAM info user "$member_email" | grep -q "Suspended: False" 2>/dev/null; then
                            ((active_members++))
                        else
                            ((suspended_members++))
                        fi
                    else
                        ((external_members++))
                    fi
                done
                
                # Update counters in parent process via file
                echo "$active_members" > "/tmp/active_${drive_id}_$$"
                echo "$total_members" > "/tmp/total_${drive_id}_$$"
                echo "$suspended_members" > "/tmp/suspended_${drive_id}_$$"
                echo "$external_members" > "/tmp/external_${drive_id}_$$"
            fi
            
            # Read counters
            active_members=$(cat "/tmp/active_${drive_id}_$$" 2>/dev/null || echo "0")
            total_members=$(cat "/tmp/total_${drive_id}_$$" 2>/dev/null || echo "0")
            suspended_members=$(cat "/tmp/suspended_${drive_id}_$$" 2>/dev/null || echo "0")
            external_members=$(cat "/tmp/external_${drive_id}_$$" 2>/dev/null || echo "0")
            
            # Log drive with no active users
            if [[ $active_members -eq 0 ]]; then
                ((inactive_count++))
                echo -e "  ${RED}âš ï¸  NO ACTIVE USERS${NC}"
                
                # Add to report
                {
                    echo "Drive Name: $drive_name"
                    echo "Drive ID: $drive_id"
                    echo "Creator: $creator_email"
                    echo "Created: $creation_time"
                    echo "Total Members: $total_members"
                    echo "Active Members: $active_members"
                    echo "Suspended Members: $suspended_members"
                    echo "External Members: $external_members"
                    echo "Status: INACTIVE - No active users"
                    echo "Recommendation: Consider archiving or transferring ownership"
                    echo "---"
                    echo ""
                } >> "$analysis_report"
                
                # Add to inactive drives list
                echo "$drive_id,$drive_name,$active_members,$total_members" >> "$inactive_drives"
            else
                echo "  âœ… $active_members active users"
            fi
            
            # Cleanup temp files
            rm -f "$members_file" "/tmp/active_${drive_id}_$$" "/tmp/total_${drive_id}_$$" "/tmp/suspended_${drive_id}_$$" "/tmp/external_${drive_id}_$$"
        else
            echo -e "  ${YELLOW}âš ï¸  Unable to retrieve members${NC}"
        fi
    done
    
    # Update final counts in report
    inactive_count=$(wc -l < "$inactive_drives" 2>/dev/null || echo "0")
    
    {
        echo ""
        echo "=== ANALYSIS SUMMARY ==="
        echo "Total Drives: $total_drives"
        echo "Inactive Drives (no active users): $inactive_count"
        echo "Active Drives: $((total_drives - inactive_count))"
        echo "Percentage Inactive: $(echo "scale=1; $inactive_count * 100 / $total_drives" | bc 2>/dev/null || echo "N/A")%"
        echo ""
        echo "=== RECOMMENDATIONS ==="
        if [[ $inactive_count -gt 0 ]]; then
            echo "â€¢ Review inactive drives for potential archival"
            echo "â€¢ Contact creators about drive usage plans"
            echo "â€¢ Consider transferring ownership to active users"
            echo "â€¢ Archive drives with no recent activity"
        else
            echo "â€¢ All shared drives have active users"
            echo "â€¢ No immediate cleanup actions needed"
        fi
        echo ""
        echo "Report generated by GWOMBAT - $(date)"
    } >> "$analysis_report"
    
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo -e "${GREEN}ðŸ“Š Analysis Complete${NC}"
    echo ""
    echo -e "${CYAN}Results Summary:${NC}"
    echo "  â€¢ Total Drives Analyzed: $total_drives"
    echo "  â€¢ Drives with No Active Users: $inactive_count"
    echo "  â€¢ Percentage Inactive: $(echo "scale=1; $inactive_count * 100 / $total_drives" | bc 2>/dev/null || echo "N/A")%"
    echo ""
    echo -e "${YELLOW}ðŸ“‹ Detailed Report: $analysis_report${NC}"
    
    if [[ $inactive_count -gt 0 ]]; then
        echo ""
        echo -e "${RED}âš ï¸  Found $inactive_count shared drives with no active users:${NC}"
        echo ""
        
        if [[ -f "$inactive_drives" ]]; then
            echo -e "${WHITE}Drive Name                                    | Active | Total${NC}"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            while IFS=',' read -r drive_id drive_name active_members total_members; do
                printf "%-45s | %-6s | %-5s\n" "${drive_name:0:44}" "$active_members" "$total_members"
            done < "$inactive_drives"
        fi
        
        echo ""
        echo -e "${CYAN}ðŸ’¡ Recommendations:${NC}"
        echo "  â€¢ Review these drives for potential cleanup"
        echo "  â€¢ Contact drive creators about continued usage"
        echo "  â€¢ Consider archiving unused drives"
        echo "  â€¢ Use option 5 to generate detailed cleanup recommendations"
    else
        echo ""
        echo -e "${GREEN}âœ… Excellent! All shared drives have active users.${NC}"
    fi
    
    # Cleanup
    rm -f "$drive_list" "$inactive_drives"
    
    echo ""
    log_operation "shared_drive_inactive_analysis" "$USER" "success" "Analyzed $total_drives drives, found $inactive_count inactive"
    read -p "Press Enter to continue..."
}

# Detailed Analysis - Show inactive vs active member breakdown
detailed_inactive_drive_analysis() {
    echo -e "${BLUE}ðŸ“Š Detailed Analysis - Drive Member Activity Status${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "This analysis provides detailed member status for all shared drives"
    echo ""
    
    # Check dependencies
    if ! command -v "$GAM" >/dev/null 2>&1; then
        echo -e "${RED}GAM not found. Please ensure GAM is installed and configured.${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo "Performing detailed analysis of all shared drives..."
    echo ""
    
    # Create temporary files
    local drive_list="/tmp/shared_drives_detailed_$$.txt"
    local detailed_report="/tmp/detailed_drive_analysis_$(date +%Y%m%d_%H%M%S).txt"
    
    # Get all shared drives
    echo "Retrieving shared drive list..."
    if ! $GAM print shareddrives > "$drive_list" 2>/dev/null; then
        echo -e "${RED}Failed to retrieve shared drives. Check GAM configuration.${NC}"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    local total_drives=$(tail -n +2 "$drive_list" | wc -l)
    echo "Analyzing $total_drives shared drives..."
    echo ""
    
    # Initialize detailed report
    {
        echo "=== GWOMBAT Detailed Shared Drive Member Analysis ==="
        echo "Generated: $(date)"
        echo "Domain: ${DOMAIN:-Unknown}"
        echo "Analysis Type: Detailed member status breakdown"
        echo ""
        echo "Legend:"
        echo "  âœ… Active - User is active in domain"
        echo "  âŒ Suspended - User is suspended"
        echo "  ðŸ”— External - User is outside domain"
        echo "  â“ Unknown - Unable to determine status"
        echo ""
        echo "========================================================================================"
        echo ""
    } > "$detailed_report"
    
    local processed=0
    local drives_with_issues=0
    local total_active=0
    local total_suspended=0
    local total_external=0
    
    # Analyze each drive in detail
    tail -n +2 "$drive_list" | while IFS=',' read -r drive_id drive_name creator_email creation_time rest; do
        ((processed++))
        
        # Clean up drive name
        drive_name=$(echo "$drive_name" | sed 's/^"//; s/"$//; s/""/"/')
        
        echo "[$processed/$total_drives] Analyzing: $drive_name"
        
        # Get drive members
        local members_file="/tmp/drive_members_detailed_${drive_id}_$$.txt"
        
        {
            echo "DRIVE: $drive_name"
            echo "ID: $drive_id"
            echo "Creator: $creator_email"
            echo "Created: $creation_time"
            echo ""
        } >> "$detailed_report"
        
        if $GAM print shareddrivemembers shareddrive "$drive_id" > "$members_file" 2>/dev/null; then
            local drive_active=0
            local drive_suspended=0
            local drive_external=0
            local drive_unknown=0
            
            echo "Members:" >> "$detailed_report"
            
            if [[ -s "$members_file" ]] && [[ $(wc -l < "$members_file") -gt 1 ]]; then
                # Process each member
                tail -n +2 "$members_file" | while IFS=',' read -r member_email member_role member_type rest; do
                    # Clean up email and role
                    member_email=$(echo "$member_email" | sed 's/^"//; s/"$//')
                    member_role=$(echo "$member_role" | sed 's/^"//; s/"$//')
                    
                    local status_icon="â“"
                    local status_text="Unknown"
                    
                    # Determine member status
                    if [[ "$member_email" == *"@${DOMAIN:-yourdomain.edu}" ]]; then
                        # Domain user - check if active
                        if $GAM info user "$member_email" >/dev/null 2>&1; then
                            if $GAM info user "$member_email" | grep -q "Suspended: False" 2>/dev/null; then
                                status_icon="âœ…"
                                status_text="Active"
                                ((drive_active++))
                            else
                                status_icon="âŒ"
                                status_text="Suspended"
                                ((drive_suspended++))
                            fi
                        else
                            status_icon="â“"
                            status_text="Not Found"
                            ((drive_unknown++))
                        fi
                    else
                        # External user
                        status_icon="ðŸ”—"
                        status_text="External"
                        ((drive_external++))
                    fi
                    
                    printf "  %s %-50s | %-12s | %s\n" "$status_icon" "$member_email" "$member_role" "$status_text" >> "$detailed_report"
                done
                
                # Update parent process counters
                echo "$drive_active" > "/tmp/drive_active_${drive_id}_$$"
                echo "$drive_suspended" > "/tmp/drive_suspended_${drive_id}_$$"
                echo "$drive_external" > "/tmp/drive_external_${drive_id}_$$"
                echo "$drive_unknown" > "/tmp/drive_unknown_${drive_id}_$$"
            else
                echo "  (No members found)" >> "$detailed_report"
            fi
            
            # Read counters back
            drive_active=$(cat "/tmp/drive_active_${drive_id}_$$" 2>/dev/null || echo "0")
            drive_suspended=$(cat "/tmp/drive_suspended_${drive_id}_$$" 2>/dev/null || echo "0")
            drive_external=$(cat "/tmp/drive_external_${drive_id}_$$" 2>/dev/null || echo "0")
            drive_unknown=$(cat "/tmp/drive_unknown_${drive_id}_$$" 2>/dev/null || echo "0")
            
            # Add summary for this drive
            {
                echo ""
                echo "Summary for this drive:"
                echo "  Active Domain Users: $drive_active"
                echo "  Suspended Users: $drive_suspended"
                echo "  External Users: $drive_external"
                echo "  Unknown Status: $drive_unknown"
                echo "  Total Members: $((drive_active + drive_suspended + drive_external + drive_unknown))"
                
                if [[ $drive_active -eq 0 ]]; then
                    echo "  âš ï¸  STATUS: NO ACTIVE DOMAIN USERS"
                    ((drives_with_issues++))
                else
                    echo "  âœ… STATUS: Has active domain users"
                fi
                
                echo ""
                echo "----------------------------------------------------------------------------------------"
                echo ""
            } >> "$detailed_report"
            
            # Display progress info
            if [[ $drive_active -eq 0 ]]; then
                echo "  âš ï¸  No active domain users ($((drive_suspended + drive_external + drive_unknown)) total members)"
            else
                echo "  âœ… $drive_active active users"
            fi
            
            # Update totals
            total_active=$((total_active + drive_active))
            total_suspended=$((total_suspended + drive_suspended))
            total_external=$((total_external + drive_external))
            
            # Cleanup temp files
            rm -f "$members_file" "/tmp/drive_active_${drive_id}_$$" "/tmp/drive_suspended_${drive_id}_$$" "/tmp/drive_external_${drive_id}_$$" "/tmp/drive_unknown_${drive_id}_$$"
        else
            echo "  âŒ Unable to retrieve members" 
            {
                echo "  âŒ Unable to retrieve member list for this drive"
                echo ""
                echo "----------------------------------------------------------------------------------------"
                echo ""
            } >> "$detailed_report"
        fi
    done
    
    # Add final summary to report
    {
        echo ""
        echo "=== OVERALL ANALYSIS SUMMARY ==="
        echo "Total Drives Analyzed: $total_drives"
        echo "Drives with NO Active Domain Users: $drives_with_issues"
        echo "Drives with Active Domain Users: $((total_drives - drives_with_issues))"
        echo ""
        echo "Domain User Statistics:"
        echo "  Total Active Domain Users: $total_active"
        echo "  Total Suspended Domain Users: $total_suspended"
        echo "  Total External Users: $total_external"
        echo ""
        echo "Key Findings:"
        if [[ $drives_with_issues -gt 0 ]]; then
            echo "  â€¢ $drives_with_issues drives have no active domain users"
            echo "  â€¢ These drives may need ownership transfer or archival"
            echo "  â€¢ Review external user access for security compliance"
        else
            echo "  â€¢ All drives have active domain users"
            echo "  â€¢ Drive membership appears healthy"
        fi
        echo ""
        echo "Generated by GWOMBAT - $(date)"
    } >> "$detailed_report"
    
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo -e "${GREEN}ðŸ“Š Detailed Analysis Complete${NC}"
    echo ""
    echo -e "${CYAN}Overall Summary:${NC}"
    echo "  â€¢ Total Drives Analyzed: $total_drives"
    echo "  â€¢ Drives with NO Active Users: $drives_with_issues"
    echo "  â€¢ Total Active Domain Users: $total_active"
    echo "  â€¢ Total Suspended Users: $total_suspended"
    echo "  â€¢ Total External Users: $total_external"
    echo ""
    echo -e "${YELLOW}ðŸ“‹ Detailed Report: $detailed_report${NC}"
    
    if [[ $drives_with_issues -gt 0 ]]; then
        echo ""
        echo -e "${RED}âš ï¸  Attention Required:${NC}"
        echo "  â€¢ $drives_with_issues shared drives have no active domain users"
        echo "  â€¢ Review the detailed report for specific drive analysis"
        echo "  â€¢ Consider using the cleanup recommendations tool (option 5)"
    fi
    
    # Cleanup
    rm -f "$drive_list"
    
    echo ""
    log_operation "shared_drive_detailed_analysis" "$USER" "success" "Detailed analysis: $total_drives drives, $drives_with_issues with issues"
    read -p "Press Enter to continue..."
}

# Comprehensive Drive Status Report
comprehensive_drive_status_report() {
    echo -e "${BLUE}ðŸ“‹ Comprehensive Drive Status Report${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Generating comprehensive status report for all shared drives..."
    echo "This may take several minutes for large organizations."
    echo ""
    
    # Ask for confirmation
    read -p "Continue with comprehensive analysis? (y/N): " confirm
    if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
        echo "Analysis cancelled."
        read -p "Press Enter to continue..."
        return
    fi
    
    echo "Starting comprehensive analysis..."
    echo "This analysis includes member status, drive activity, and usage patterns."
    read -p "Press Enter to continue..."
}

# Custom Drive Analysis with Filters
custom_drive_analysis() {
    echo -e "${BLUE}ðŸ”„ Custom Drive Analysis${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Configure custom analysis criteria for shared drives"
    echo ""
    echo -e "${YELLOW}Available Filters:${NC}"
    echo "1. Filter by Drive Name Pattern"
    echo "2. Filter by Creator Email"
    echo "3. Filter by Creation Date Range"
    echo "4. Filter by Member Count Range"
    echo "5. Filter by Activity Level"
    echo ""
    read -p "Select filter type (1-5): " filter_type
    
    case $filter_type in
        1)
            read -p "Enter drive name pattern (supports wildcards): " name_pattern
            echo "Analyzing drives matching pattern: $name_pattern"
            ;;
        2)
            read -p "Enter creator email: " creator_email
            echo "Analyzing drives created by: $creator_email"
            ;;
        3)
            read -p "Enter start date (YYYY-MM-DD): " start_date
            read -p "Enter end date (YYYY-MM-DD): " end_date
            echo "Analyzing drives created between $start_date and $end_date"
            ;;
        4)
            read -p "Enter minimum member count: " min_members
            read -p "Enter maximum member count: " max_members
            echo "Analyzing drives with $min_members to $max_members members"
            ;;
        5)
            echo "Activity level filters:"
            echo "  a. No activity (inactive drives)"
            echo "  b. Low activity (minimal usage)"
            echo "  c. High activity (frequent usage)"
            read -p "Select activity level (a/b/c): " activity_level
            ;;
        *)
            echo "Invalid filter type"
            read -p "Press Enter to continue..."
            return
            ;;
    esac
    
    echo ""
    echo "Custom analysis feature - Implementation in progress"
    echo "This will provide filtered analysis based on your criteria."
    read -p "Press Enter to continue..."
}

# Generate Cleanup Recommendations
generate_drive_cleanup_recommendations() {
    echo -e "${BLUE}ðŸ“‹ Drive Cleanup Recommendations${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Generating intelligent cleanup recommendations based on drive analysis..."
    echo ""
    
    # Create recommendations file
    local recommendations_file="/tmp/drive_cleanup_recommendations_$(date +%Y%m%d_%H%M%S).txt"
    
    {
        echo "=== GWOMBAT Shared Drive Cleanup Recommendations ==="
        echo "Generated: $(date)"
        echo "Domain: ${DOMAIN:-Unknown}"
        echo ""
        echo "This report provides actionable recommendations for shared drive cleanup"
        echo "and optimization based on current drive analysis."
        echo ""
        echo "=== CLEANUP CRITERIA ==="
        echo "â€¢ No Active Users: Drives with zero active domain users"
        echo "â€¢ Suspended Only: Drives with only suspended users"
        echo "â€¢ External Only: Drives with only external users"
        echo "â€¢ Creator Inactive: Drives where creator is suspended/inactive"
        echo "â€¢ Old Unused: Drives with no recent activity"
        echo ""
        echo "=== RECOMMENDATIONS ==="
        echo ""
    } > "$recommendations_file"
    
    echo "Analyzing current shared drive state..."
    echo ""
    echo "Cleanup recommendations have been generated!"
    echo ""
    echo -e "${CYAN}Recommendation Categories:${NC}"
    echo "  â€¢ Immediate Action Required"
    echo "  â€¢ Review and Consider"
    echo "  â€¢ Monitor for Future"
    echo "  â€¢ No Action Needed"
    echo ""
    echo -e "${YELLOW}ðŸ“‹ Recommendations Report: $recommendations_file${NC}"
    echo ""
    echo "This feature provides detailed cleanup guidance based on:"
    echo "  â€¢ User activity patterns"
    echo "  â€¢ Drive usage statistics"
    echo "  â€¢ Organizational policies"
    echo "  â€¢ Security best practices"
    
    echo ""
    log_operation "shared_drive_cleanup_recommendations" "$USER" "success" "Generated cleanup recommendations"
    read -p "Press Enter to continue..."
}

# Data Retention Management Menu
retention_management_menu() {
    render_menu "system_administration"
}

# Performance Reports Menu
performance_reports_menu() {
    render_menu "reports_monitoring"
}

# Log Management Menu
log_management_menu() {
    render_menu "system_administration"
}

# Activity Reports Menu
activity_reports_menu() {
    render_menu "reports_monitoring"
}

# Real-time Monitoring Menu
monitoring_menu() {
    render_menu "system_administration"
}

# Drive Cleanup Operations Menu
drive_cleanup_menu() {
    render_menu "file_drive_operations"
}

# Backup Operations Menu
backup_operations_menu() {
    render_menu "system_administration"
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CSV OPERATIONS FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Export User Accounts to CSV
export_user_accounts_to_csv() {
    echo -e "${CYAN}ðŸ“¤ Export User Accounts to CSV${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    local export_file="exports/user_accounts_$(date +%Y%m%d_%H%M%S).csv"
    mkdir -p exports
    
    echo "Creating comprehensive user account export..."
    echo ""
    
    # Create CSV header
    echo "Email,FirstName,LastName,FullName,OrganizationalUnit,CreationTime,LastLoginTime,Suspended,Admin,2FA_Enrolled,Storage_Used_GB,Storage_Quota_GB,Account_Type" > "$export_file"
    
    # Export all user accounts with comprehensive details
    echo "Gathering user account data from domain..."
    $GAM_PATH print users fullname firstname lastname ou creationtime lastlogintime suspended admin agreed2terms isenabled2fa primaryemail agreedtoterms recoveryphone recoveryemail > temp_users.csv 2>/dev/null
    
    if [[ -f temp_users.csv ]]; then
        # Process each user and add storage data
        tail -n +2 temp_users.csv | while IFS=',' read -r email firstname lastname fullname ou creation lastlogin suspended admin tos twofa rest; do
            # Get storage usage using correct GAM7 syntax
            user_info=$($GAM_PATH info user "$email" 2>/dev/null)
            
            # Parse storage used from GAM info output
            storage_used_gb="0"
            if [[ -n "$user_info" ]]; then
                # Try multiple patterns for GAM7 storage information
                if echo "$user_info" | grep -q "Storage Used:"; then
                    storage_line=$(echo "$user_info" | grep "Storage Used:" | head -1)
                    size_value=$(echo "$storage_line" | sed -n 's/.*Storage Used: *\([0-9.]*\).*/\1/p')
                    size_unit=$(echo "$storage_line" | sed -n 's/.*Storage Used: *[0-9.]* *\([A-Za-z]*\).*/\1/p')
                    
                    if [[ -n "$size_value" && -n "$size_unit" ]]; then
                        case "${size_unit,,}" in
                            "gb") storage_used_gb="$size_value" ;;
                            "mb") storage_used_gb=$(echo "scale=3; $size_value / 1024" | bc 2>/dev/null || echo "0") ;;
                            "kb") storage_used_gb=$(echo "scale=3; $size_value / 1048576" | bc 2>/dev/null || echo "0") ;;
                            "bytes"|"b") storage_used_gb=$(echo "scale=3; $size_value / 1073741824" | bc 2>/dev/null || echo "0") ;;
                            *) storage_used_gb="0" ;;
                        esac
                    fi
                elif echo "$user_info" | grep -qi "quota.*used"; then
                    quota_line=$(echo "$user_info" | grep -i "quota.*used" | head -1)
                    size_value=$(echo "$quota_line" | grep -o '[0-9.]*' | head -1)
                    size_unit=$(echo "$quota_line" | grep -o -i '\(bytes\|kb\|mb\|gb\|tb\)' | head -1)
                    
                    if [[ -n "$size_value" && -n "$size_unit" ]]; then
                        case "${size_unit,,}" in
                            "gb") storage_used_gb="$size_value" ;;
                            "mb") storage_used_gb=$(echo "scale=3; $size_value / 1024" | bc 2>/dev/null || echo "0") ;;
                            "kb") storage_used_gb=$(echo "scale=3; $size_value / 1048576" | bc 2>/dev/null || echo "0") ;;
                            "bytes"|"b") storage_used_gb=$(echo "scale=3; $size_value / 1073741824" | bc 2>/dev/null || echo "0") ;;
                            *) storage_used_gb="0" ;;
                        esac
                    fi
                fi
            fi
            
            # Set default quota (15 GB for standard users)
            storage_quota="15"
            
            # Determine account type
            account_type="Standard"
            if [[ "$admin" == "True" ]]; then
                account_type="Admin"
            elif [[ "$suspended" == "True" ]]; then
                account_type="Suspended"
            fi
            
            # storage_used_gb already calculated above from GAM info output
            
            echo "\"$email\",\"$firstname\",\"$lastname\",\"$fullname\",\"$ou\",\"$creation\",\"$lastlogin\",\"$suspended\",\"$admin\",\"$twofa\",\"$storage_used_gb\",\"$storage_quota\",\"$account_type\"" >> "$export_file"
        done
        
        rm -f temp_users.csv
        
        echo "âœ… User accounts exported successfully!"
        echo "ðŸ“ Export file: $export_file"
        echo "ðŸ“Š Records: $(tail -n +2 "$export_file" | wc -l) user accounts"
    else
        echo "âŒ Failed to gather user account data"
        return 1
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# Export Account Lists to CSV
export_account_lists_to_csv() {
    echo -e "${CYAN}ðŸ“‹ Export Account Lists to CSV${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    local export_file="exports/account_lists_$(date +%Y%m%d_%H%M%S).csv"
    mkdir -p exports
    
    echo "Exporting all account lists from database..."
    echo ""
    
    # Create CSV header
    echo "List_Name,Account_Count,Created_Date,Last_Modified,Category,Description" > "$export_file"
    
    # Export account lists from database
    sqlite3 "$DATABASE_PATH" "
        SELECT name, 
               COUNT(*) as account_count,
               created_at,
               updated_at,
               CASE 
                   WHEN name LIKE '%stage1%' OR name LIKE '%recently_suspended%' THEN 'Stage 1 - Recently Suspended'
                   WHEN name LIKE '%stage2%' OR name LIKE '%pending%' THEN 'Stage 2 - Pending Deletion'
                   WHEN name LIKE '%stage3%' OR name LIKE '%sharing%' THEN 'Stage 3 - Sharing Analysis'
                   WHEN name LIKE '%stage4%' OR name LIKE '%decision%' THEN 'Stage 4 - Final Decisions'
                   WHEN name LIKE '%stage5%' OR name LIKE '%deletion%' THEN 'Stage 5 - Account Deletion'
                   WHEN name LIKE '%admin%' THEN 'Administrative'
                   WHEN name LIKE '%group%' THEN 'Group Management'
                   ELSE 'General'
               END as category,
               'Account lifecycle management list' as description
        FROM account_lists 
        GROUP BY name, created_at, updated_at
        ORDER BY name;
    " | while IFS='|' read -r name count created updated category description; do
        echo "\"$name\",\"$count\",\"$created\",\"$updated\",\"$category\",\"$description\"" >> "$export_file"
    done
    
    echo "âœ… Account lists exported successfully!"
    echo "ðŸ“ Export file: $export_file"
    echo "ðŸ“Š Lists exported: $(tail -n +2 "$export_file" | wc -l) account lists"
    echo ""
    read -p "Press Enter to continue..."
}

# Export Storage Data to CSV
export_storage_data_to_csv() {
    echo -e "${CYAN}ðŸ“Š Export Storage Data to CSV${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    local export_file="exports/storage_data_$(date +%Y%m%d_%H%M%S).csv"
    mkdir -p exports
    
    echo "Exporting storage usage data..."
    echo ""
    
    # Create CSV header
    echo "Email,Total_Size_GB,Gmail_Size_GB,Drive_Size_GB,Photos_Size_GB,Scan_Date,Days_Since_Scan,Storage_Trend" > "$export_file"
    
    # Export storage data from database
    sqlite3 "$DATABASE_PATH" "
        SELECT email,
               ROUND(total_size_gb, 2) as total_gb,
               ROUND(gmail_size_gb, 2) as gmail_gb,
               ROUND(drive_size_gb, 2) as drive_gb,
               ROUND(photos_size_gb, 2) as photos_gb,
               scan_time,
               ROUND(julianday('now') - julianday(scan_time), 0) as days_since,
               CASE 
                   WHEN total_size_gb > 10 THEN 'High Usage'
                   WHEN total_size_gb > 5 THEN 'Medium Usage'
                   ELSE 'Low Usage'
               END as trend
        FROM storage_size_history 
        WHERE scan_time = (
            SELECT MAX(scan_time) 
            FROM storage_size_history s2 
            WHERE s2.email = storage_size_history.email
        )
        ORDER BY total_size_gb DESC;
    " | while IFS='|' read -r email total gmail drive photos scan_date days trend; do
        echo "\"$email\",\"$total\",\"$gmail\",\"$drive\",\"$photos\",\"$scan_date\",\"$days\",\"$trend\"" >> "$export_file"
    done
    
    echo "âœ… Storage data exported successfully!"
    echo "ðŸ“ Export file: $export_file"
    echo "ðŸ“Š Records: $(tail -n +2 "$export_file" | wc -l) storage records"
    echo ""
    read -p "Press Enter to continue..."
}

# Export Analytics Data to CSV
export_analytics_data_to_csv() {
    echo -e "${CYAN}ðŸ“ˆ Export Analytics Data to CSV${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    local export_file="exports/analytics_data_$(date +%Y%m%d_%H%M%S).csv"
    mkdir -p exports
    
    echo "Exporting analytics and metrics data..."
    echo ""
    
    # Create CSV header
    echo "Metric_Category,Metric_Name,Value,Unit,Date_Generated,Period,Trend" > "$export_file"
    
    # Generate analytics data
    local total_users=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE is_active = 1;" 2>/dev/null || echo "0")
    local suspended_users=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE suspended = 1;" 2>/dev/null || echo "0")
    local admin_users=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE admin = 1;" 2>/dev/null || echo "0")
    local total_storage=$(sqlite3 "$DATABASE_PATH" "SELECT ROUND(SUM(total_size_gb), 2) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history);" 2>/dev/null || echo "0")
    local avg_storage=$(sqlite3 "$DATABASE_PATH" "SELECT ROUND(AVG(total_size_gb), 2) FROM storage_size_history WHERE scan_time = (SELECT MAX(scan_time) FROM storage_size_history);" 2>/dev/null || echo "0")
    
    local current_date=$(date '+%Y-%m-%d %H:%M:%S')
    
    # Add analytics rows
    echo "\"User_Management\",\"Total_Active_Users\",\"$total_users\",\"accounts\",\"$current_date\",\"current\",\"stable\"" >> "$export_file"
    echo "\"User_Management\",\"Suspended_Users\",\"$suspended_users\",\"accounts\",\"$current_date\",\"current\",\"monitored\"" >> "$export_file"
    echo "\"User_Management\",\"Admin_Users\",\"$admin_users\",\"accounts\",\"$current_date\",\"current\",\"stable\"" >> "$export_file"
    echo "\"Storage_Analytics\",\"Total_Storage_Usage\",\"$total_storage\",\"GB\",\"$current_date\",\"current\",\"growing\"" >> "$export_file"
    echo "\"Storage_Analytics\",\"Average_Storage_Per_User\",\"$avg_storage\",\"GB\",\"$current_date\",\"current\",\"stable\"" >> "$export_file"
    
    # Add monthly trends
    sqlite3 "$DATABASE_PATH" "
        SELECT 'Monthly_Trends' as category,
               'Storage_Growth_' || strftime('%Y_%m', scan_time) as metric,
               ROUND(SUM(total_size_gb), 2) as value,
               'GB' as unit,
               strftime('%Y-%m-%d', MAX(scan_time)) as date,
               strftime('%Y-%m', scan_time) as period,
               'historical' as trend
        FROM storage_size_history 
        WHERE scan_time >= date('now', '-6 months')
        GROUP BY strftime('%Y-%m', scan_time)
        ORDER BY period DESC;
    " | while IFS='|' read -r category metric value unit date period trend; do
        echo "\"$category\",\"$metric\",\"$value\",\"$unit\",\"$date\",\"$period\",\"$trend\"" >> "$export_file"
    done
    
    echo "âœ… Analytics data exported successfully!"
    echo "ðŸ“ Export file: $export_file"
    echo "ðŸ“Š Metrics: $(tail -n +2 "$export_file" | wc -l) analytics records"
    echo ""
    read -p "Press Enter to continue..."
}

# Export Database Tables to CSV
export_database_tables_to_csv() {
    echo -e "${CYAN}ðŸ—ƒï¸ Export Database Tables to CSV${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    mkdir -p exports/database_export_$(date +%Y%m%d_%H%M%S)
    local export_dir="exports/database_export_$(date +%Y%m%d_%H%M%S)"
    
    echo "Exporting all database tables to CSV format..."
    echo "ðŸ“ Export directory: $export_dir"
    echo ""
    
    # Get list of all tables
    local tables=$(sqlite3 "$DATABASE_PATH" ".tables" 2>/dev/null)
    
    for table in $tables; do
        echo "ðŸ“‹ Exporting table: $table"
        
        # Export table to CSV
        sqlite3 -header -csv "$DATABASE_PATH" "SELECT * FROM $table;" > "$export_dir/${table}.csv" 2>/dev/null
        
        if [[ -f "$export_dir/${table}.csv" ]]; then
            local record_count=$(tail -n +2 "$export_dir/${table}.csv" | wc -l)
            echo "   âœ… $record_count records exported"
        else
            echo "   âŒ Export failed"
        fi
    done
    
    # Create export summary
    echo "Table_Name,Record_Count,Export_Date,File_Size_KB" > "$export_dir/export_summary.csv"
    for csv_file in "$export_dir"/*.csv; do
        if [[ -f "$csv_file" && "$(basename "$csv_file")" != "export_summary.csv" ]]; then
            local table_name=$(basename "$csv_file" .csv)
            local record_count=$(tail -n +2 "$csv_file" | wc -l)
            local file_size=$(du -k "$csv_file" | cut -f1)
            echo "\"$table_name\",\"$record_count\",\"$(date)\",\"$file_size\"" >> "$export_dir/export_summary.csv"
        fi
    done
    
    echo ""
    echo "âœ… Database export completed!"
    echo "ðŸ“ Export directory: $export_dir"
    echo "ðŸ“Š Tables exported: $(ls "$export_dir"/*.csv | wc -l) tables"
    echo ""
    read -p "Press Enter to continue..."
}

# CSV Format Converter
csv_format_converter() {
    echo -e "${CYAN}ðŸ”§ CSV Format Converter${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "CSV Format Conversion Options:"
    echo "1. Convert delimiter (comma to semicolon, tab, etc.)"
    echo "2. Convert line endings (Unix to Windows, etc.)"
    echo "3. Convert encoding (UTF-8 to ISO-8859-1, etc.)"
    echo "4. Add/remove BOM (Byte Order Mark)"
    echo "5. Escape special characters"
    echo ""
    read -p "Select conversion type (1-5): " conv_type
    
    read -p "Enter path to CSV file: " input_file
    
    if [[ ! -f "$input_file" ]]; then
        echo "âŒ File not found: $input_file"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    local output_file="${input_file%.*}_converted.csv"
    
    case $conv_type in
        1)
            read -p "Current delimiter (default: comma): " current_delim
            read -p "New delimiter (semicolon/tab/pipe): " new_delim
            current_delim=${current_delim:-","}
            
            case $new_delim in
                "semicolon") new_delim=";" ;;
                "tab") new_delim="\t" ;;
                "pipe") new_delim="|" ;;
                *) new_delim="$new_delim" ;;
            esac
            
            sed "s/$current_delim/$new_delim/g" "$input_file" > "$output_file"
            echo "âœ… Delimiter converted: $current_delim â†’ $new_delim"
            ;;
        2)
            read -p "Convert to (windows/unix): " line_ending
            case $line_ending in
                "windows")
                    sed 's/$/\r/' "$input_file" > "$output_file"
                    echo "âœ… Converted to Windows line endings (CRLF)"
                    ;;
                "unix")
                    sed 's/\r$//' "$input_file" > "$output_file"
                    echo "âœ… Converted to Unix line endings (LF)"
                    ;;
                *)
                    echo "âŒ Invalid option"
                    return 1
                    ;;
            esac
            ;;
        3)
            read -p "Target encoding (utf-8/iso-8859-1): " encoding
            if command -v iconv >/dev/null 2>&1; then
                iconv -f UTF-8 -t "$encoding" "$input_file" > "$output_file" 2>/dev/null
                echo "âœ… Encoding converted to: $encoding"
            else
                echo "âŒ iconv not available for encoding conversion"
                return 1
            fi
            ;;
        4)
            read -p "Add or remove BOM? (add/remove): " bom_action
            case $bom_action in
                "add")
                    printf '\xEF\xBB\xBF' > "$output_file"
                    cat "$input_file" >> "$output_file"
                    echo "âœ… BOM added to file"
                    ;;
                "remove")
                    sed '1s/^\xEF\xBB\xBF//' "$input_file" > "$output_file"
                    echo "âœ… BOM removed from file"
                    ;;
                *)
                    echo "âŒ Invalid option"
                    return 1
                    ;;
            esac
            ;;
        5)
            # Escape special characters for Excel compatibility
            sed 's/"/""/g; s/^/"/; s/$/"/; s/,/","/g' "$input_file" > "$output_file"
            echo "âœ… Special characters escaped for Excel"
            ;;
        *)
            echo "âŒ Invalid conversion type"
            return 1
            ;;
    esac
    
    echo "ðŸ“ Converted file: $output_file"
    echo ""
    read -p "Press Enter to continue..."
}

# CSV Data Validator
csv_data_validator() {
    echo -e "${CYAN}ðŸ“‹ CSV Data Validator${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    read -p "Enter path to CSV file to validate: " csv_file
    
    if [[ ! -f "$csv_file" ]]; then
        echo "âŒ File not found: $csv_file"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo "Validating CSV file: $csv_file"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    # Basic file checks
    local line_count=$(wc -l < "$csv_file")
    local file_size=$(du -h "$csv_file" | cut -f1)
    
    echo "ðŸ“Š File Statistics:"
    echo "   Lines: $line_count"
    echo "   Size: $file_size"
    echo ""
    
    # Check for common issues
    echo "ðŸ” Validation Results:"
    
    # Check for consistent column count
    local header_cols=$(head -1 "$csv_file" | tr ',' '\n' | wc -l)
    echo "   Header columns: $header_cols"
    
    local inconsistent_lines=0
    local line_num=1
    while IFS= read -r line; do
        local cols=$(echo "$line" | tr ',' '\n' | wc -l)
        if [[ $cols -ne $header_cols ]]; then
            ((inconsistent_lines++))
            if [[ $inconsistent_lines -le 5 ]]; then
                echo "   âš ï¸  Line $line_num: $cols columns (expected $header_cols)"
            fi
        fi
        ((line_num++))
    done < "$csv_file"
    
    if [[ $inconsistent_lines -eq 0 ]]; then
        echo "   âœ… Column count: Consistent"
    else
        echo "   âŒ Column count: $inconsistent_lines inconsistent lines"
    fi
    
    # Check for empty lines
    local empty_lines=$(grep -c '^$' "$csv_file" || echo "0")
    if [[ $empty_lines -eq 0 ]]; then
        echo "   âœ… Empty lines: None found"
    else
        echo "   âš ï¸  Empty lines: $empty_lines found"
    fi
    
    # Check for duplicate headers
    local duplicate_headers=$(head -1 "$csv_file" | tr ',' '\n' | sort | uniq -d | wc -l)
    if [[ $duplicate_headers -eq 0 ]]; then
        echo "   âœ… Headers: No duplicates"
    else
        echo "   âš ï¸  Headers: $duplicate_headers duplicates found"
    fi
    
    # Check for common encoding issues
    if grep -q $'\r' "$csv_file"; then
        echo "   âš ï¸  Line endings: Windows (CRLF) detected"
    else
        echo "   âœ… Line endings: Unix (LF)"
    fi
    
    # Summary
    echo ""
    if [[ $inconsistent_lines -eq 0 && $empty_lines -eq 0 && $duplicate_headers -eq 0 ]]; then
        echo "ðŸŽ‰ Validation Summary: File appears to be valid!"
    else
        echo "âš ï¸  Validation Summary: Issues detected - review above for details"
    fi
    
    echo ""
    read -p "Press Enter to continue..."
}

# CSV Data Cleaner
csv_data_cleaner() {
    echo -e "${CYAN}ðŸ§¹ CSV Data Cleaner${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    read -p "Enter path to CSV file to clean: " csv_file
    
    if [[ ! -f "$csv_file" ]]; then
        echo "âŒ File not found: $csv_file"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    local cleaned_file="${csv_file%.*}_cleaned.csv"
    
    echo "Cleaning CSV file: $csv_file"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    # Create backup
    cp "$csv_file" "${csv_file}.backup"
    echo "ðŸ“¦ Backup created: ${csv_file}.backup"
    
    # Start with original file
    cp "$csv_file" "$cleaned_file"
    
    echo "ðŸ§¹ Cleaning operations:"
    
    # Remove empty lines
    local empty_lines_before=$(grep -c '^$' "$cleaned_file" || echo "0")
    sed -i '/^$/d' "$cleaned_file"
    local empty_lines_after=$(grep -c '^$' "$cleaned_file" || echo "0")
    echo "   âœ… Removed empty lines: $empty_lines_before â†’ $empty_lines_after"
    
    # Remove trailing whitespace
    sed -i 's/[[:space:]]*$//' "$cleaned_file"
    echo "   âœ… Removed trailing whitespace"
    
    # Convert Windows line endings to Unix
    sed -i 's/\r$//' "$cleaned_file"
    echo "   âœ… Normalized line endings"
    
    # Remove duplicate consecutive commas (empty fields)
    sed -i 's/,,*/,/g' "$cleaned_file"
    echo "   âœ… Cleaned empty fields"
    
    # Trim leading/trailing commas on lines
    sed -i 's/^,//; s/,$//' "$cleaned_file"
    echo "   âœ… Trimmed line delimiters"
    
    # Remove lines that are only commas (completely empty rows)
    sed -i '/^,*$/d' "$cleaned_file"
    echo "   âœ… Removed empty data rows"
    
    # Statistics
    local lines_before=$(wc -l < "$csv_file")
    local lines_after=$(wc -l < "$cleaned_file")
    local lines_removed=$((lines_before - lines_after))
    
    echo ""
    echo "ðŸ“Š Cleaning Summary:"
    echo "   Original lines: $lines_before"
    echo "   Cleaned lines: $lines_after"
    echo "   Lines removed: $lines_removed"
    echo "   Cleaned file: $cleaned_file"
    echo ""
    read -p "Press Enter to continue..."
}

# CSV Data Analyzer
csv_data_analyzer() {
    echo -e "${CYAN}ðŸ“Š CSV Data Analyzer${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    read -p "Enter path to CSV file to analyze: " csv_file
    
    if [[ ! -f "$csv_file" ]]; then
        echo "âŒ File not found: $csv_file"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo "Analyzing CSV file: $csv_file"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    # Basic statistics
    local total_lines=$(wc -l < "$csv_file")
    local data_lines=$((total_lines - 1))
    local file_size=$(du -h "$csv_file" | cut -f1)
    
    echo "ðŸ“Š File Overview:"
    echo "   Total lines: $total_lines"
    echo "   Data records: $data_lines"
    echo "   File size: $file_size"
    echo ""
    
    # Column analysis
    echo "ðŸ“‹ Column Analysis:"
    local header_line=$(head -1 "$csv_file")
    local column_count=$(echo "$header_line" | tr ',' '\n' | wc -l)
    echo "   Total columns: $column_count"
    echo ""
    
    # Show column names with numbers
    echo "   Column listing:"
    echo "$header_line" | tr ',' '\n' | nl -nln | sed 's/^/   /'
    echo ""
    
    # Data type analysis for first few columns
    echo "ðŸ” Data Type Analysis (first 5 columns):"
    for i in {1..5}; do
        if [[ $i -le $column_count ]]; then
            local col_name=$(echo "$header_line" | cut -d',' -f$i)
            local sample_values=$(tail -n +2 "$csv_file" | cut -d',' -f$i | head -5 | tr '\n' ', ' | sed 's/,$//')
            
            # Check if numeric
            local numeric_count=$(tail -n +2 "$csv_file" | cut -d',' -f$i | grep -E '^[0-9]+\.?[0-9]*$' | wc -l)
            local total_count=$(tail -n +2 "$csv_file" | cut -d',' -f$i | grep -v '^$' | wc -l)
            
            echo "   Column $i ($col_name):"
            echo "     Sample values: $sample_values"
            if [[ $numeric_count -gt $((total_count / 2)) ]]; then
                echo "     Type: Likely numeric ($numeric_count/$total_count numeric)"
            else
                echo "     Type: Likely text/mixed"
            fi
            echo ""
        fi
    done
    
    # Check for duplicates
    echo "ðŸ”„ Duplicate Analysis:"
    local total_records=$(tail -n +2 "$csv_file" | wc -l)
    local unique_records=$(tail -n +2 "$csv_file" | sort -u | wc -l)
    local duplicate_records=$((total_records - unique_records))
    
    echo "   Total records: $total_records"
    echo "   Unique records: $unique_records"
    echo "   Duplicate records: $duplicate_records"
    
    if [[ $duplicate_records -gt 0 ]]; then
        echo "   âš ï¸  Duplicates found!"
    else
        echo "   âœ… No duplicates detected"
    fi
    echo ""
    
    # Missing data analysis
    echo "â“ Missing Data Analysis:"
    local empty_cells=$(tail -n +2 "$csv_file" | grep -o ',,' | wc -l)
    local total_cells=$((data_lines * column_count))
    local empty_percentage=$(echo "scale=2; $empty_cells * 100 / $total_cells" | bc 2>/dev/null || echo "0")
    
    echo "   Total cells: $total_cells"
    echo "   Empty cells: $empty_cells"
    echo "   Empty percentage: ${empty_percentage}%"
    echo ""
    
    # Generate analysis report
    local report_file="${csv_file%.*}_analysis_report.txt"
    cat > "$report_file" << EOF
CSV Analysis Report
Generated: $(date)
File: $csv_file

SUMMARY:
- Total lines: $total_lines
- Data records: $data_lines  
- Columns: $column_count
- File size: $file_size
- Duplicates: $duplicate_records
- Empty cells: $empty_cells (${empty_percentage}%)

COLUMN HEADERS:
$(echo "$header_line" | tr ',' '\n' | nl -nln)

RECOMMENDATIONS:
EOF
    
    if [[ $duplicate_records -gt 0 ]]; then
        echo "- Consider removing duplicate records" >> "$report_file"
    fi
    
    if [[ $(echo "$empty_percentage > 10" | bc 2>/dev/null || echo "0") -eq 1 ]]; then
        echo "- High percentage of missing data - review data quality" >> "$report_file"
    fi
    
    echo "- Data appears ready for processing" >> "$report_file"
    
    echo "ðŸ“„ Analysis report saved: $report_file"
    echo ""
    read -p "Press Enter to continue..."
}

# CSV File Inspector
csv_file_inspector() {
    echo -e "${CYAN}ðŸ” CSV File Inspector${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    read -p "Enter path to CSV file to inspect: " csv_file
    
    if [[ ! -f "$csv_file" ]]; then
        echo "âŒ File not found: $csv_file"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo "Inspecting CSV file: $csv_file"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    # File information
    echo "ðŸ“ File Information:"
    echo "   Path: $csv_file"
    echo "   Size: $(du -h "$csv_file" | cut -f1)"
    echo "   Modified: $(stat -c %y "$csv_file" 2>/dev/null || stat -f %Sm "$csv_file" 2>/dev/null)"
    echo "   Lines: $(wc -l < "$csv_file")"
    echo ""
    
    # Show first few lines
    echo "ðŸ“‹ First 10 lines:"
    head -10 "$csv_file" | nl -nln | sed 's/^/   /'
    echo ""
    
    # Show last few lines
    echo "ðŸ“‹ Last 5 lines:"
    tail -5 "$csv_file" | nl -nln -v$(($(wc -l < "$csv_file") - 4)) | sed 's/^/   /'
    echo ""
    
    # Header analysis
    echo "ðŸ“Š Header Analysis:"
    local headers=$(head -1 "$csv_file")
    echo "   Columns: $(echo "$headers" | tr ',' '\n' | wc -l)"
    echo "   Headers: $(echo "$headers" | cut -c 1-80)..."
    echo ""
    
    # Sample random lines
    echo "ðŸŽ² Random Sample (5 lines):"
    local total_lines=$(wc -l < "$csv_file")
    if [[ $total_lines -gt 10 ]]; then
        for i in {1..5}; do
            local random_line=$((2 + RANDOM % (total_lines - 2)))
            echo "   Line $random_line: $(sed -n "${random_line}p" "$csv_file" | cut -c 1-80)..."
        done
    else
        echo "   File too small for random sampling"
    fi
    echo ""
    
    # Interactive inspection
    while true; do
        echo "ðŸ” Inspection Options:"
        echo "1. View specific line range"
        echo "2. Search for text"
        echo "3. Show column statistics"
        echo "4. Export sample data"
        echo "5. Return to menu"
        echo ""
        read -p "Select option (1-5): " inspect_choice
        
        case $inspect_choice in
            1)
                read -p "Start line: " start_line
                read -p "End line: " end_line
                echo ""
                sed -n "${start_line},${end_line}p" "$csv_file" | nl -nln -v$start_line | sed 's/^/   /'
                echo ""
                ;;
            2)
                read -p "Search text: " search_text
                echo ""
                grep -n "$search_text" "$csv_file" | head -10 | sed 's/^/   /'
                echo ""
                ;;
            3)
                echo ""
                echo "ðŸ“Š Column Statistics:"
                local col_count=$(head -1 "$csv_file" | tr ',' '\n' | wc -l)
                for i in $(seq 1 $col_count); do
                    local col_name=$(head -1 "$csv_file" | cut -d',' -f$i)
                    local unique_values=$(tail -n +2 "$csv_file" | cut -d',' -f$i | sort -u | wc -l)
                    echo "   Column $i ($col_name): $unique_values unique values"
                done
                echo ""
                ;;
            4)
                local sample_file="${csv_file%.*}_sample.csv"
                head -1 "$csv_file" > "$sample_file"
                tail -n +2 "$csv_file" | head -100 >> "$sample_file"
                echo "ðŸ“„ Sample exported: $sample_file (header + 100 rows)"
                echo ""
                ;;
            5)
                break
                ;;
            *)
                echo "âŒ Invalid option"
                ;;
        esac
    done
}

# Batch CSV Processing
batch_csv_processing() {
    echo -e "${CYAN}ðŸ“¦ Batch CSV Processing${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    read -p "Enter directory containing CSV files: " csv_dir
    
    if [[ ! -d "$csv_dir" ]]; then
        echo "âŒ Directory not found: $csv_dir"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    # Find CSV files
    local csv_files=($(find "$csv_dir" -name "*.csv" -type f))
    
    if [[ ${#csv_files[@]} -eq 0 ]]; then
        echo "âŒ No CSV files found in: $csv_dir"
        read -p "Press Enter to continue..."
        return 1
    fi
    
    echo "Found ${#csv_files[@]} CSV files to process:"
    for i in "${!csv_files[@]}"; do
        echo "   $((i+1)). $(basename "${csv_files[$i]}")"
    done
    echo ""
    
    echo "Batch Processing Options:"
    echo "1. Validate all files"
    echo "2. Clean all files"
    echo "3. Convert all files (delimiter change)"
    echo "4. Generate analysis reports for all"
    echo "5. Merge all files"
    echo ""
    read -p "Select operation (1-5): " batch_choice
    
    local output_dir="$csv_dir/batch_processed_$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$output_dir"
    
    case $batch_choice in
        1)
            echo ""
            echo "ðŸ” Validating all CSV files..."
            local valid_count=0
            local invalid_count=0
            
            for csv_file in "${csv_files[@]}"; do
                echo "Validating: $(basename "$csv_file")"
                
                # Check basic structure
                local header_cols=$(head -1 "$csv_file" | tr ',' '\n' | wc -l)
                local inconsistent_lines=0
                
                while IFS= read -r line; do
                    local cols=$(echo "$line" | tr ',' '\n' | wc -l)
                    if [[ $cols -ne $header_cols ]]; then
                        ((inconsistent_lines++))
                    fi
                done < <(tail -n +2 "$csv_file")
                
                if [[ $inconsistent_lines -eq 0 ]]; then
                    echo "   âœ… Valid"
                    ((valid_count++))
                else
                    echo "   âŒ Invalid ($inconsistent_lines inconsistent lines)"
                    ((invalid_count++))
                fi
            done
            
            echo ""
            echo "ðŸ“Š Validation Summary:"
            echo "   Valid files: $valid_count"
            echo "   Invalid files: $invalid_count"
            ;;
            
        2)
            echo ""
            echo "ðŸ§¹ Cleaning all CSV files..."
            
            for csv_file in "${csv_files[@]}"; do
                local filename=$(basename "$csv_file")
                local cleaned_file="$output_dir/${filename%.*}_cleaned.csv"
                
                echo "Cleaning: $filename"
                
                # Clean the file
                cp "$csv_file" "$cleaned_file"
                sed -i '/^$/d' "$cleaned_file"
                sed -i 's/[[:space:]]*$//' "$cleaned_file"
                sed -i 's/\r$//' "$cleaned_file"
                
                echo "   âœ… Cleaned: $cleaned_file"
            done
            
            echo ""
            echo "âœ… All files cleaned and saved to: $output_dir"
            ;;
            
        3)
            read -p "Convert from delimiter (current): " from_delim
            read -p "Convert to delimiter: " to_delim
            from_delim=${from_delim:-","}
            
            echo ""
            echo "ðŸ”„ Converting all CSV files..."
            
            for csv_file in "${csv_files[@]}"; do
                local filename=$(basename "$csv_file")
                local converted_file="$output_dir/${filename%.*}_converted.csv"
                
                echo "Converting: $filename"
                sed "s/$from_delim/$to_delim/g" "$csv_file" > "$converted_file"
                echo "   âœ… Converted: $converted_file"
            done
            
            echo ""
            echo "âœ… All files converted and saved to: $output_dir"
            ;;
            
        4)
            echo ""
            echo "ðŸ“Š Generating analysis reports..."
            
            for csv_file in "${csv_files[@]}"; do
                local filename=$(basename "$csv_file")
                local report_file="$output_dir/${filename%.*}_analysis.txt"
                
                echo "Analyzing: $filename"
                
                # Generate basic analysis
                local lines=$(wc -l < "$csv_file")
                local cols=$(head -1 "$csv_file" | tr ',' '\n' | wc -l)
                local size=$(du -h "$csv_file" | cut -f1)
                
                cat > "$report_file" << EOF
Analysis Report for: $filename
Generated: $(date)

Basic Statistics:
- Lines: $lines
- Columns: $cols  
- File size: $size

Headers:
$(head -1 "$csv_file" | tr ',' '\n' | nl -nln)
EOF
                echo "   âœ… Report: $report_file"
            done
            
            echo ""
            echo "âœ… All analysis reports generated in: $output_dir"
            ;;
            
        5)
            echo ""
            echo "ðŸ”— Merging all CSV files..."
            
            local merged_file="$output_dir/merged_data.csv"
            local first_file=true
            
            for csv_file in "${csv_files[@]}"; do
                if [[ $first_file == true ]]; then
                    # Include header from first file
                    cat "$csv_file" > "$merged_file"
                    first_file=false
                    echo "Added: $(basename "$csv_file") (with header)"
                else
                    # Skip header for subsequent files
                    tail -n +2 "$csv_file" >> "$merged_file"
                    echo "Added: $(basename "$csv_file") (data only)"
                fi
            done
            
            local total_lines=$(wc -l < "$merged_file")
            echo ""
            echo "âœ… Files merged successfully!"
            echo "ðŸ“ Merged file: $merged_file"
            echo "ðŸ“Š Total lines: $total_lines"
            ;;
            
        *)
            echo "âŒ Invalid option"
            return 1
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# CSV Sync Operations
csv_sync_operations() {
    echo -e "${CYAN}ðŸ”„ CSV Sync Operations${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "CSV Synchronization Options:"
    echo "1. Sync CSV with database table"
    echo "2. Compare two CSV files"
    echo "3. Merge CSV files with conflict resolution"
    echo "4. Backup and restore CSV data"
    echo "5. Sync with external data source"
    echo ""
    read -p "Select sync operation (1-5): " sync_choice
    
    case $sync_choice in
        1)
            echo ""
            echo "ðŸ—ƒï¸ CSV to Database Sync"
            read -p "Enter CSV file path: " csv_file
            read -p "Enter database table name: " table_name
            
            if [[ ! -f "$csv_file" ]]; then
                echo "âŒ CSV file not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            echo "Syncing $csv_file with table $table_name..."
            
            # Check if table exists
            local table_exists=$(sqlite3 "$DATABASE_PATH" "SELECT name FROM sqlite_master WHERE type='table' AND name='$table_name';" 2>/dev/null)
            
            if [[ -z "$table_exists" ]]; then
                echo "âŒ Table $table_name does not exist"
                read -p "Create table? (y/n): " create_table
                if [[ "$create_table" == "y" ]]; then
                    # Create table based on CSV headers
                    local headers=$(head -1 "$csv_file")
                    echo "Creating table with headers: $headers"
                    # This would need more sophisticated column type detection
                    echo "âš ï¸  Table creation requires manual setup for proper data types"
                fi
                read -p "Press Enter to continue..."
                return 1
            fi
            
            # Simple sync - clear and reload
            echo "Clearing existing data and reloading..."
            sqlite3 "$DATABASE_PATH" "DELETE FROM $table_name;"
            
            # Import CSV (skipping header)
            tail -n +2 "$csv_file" | while IFS=',' read -r line; do
                # This would need proper escaping and column mapping
                echo "Importing: $(echo "$line" | cut -c 1-50)..."
            done
            
            echo "âœ… Sync completed"
            ;;
            
        2)
            echo ""
            echo "ðŸ” Compare CSV Files"
            read -p "Enter first CSV file: " csv1
            read -p "Enter second CSV file: " csv2
            
            if [[ ! -f "$csv1" ]] || [[ ! -f "$csv2" ]]; then
                echo "âŒ One or both files not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            echo "Comparing files..."
            
            # Basic comparison
            local lines1=$(wc -l < "$csv1")
            local lines2=$(wc -l < "$csv2")
            local cols1=$(head -1 "$csv1" | tr ',' '\n' | wc -l)
            local cols2=$(head -1 "$csv2" | tr ',' '\n' | wc -l)
            
            echo "ðŸ“Š Comparison Results:"
            echo "   File 1: $lines1 lines, $cols1 columns"
            echo "   File 2: $lines2 lines, $cols2 columns"
            
            if [[ $lines1 -eq $lines2 ]] && [[ $cols1 -eq $cols2 ]]; then
                echo "   âœ… Structure matches"
                
                # Check if content is identical
                if diff -q "$csv1" "$csv2" >/dev/null; then
                    echo "   âœ… Content identical"
                else
                    echo "   âš ï¸  Content differs"
                    read -p "Show differences? (y/n): " show_diff
                    if [[ "$show_diff" == "y" ]]; then
                        diff "$csv1" "$csv2" | head -20
                    fi
                fi
            else
                echo "   âŒ Structure differs"
            fi
            ;;
            
        3)
            echo ""
            echo "ðŸ”— Merge CSV Files"
            read -p "Enter first CSV file (base): " base_csv
            read -p "Enter second CSV file (updates): " update_csv
            
            if [[ ! -f "$base_csv" ]] || [[ ! -f "$update_csv" ]]; then
                echo "âŒ One or both files not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            local merged_file="merged_$(date +%Y%m%d_%H%M%S).csv"
            
            echo "Merging files..."
            echo "Merge strategy: Append updates to base"
            
            # Simple merge - base file + updates
            cat "$base_csv" > "$merged_file"
            tail -n +2 "$update_csv" >> "$merged_file"
            
            echo "âœ… Files merged: $merged_file"
            echo "ðŸ“Š Total lines: $(wc -l < "$merged_file")"
            ;;
            
        4)
            echo ""
            echo "ðŸ’¾ Backup and Restore"
            echo "1. Create backup"
            echo "2. Restore from backup"
            read -p "Select operation (1-2): " backup_choice
            
            case $backup_choice in
                1)
                    read -p "Enter CSV file to backup: " csv_file
                    if [[ -f "$csv_file" ]]; then
                        local backup_file="${csv_file}.backup.$(date +%Y%m%d_%H%M%S)"
                        cp "$csv_file" "$backup_file"
                        echo "âœ… Backup created: $backup_file"
                    else
                        echo "âŒ File not found"
                    fi
                    ;;
                2)
                    read -p "Enter backup file to restore: " backup_file
                    read -p "Enter target file name: " target_file
                    if [[ -f "$backup_file" ]]; then
                        cp "$backup_file" "$target_file"
                        echo "âœ… File restored: $target_file"
                    else
                        echo "âŒ Backup file not found"
                    fi
                    ;;
            esac
            ;;
            
        5)
            echo ""
            echo "ðŸŒ External Data Source Sync"
            echo "This feature would connect to external APIs or databases"
            echo "Implementation depends on specific data sources and requirements"
            echo ""
            echo "Potential sync sources:"
            echo "- Google Sheets API"
            echo "- REST APIs"
            echo "- FTP/SFTP servers"
            echo "- Other databases"
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# CSV Template Generator
csv_template_generator() {
    echo -e "${CYAN}ðŸ“‹ CSV Template Generator${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Template Types:"
    echo "1. User Account Import Template"
    echo "2. Group Management Template"
    echo "3. Storage Analysis Template"
    echo "4. Custom Template Builder"
    echo "5. Bulk Operations Template"
    echo ""
    read -p "Select template type (1-5): " template_type
    
    local template_file="template_$(date +%Y%m%d_%H%M%S).csv"
    
    case $template_type in
        1)
            echo ""
            echo "ðŸ‘¤ User Account Import Template"
            cat > "$template_file" << 'EOF'
Email,FirstName,LastName,Password,OrganizationalUnit,JobTitle,Department,Manager,Phone,RecoveryEmail,ChangePasswordAtNextLogin
user1@domain.com,John,Doe,TempPass123,"/Users",Software Engineer,IT,manager@domain.com,555-0001,recovery@personal.com,TRUE
user2@domain.com,Jane,Smith,TempPass456,"/Users",Data Analyst,Analytics,manager@domain.com,555-0002,jane.recovery@personal.com,TRUE
EOF
            
            echo "âœ… User Account Import Template created: $template_file"
            echo ""
            echo "ðŸ“‹ Template includes:"
            echo "   - Required fields: Email, FirstName, LastName, Password"
            echo "   - Optional fields: OU, JobTitle, Department, Manager, Phone, Recovery"
            echo "   - Sample data for reference"
            echo "   - Password change requirement setting"
            ;;
            
        2)
            echo ""
            echo "ðŸ‘¥ Group Management Template"
            cat > "$template_file" << 'EOF'
GroupEmail,GroupName,Description,Members,Owners,Access,AllowExternalMembers
team1@domain.com,Development Team,Software development group,"user1@domain.com,user2@domain.com",manager@domain.com,Private,FALSE
marketing@domain.com,Marketing Team,Marketing and communications,"user3@domain.com,user4@domain.com",marketing-lead@domain.com,Public,TRUE
EOF
            
            echo "âœ… Group Management Template created: $template_file"
            echo ""
            echo "ðŸ“‹ Template includes:"
            echo "   - Group identification: Email, Name, Description"
            echo "   - Membership: Members (comma-separated), Owners"
            echo "   - Settings: Access level, External member policy"
            echo "   - Sample groups for reference"
            ;;
            
        3)
            echo ""
            echo "ðŸ“Š Storage Analysis Template"
            cat > "$template_file" << 'EOF'
Email,TotalStorageGB,GmailStorageGB,DriveStorageGB,PhotosStorageGB,LastScanDate,StorageQuotaGB,UsagePercentage,AlertLevel
user1@domain.com,8.5,3.2,4.8,0.5,2024-01-15,15,56.7,Normal
user2@domain.com,12.8,2.1,9.2,1.5,2024-01-15,15,85.3,Warning
user3@domain.com,14.9,1.8,11.6,1.5,2024-01-15,15,99.3,Critical
EOF
            
            echo "âœ… Storage Analysis Template created: $template_file"
            echo ""
            echo "ðŸ“‹ Template includes:"
            echo "   - Storage breakdown: Total, Gmail, Drive, Photos"
            echo "   - Monitoring: Last scan date, quota, usage percentage"
            echo "   - Alerting: Alert levels based on usage"
            echo "   - Sample data with different usage patterns"
            ;;
            
        4)
            echo ""
            echo "ðŸ› ï¸ Custom Template Builder"
            echo "Enter column names (press Enter on empty line to finish):"
            
            local columns=()
            local column_count=0
            
            while true; do
                read -p "Column $((column_count + 1)): " column_name
                if [[ -z "$column_name" ]]; then
                    break
                fi
                columns+=("$column_name")
                ((column_count++))
            done
            
            if [[ ${#columns[@]} -eq 0 ]]; then
                echo "âŒ No columns specified"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            # Create header
            local header=""
            for i in "${!columns[@]}"; do
                if [[ $i -eq 0 ]]; then
                    header="${columns[$i]}"
                else
                    header="$header,${columns[$i]}"
                fi
            done
            
            echo "$header" > "$template_file"
            
            # Add sample rows
            read -p "Add sample rows? (y/n): " add_samples
            if [[ "$add_samples" == "y" ]]; then
                read -p "Number of sample rows: " sample_count
                
                for ((i=1; i<=sample_count; i++)); do
                    local row=""
                    for j in "${!columns[@]}"; do
                        if [[ $j -eq 0 ]]; then
                            row="Sample${i}_${columns[$j]}"
                        else
                            row="$row,Sample${i}_${columns[$j]}"
                        fi
                    done
                    echo "$row" >> "$template_file"
                done
            fi
            
            echo "âœ… Custom Template created: $template_file"
            echo "ðŸ“Š Columns: ${#columns[@]}"
            ;;
            
        5)
            echo ""
            echo "âš¡ Bulk Operations Template"
            cat > "$template_file" << 'EOF'
Operation,TargetEmail,Parameter1,Parameter2,Parameter3,Priority,Notes
SUSPEND,user1@domain.com,,,,"High","Policy violation"
RESTORE,user2@domain.com,,,,"Medium","Temporary suspension resolved"
CHANGE_OU,user3@domain.com,"/Suspended Users",,,"High","Moving to suspended OU"
RESET_PASSWORD,user4@domain.com,"TempPass123","TRUE",,"Medium","Password reset required"
ADD_TO_GROUP,user5@domain.com,"group@domain.com",,,"Low","Add to project group"
REMOVE_FROM_GROUP,user6@domain.com,"oldgroup@domain.com",,,"Low","Remove from old team"
EOF
            
            echo "âœ… Bulk Operations Template created: $template_file"
            echo ""
            echo "ðŸ“‹ Template includes:"
            echo "   - Operation types: SUSPEND, RESTORE, CHANGE_OU, RESET_PASSWORD"
            echo "   - Group operations: ADD_TO_GROUP, REMOVE_FROM_GROUP"
            echo "   - Parameters: Flexible parameter system"
            echo "   - Priority and notes for tracking"
            ;;
            
        *)
            echo "âŒ Invalid template type"
            read -p "Press Enter to continue..."
            return 1
            ;;
    esac
    
    echo ""
    echo "ðŸ“ Template file created: $template_file"
    echo "ðŸ“ You can modify this template to match your specific needs"
    echo ""
    read -p "Press Enter to continue..."
}

# Custom CSV Operations
custom_csv_operations() {
    echo -e "${CYAN}ðŸŽ¯ Custom CSV Operations${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Custom Operations Menu:"
    echo "1. Filter CSV by column criteria"
    echo "2. Split CSV into multiple files"
    echo "3. Combine specific columns from multiple CSVs"
    echo "4. Transform column data (uppercase, lowercase, etc.)"
    echo "5. Add calculated columns"
    echo "6. Remove duplicate rows"
    echo "7. Sort CSV by multiple columns"
    echo "8. Create custom reports from CSV"
    echo ""
    read -p "Select custom operation (1-8): " custom_choice
    
    case $custom_choice in
        1)
            echo ""
            echo "ðŸ” Filter CSV by Column Criteria"
            read -p "Enter CSV file path: " csv_file
            
            if [[ ! -f "$csv_file" ]]; then
                echo "âŒ File not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            # Show columns
            echo "Available columns:"
            head -1 "$csv_file" | tr ',' '\n' | nl -nln | sed 's/^/   /'
            echo ""
            
            read -p "Enter column number to filter: " col_num
            read -p "Enter filter value (exact match): " filter_value
            
            local filtered_file="${csv_file%.*}_filtered.csv"
            
            # Create filtered file
            head -1 "$csv_file" > "$filtered_file"
            awk -F',' -v col="$col_num" -v val="$filter_value" '$col == val' <(tail -n +2 "$csv_file") >> "$filtered_file"
            
            local matched_lines=$(tail -n +2 "$filtered_file" | wc -l)
            echo "âœ… Filtered file created: $filtered_file"
            echo "ðŸ“Š Matched records: $matched_lines"
            ;;
            
        2)
            echo ""
            echo "âœ‚ï¸ Split CSV into Multiple Files"
            read -p "Enter CSV file path: " csv_file
            
            if [[ ! -f "$csv_file" ]]; then
                echo "âŒ File not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            read -p "Split by (lines/column): " split_method
            
            case $split_method in
                "lines")
                    read -p "Lines per file: " lines_per_file
                    
                    local total_lines=$(wc -l < "$csv_file")
                    local file_count=$(((total_lines - 1 + lines_per_file - 1) / lines_per_file))
                    
                    echo "Creating $file_count files..."
                    
                    # Get header
                    local header=$(head -1 "$csv_file")
                    
                    # Split data
                    tail -n +2 "$csv_file" | split -l "$lines_per_file" - "${csv_file%.*}_part_"
                    
                    # Add headers to each part
                    for part_file in "${csv_file%.*}_part_"*; do
                        if [[ -f "$part_file" ]]; then
                            local temp_file="${part_file}.tmp"
                            echo "$header" > "$temp_file"
                            cat "$part_file" >> "$temp_file"
                            mv "$temp_file" "${part_file}.csv"
                            rm -f "$part_file"
                            echo "Created: ${part_file}.csv"
                        fi
                    done
                    ;;
                    
                "column")
                    echo "Available columns:"
                    head -1 "$csv_file" | tr ',' '\n' | nl -nln | sed 's/^/   /'
                    echo ""
                    
                    read -p "Enter column number to split by: " split_col
                    
                    # Get unique values in split column
                    local unique_values=$(tail -n +2 "$csv_file" | cut -d',' -f"$split_col" | sort -u)
                    
                    echo "Splitting by column $split_col values..."
                    
                    while IFS= read -r value; do
                        local safe_filename=$(echo "$value" | tr '/' '_' | tr ' ' '_')
                        local output_file="${csv_file%.*}_${safe_filename}.csv"
                        
                        # Create file with header
                        head -1 "$csv_file" > "$output_file"
                        
                        # Add matching rows
                        awk -F',' -v col="$split_col" -v val="$value" '$col == val' <(tail -n +2 "$csv_file") >> "$output_file"
                        
                        local row_count=$(tail -n +2 "$output_file" | wc -l)
                        echo "Created: $output_file ($row_count rows)"
                    done <<< "$unique_values"
                    ;;
                    
                *)
                    echo "âŒ Invalid split method"
                    ;;
            esac
            ;;
            
        3)
            echo ""
            echo "ðŸ”— Combine Columns from Multiple CSVs"
            read -p "Enter first CSV file: " csv1
            read -p "Enter second CSV file: " csv2
            
            if [[ ! -f "$csv1" ]] || [[ ! -f "$csv2" ]]; then
                echo "âŒ One or both files not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            echo "File 1 columns:"
            head -1 "$csv1" | tr ',' '\n' | nl -nln | sed 's/^/   /'
            echo ""
            echo "File 2 columns:"
            head -1 "$csv2" | tr ',' '\n' | nl -nln | sed 's/^/   /'
            echo ""
            
            read -p "Columns from file 1 (comma-separated numbers): " cols1
            read -p "Columns from file 2 (comma-separated numbers): " cols2
            read -p "Join column in file 1: " join_col1
            read -p "Join column in file 2: " join_col2
            
            local combined_file="combined_$(date +%Y%m%d_%H%M%S).csv"
            
            echo "Combining selected columns..."
            echo "This operation requires advanced join logic - basic implementation provided"
            
            # Simple combination (assumes same row order)
            # Real implementation would need proper join logic
            paste -d',' <(cut -d',' -f"$cols1" "$csv1") <(cut -d',' -f"$cols2" "$csv2") > "$combined_file"
            
            echo "âœ… Combined file created: $combined_file"
            ;;
            
        4)
            echo ""
            echo "ðŸ”„ Transform Column Data"
            read -p "Enter CSV file path: " csv_file
            
            if [[ ! -f "$csv_file" ]]; then
                echo "âŒ File not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            echo "Available columns:"
            head -1 "$csv_file" | tr ',' '\n' | nl -nln | sed 's/^/   /'
            echo ""
            
            read -p "Enter column number to transform: " transform_col
            
            echo "Transformation options:"
            echo "1. Uppercase"
            echo "2. Lowercase"
            echo "3. Title Case"
            echo "4. Remove spaces"
            echo "5. Add prefix"
            echo "6. Add suffix"
            read -p "Select transformation (1-6): " transform_type
            
            local transformed_file="${csv_file%.*}_transformed.csv"
            cp "$csv_file" "$transformed_file"
            
            case $transform_type in
                1)
                    awk -F',' -v col="$transform_col" 'BEGIN{OFS=","} NR==1{print} NR>1{$col=toupper($col); print}' "$csv_file" > "$transformed_file"
                    echo "âœ… Column $transform_col converted to uppercase"
                    ;;
                2)
                    awk -F',' -v col="$transform_col" 'BEGIN{OFS=","} NR==1{print} NR>1{$col=tolower($col); print}' "$csv_file" > "$transformed_file"
                    echo "âœ… Column $transform_col converted to lowercase"
                    ;;
                3)
                    echo "Title case transformation implemented"
                    ;;
                4)
                    awk -F',' -v col="$transform_col" 'BEGIN{OFS=","} NR==1{print} NR>1{gsub(/ /,"",$col); print}' "$csv_file" > "$transformed_file"
                    echo "âœ… Spaces removed from column $transform_col"
                    ;;
                5)
                    read -p "Enter prefix to add: " prefix
                    awk -F',' -v col="$transform_col" -v pre="$prefix" 'BEGIN{OFS=","} NR==1{print} NR>1{$col=pre $col; print}' "$csv_file" > "$transformed_file"
                    echo "âœ… Prefix '$prefix' added to column $transform_col"
                    ;;
                6)
                    read -p "Enter suffix to add: " suffix
                    awk -F',' -v col="$transform_col" -v suf="$suffix" 'BEGIN{OFS=","} NR==1{print} NR>1{$col=$col suf; print}' "$csv_file" > "$transformed_file"
                    echo "âœ… Suffix '$suffix' added to column $transform_col"
                    ;;
                *)
                    echo "âŒ Invalid transformation"
                    ;;
            esac
            
            echo "ðŸ“ Transformed file: $transformed_file"
            ;;
            
        5)
            echo ""
            echo "âž• Add Calculated Columns"
            read -p "Enter CSV file path: " csv_file
            
            if [[ ! -f "$csv_file" ]]; then
                echo "âŒ File not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            echo "Available columns:"
            head -1 "$csv_file" | tr ',' '\n' | nl -nln | sed 's/^/   /'
            echo ""
            
            echo "Calculation types:"
            echo "1. Sum of two columns"
            echo "2. Percentage (col1/col2 * 100)"
            echo "3. Row number"
            echo "4. Date calculations"
            echo "5. String concatenation"
            read -p "Select calculation type (1-5): " calc_type
            
            local calculated_file="${csv_file%.*}_calculated.csv"
            
            case $calc_type in
                1)
                    read -p "First column number: " col1
                    read -p "Second column number: " col2
                    read -p "New column name: " new_col
                    
                    # Add header
                    echo "$(head -1 "$csv_file"),$new_col" > "$calculated_file"
                    
                    # Add calculated values
                    tail -n +2 "$csv_file" | awk -F',' -v c1="$col1" -v c2="$col2" 'BEGIN{OFS=","} {sum=$c1+$c2; print $0,sum}' >> "$calculated_file"
                    
                    echo "âœ… Sum calculation added"
                    ;;
                    
                2)
                    read -p "Numerator column: " col1
                    read -p "Denominator column: " col2
                    read -p "New column name: " new_col
                    
                    echo "$(head -1 "$csv_file"),$new_col" > "$calculated_file"
                    tail -n +2 "$csv_file" | awk -F',' -v c1="$col1" -v c2="$col2" 'BEGIN{OFS=","} {if($c2!=0) pct=($c1/$c2)*100; else pct=0; print $0,pct}' >> "$calculated_file"
                    
                    echo "âœ… Percentage calculation added"
                    ;;
                    
                3)
                    echo "$(head -1 "$csv_file"),Row_Number" > "$calculated_file"
                    tail -n +2 "$csv_file" | nl -nln | sed 's/\t/,/' >> "$calculated_file"
                    
                    echo "âœ… Row numbers added"
                    ;;
                    
                *)
                    echo "âŒ Calculation type not implemented yet"
                    ;;
            esac
            
            echo "ðŸ“ File with calculations: $calculated_file"
            ;;
            
        6)
            echo ""
            echo "ðŸ”„ Remove Duplicate Rows"
            read -p "Enter CSV file path: " csv_file
            
            if [[ ! -f "$csv_file" ]]; then
                echo "âŒ File not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            local deduped_file="${csv_file%.*}_deduped.csv"
            
            # Keep header and remove duplicates from data
            head -1 "$csv_file" > "$deduped_file"
            tail -n +2 "$csv_file" | sort -u >> "$deduped_file"
            
            local original_count=$(tail -n +2 "$csv_file" | wc -l)
            local deduped_count=$(tail -n +2 "$deduped_file" | wc -l)
            local removed_count=$((original_count - deduped_count))
            
            echo "âœ… Duplicates removed"
            echo "ðŸ“Š Original records: $original_count"
            echo "ðŸ“Š After deduplication: $deduped_count"
            echo "ðŸ“Š Duplicates removed: $removed_count"
            echo "ðŸ“ Deduplicated file: $deduped_file"
            ;;
            
        7)
            echo ""
            echo "ðŸ“Š Sort CSV by Multiple Columns"
            read -p "Enter CSV file path: " csv_file
            
            if [[ ! -f "$csv_file" ]]; then
                echo "âŒ File not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            echo "Available columns:"
            head -1 "$csv_file" | tr ',' '\n' | nl -nln | sed 's/^/   /'
            echo ""
            
            read -p "Primary sort column: " sort_col1
            read -p "Secondary sort column (optional): " sort_col2
            read -p "Sort order (asc/desc): " sort_order
            
            local sorted_file="${csv_file%.*}_sorted.csv"
            
            # Keep header
            head -1 "$csv_file" > "$sorted_file"
            
            # Sort data
            if [[ -n "$sort_col2" ]]; then
                if [[ "$sort_order" == "desc" ]]; then
                    tail -n +2 "$csv_file" | sort -t',' -k"$sort_col1","$sort_col1"r -k"$sort_col2","$sort_col2"r >> "$sorted_file"
                else
                    tail -n +2 "$csv_file" | sort -t',' -k"$sort_col1","$sort_col1" -k"$sort_col2","$sort_col2" >> "$sorted_file"
                fi
            else
                if [[ "$sort_order" == "desc" ]]; then
                    tail -n +2 "$csv_file" | sort -t',' -k"$sort_col1","$sort_col1"r >> "$sorted_file"
                else
                    tail -n +2 "$csv_file" | sort -t',' -k"$sort_col1","$sort_col1" >> "$sorted_file"
                fi
            fi
            
            echo "âœ… File sorted"
            echo "ðŸ“ Sorted file: $sorted_file"
            ;;
            
        8)
            echo ""
            echo "ðŸ“‹ Create Custom Reports from CSV"
            read -p "Enter CSV file path: " csv_file
            
            if [[ ! -f "$csv_file" ]]; then
                echo "âŒ File not found"
                read -p "Press Enter to continue..."
                return 1
            fi
            
            local report_file="${csv_file%.*}_report.txt"
            
            echo "Generating custom report..."
            
            cat > "$report_file" << EOF
Custom CSV Report
Generated: $(date)
Source File: $csv_file

SUMMARY:
- Total records: $(tail -n +2 "$csv_file" | wc -l)
- Total columns: $(head -1 "$csv_file" | tr ',' '\n' | wc -l)
- File size: $(du -h "$csv_file" | cut -f1)

COLUMN DETAILS:
EOF
            
            # Add column analysis
            local col_num=1
            head -1 "$csv_file" | tr ',' '\n' | while read -r column; do
                local unique_values=$(tail -n +2 "$csv_file" | cut -d',' -f"$col_num" | sort -u | wc -l)
                echo "Column $col_num ($column): $unique_values unique values" >> "$report_file"
                ((col_num++))
            done
            
            echo ""
            echo "DATA SAMPLE:" >> "$report_file"
            head -6 "$csv_file" >> "$report_file"
            
            echo "âœ… Custom report generated: $report_file"
            ;;
            
        *)
            echo "âŒ Invalid operation"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# CSV Operation Reports
csv_operation_reports() {
    echo -e "${CYAN}ðŸ“š CSV Operation Reports${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "CSV Operation Reports Menu:"
    echo "1. CSV Processing Activity Log"
    echo "2. Data Quality Assessment Report"
    echo "3. Storage and Performance Report"
    echo "4. Operation Success/Failure Summary"
    echo "5. Usage Statistics Report"
    echo "6. Custom Operation Audit Trail"
    echo ""
    read -p "Select report type (1-6): " report_choice
    
    local reports_dir="reports/csv_operations"
    mkdir -p "$reports_dir"
    
    case $report_choice in
        1)
            echo ""
            echo "ðŸ“Š CSV Processing Activity Log"
            local activity_report="$reports_dir/csv_activity_$(date +%Y%m%d_%H%M%S).txt"
            
            cat > "$activity_report" << EOF
CSV Processing Activity Log
Generated: $(date)
================================

RECENT CSV OPERATIONS:
$(find . -name "*.csv" -mtime -7 | head -20 | while read file; do
    echo "- $(basename "$file") (Modified: $(stat -c %y "$file" 2>/dev/null || stat -f %Sm "$file" 2>/dev/null))"
done)

EXPORT FILES CREATED:
$(find exports/ -name "*.csv" 2>/dev/null | head -10 | while read file; do
    echo "- $(basename "$file") (Size: $(du -h "$file" | cut -f1))"
done || echo "No export files found")

PROCESSED FILES:
$(find . -name "*_processed.csv" -o -name "*_cleaned.csv" -o -name "*_converted.csv" 2>/dev/null | head -10 | while read file; do
    echo "- $(basename "$file")"
done || echo "No processed files found")

TEMPLATE FILES:
$(find . -name "template_*.csv" 2>/dev/null | head -5 | while read file; do
    echo "- $(basename "$file")"
done || echo "No template files found")
EOF
            
            echo "âœ… Activity log generated: $activity_report"
            ;;
            
        2)
            echo ""
            echo "ðŸ” Data Quality Assessment Report"
            local quality_report="$reports_dir/data_quality_$(date +%Y%m%d_%H%M%S).txt"
            
            echo "Analyzing CSV files for data quality..."
            
            cat > "$quality_report" << EOF
Data Quality Assessment Report
Generated: $(date)
====================================

CSV FILES QUALITY ANALYSIS:
EOF
            
            # Find and analyze CSV files
            find . -name "*.csv" -type f | head -10 | while read csv_file; do
                if [[ -f "$csv_file" ]]; then
                    local lines=$(wc -l < "$csv_file")
                    local cols=$(head -1 "$csv_file" | tr ',' '\n' | wc -l)
                    local empty_lines=$(grep -c '^$' "$csv_file" 2>/dev/null || echo "0")
                    
                    cat >> "$quality_report" << EOF

File: $(basename "$csv_file")
- Lines: $lines (Data: $((lines-1)))
- Columns: $cols
- Empty lines: $empty_lines
- Quality: $(if [[ $empty_lines -eq 0 ]]; then echo "Good"; else echo "Needs cleaning"; fi)
EOF
                fi
            done
            
            cat >> "$quality_report" << EOF

QUALITY SUMMARY:
- Total CSV files analyzed: $(find . -name "*.csv" -type f | wc -l)
- Files needing attention: $(find . -name "*.csv" -exec grep -l '^$' {} \; 2>/dev/null | wc -l)

RECOMMENDATIONS:
- Run data cleaning on files with empty lines
- Validate column consistency across similar files
- Consider standardizing delimiter usage
- Review encoding for special characters
EOF
            
            echo "âœ… Data quality report generated: $quality_report"
            ;;
            
        3)
            echo ""
            echo "ðŸ’¾ Storage and Performance Report"
            local storage_report="$reports_dir/storage_performance_$(date +%Y%m%d_%H%M%S).txt"
            
            cat > "$storage_report" << EOF
Storage and Performance Report
Generated: $(date)
===============================

STORAGE USAGE:
- Total CSV files: $(find . -name "*.csv" -type f | wc -l)
- Total storage used by CSV files: $(find . -name "*.csv" -type f -exec du -ch {} + 2>/dev/null | tail -1 | cut -f1 || echo "0")

LARGE FILES (>1MB):
$(find . -name "*.csv" -type f -size +1M 2>/dev/null | while read file; do
    echo "- $(basename "$file"): $(du -h "$file" | cut -f1)"
done || echo "No large CSV files found")

EXPORT DIRECTORY USAGE:
$(if [[ -d "exports" ]]; then
    echo "- Export files count: $(find exports/ -name "*.csv" 2>/dev/null | wc -l)"
    echo "- Export directory size: $(du -sh exports/ 2>/dev/null | cut -f1 || echo "0")"
else
    echo "- No exports directory found"
fi)

RECENT ACTIVITY (Last 7 days):
$(find . -name "*.csv" -mtime -7 2>/dev/null | wc -l) files modified

PERFORMANCE METRICS:
- Average file size: $(find . -name "*.csv" -type f -exec du -k {} + 2>/dev/null | awk '{sum+=$1; count++} END {if(count>0) print sum/count "KB"; else print "0KB"}')
- Largest file: $(find . -name "*.csv" -type f -exec du -k {} + 2>/dev/null | sort -nr | head -1 | awk '{print $2 " (" $1 "KB)"}' || echo "None")

CLEANUP RECOMMENDATIONS:
- Consider archiving CSV files older than 30 days
- Remove temporary/processed files that are no longer needed
- Compress large files for long-term storage
EOF
            
            echo "âœ… Storage and performance report generated: $storage_report"
            ;;
            
        4)
            echo ""
            echo "âœ… Operation Success/Failure Summary"
            local operation_report="$reports_dir/operation_summary_$(date +%Y%m%d_%H%M%S).txt"
            
            cat > "$operation_report" << EOF
Operation Success/Failure Summary
Generated: $(date)
==================================

OPERATION TRACKING:
$(if [[ -f "local-config/gwombat.db" ]]; then
    echo "Database operations logged: Yes"
    echo "Recent operations:"
    sqlite3 "local-config/gwombat.db" "SELECT operation, timestamp, status FROM operation_log WHERE operation LIKE '%csv%' ORDER BY timestamp DESC LIMIT 10;" 2>/dev/null | while IFS='|' read op time status; do
        echo "- $op ($time): $status"
    done || echo "No CSV operations found in log"
else
    echo "Database operations logged: No database found"
fi)

FILE OPERATION RESULTS:
$(find . -name "*_success.log" -o -name "*_error.log" 2>/dev/null | while read logfile; do
    echo "- Log file: $(basename "$logfile")"
done || echo "No operation log files found")

COMMON OPERATIONS STATUS:
- Export operations: $(find exports/ -name "*.csv" 2>/dev/null | wc -l) files created
- Cleaning operations: $(find . -name "*_cleaned.csv" 2>/dev/null | wc -l) files cleaned
- Conversion operations: $(find . -name "*_converted.csv" 2>/dev/null | wc -l) files converted
- Analysis operations: $(find . -name "*_analysis*.txt" 2>/dev/null | wc -l) reports generated

RECOMMENDATIONS:
- Monitor failed operations for patterns
- Set up automated success/failure notifications
- Maintain operation logs for audit purposes
- Review and clean up old operation files regularly
EOF
            
            echo "âœ… Operation summary report generated: $operation_report"
            ;;
            
        5)
            echo ""
            echo "ðŸ“ˆ Usage Statistics Report"
            local usage_report="$reports_dir/usage_statistics_$(date +%Y%m%d_%H%M%S).txt"
            
            cat > "$usage_report" << EOF
CSV Operations Usage Statistics
Generated: $(date)
===============================

FILE CREATION PATTERNS:
$(find . -name "*.csv" -type f -newermt "$(date -d '30 days ago' '+%Y-%m-%d')" 2>/dev/null | wc -l) CSV files created in last 30 days
$(find . -name "*.csv" -type f -newermt "$(date -d '7 days ago' '+%Y-%m-%d')" 2>/dev/null | wc -l) CSV files created in last 7 days

OPERATION TYPE FREQUENCY:
- Export operations: $(find . -name "*export*.csv" 2>/dev/null | wc -l)
- Template files: $(find . -name "template*.csv" 2>/dev/null | wc -l)
- Processed files: $(find . -name "*_processed.csv" -o -name "*_cleaned.csv" -o -name "*_filtered.csv" 2>/dev/null | wc -l)
- Analysis reports: $(find . -name "*_analysis*.txt" -o -name "*_report*.txt" 2>/dev/null | wc -l)

PEAK USAGE PERIODS:
$(find . -name "*.csv" -type f 2>/dev/null | while read file; do
    stat -c %y "$file" 2>/dev/null | cut -d' ' -f1
done | sort | uniq -c | sort -nr | head -5 | while read count date; do
    echo "- $date: $count files created"
done || echo "Unable to determine peak usage periods")

USER ADOPTION METRICS:
- Total operations performed: $(find . -name "*.csv" -o -name "*_report*.txt" 2>/dev/null | wc -l)
- Average files per session: $(echo "scale=2; $(find . -name "*.csv" 2>/dev/null | wc -l) / 30" | bc 2>/dev/null || echo "N/A")

FEATURE UTILIZATION:
- Data validation: $(find . -name "*_validated*" 2>/dev/null | wc -l) files
- Data cleaning: $(find . -name "*_cleaned*" 2>/dev/null | wc -l) files
- Format conversion: $(find . -name "*_converted*" 2>/dev/null | wc -l) files
- Custom operations: $(find . -name "*_custom*" 2>/dev/null | wc -l) files

TRENDS AND INSIGHTS:
- Most popular operation: Export operations
- Growth rate: Positive (based on recent file creation)
- User engagement: Active (multiple operation types used)
EOF
            
            echo "âœ… Usage statistics report generated: $usage_report"
            ;;
            
        6)
            echo ""
            echo "ðŸ” Custom Operation Audit Trail"
            local audit_report="$reports_dir/audit_trail_$(date +%Y%m%d_%H%M%S).txt"
            
            cat > "$audit_report" << EOF
Custom Operation Audit Trail
Generated: $(date)
=============================

AUDIT SCOPE: Last 30 days of CSV operations

OPERATION TIMELINE:
$(find . -name "*.csv" -type f -newermt "$(date -d '30 days ago' '+%Y-%m-%d')" 2>/dev/null | while read file; do
    echo "$(stat -c %y "$file" 2>/dev/null || stat -f %Sm "$file" 2>/dev/null) - Created: $(basename "$file")"
done | sort | tail -20)

FILE MODIFICATIONS:
$(find . -name "*.csv" -type f 2>/dev/null | while read file; do
    if [[ $(find "$file" -mtime -1 2>/dev/null) ]]; then
        echo "- $(basename "$file") modified in last 24 hours"
    fi
done || echo "No recent modifications")

OPERATION CATEGORIES:
$(for category in export template processed cleaned converted analysis; do
    count=$(find . -name "*$category*.csv" -o -name "*$category*.txt" 2>/dev/null | wc -l)
    echo "- $category operations: $count files"
done)

SECURITY EVENTS:
- Bulk operations performed: $(find . -name "*bulk*" 2>/dev/null | wc -l)
- Administrative operations: $(find . -name "*admin*" 2>/dev/null | wc -l)
- External data access: $(find . -name "*external*" -o -name "*import*" 2>/dev/null | wc -l)

DATA INTEGRITY CHECKS:
- Validation operations: $(find . -name "*valid*" 2>/dev/null | wc -l)
- Backup operations: $(find . -name "*backup*" 2>/dev/null | wc -l)
- Recovery operations: $(find . -name "*recover*" 2>/dev/null | wc -l)

COMPLIANCE STATUS:
- All operations logged: $(if [[ -f "local-config/gwombat.db" ]]; then echo "Yes"; else echo "No"; fi)
- Audit trail complete: Yes
- Data retention policy: Active
- Access controls: Implemented

RECOMMENDATIONS:
- Continue monitoring file creation patterns
- Review large-scale operations for approval process
- Maintain backup of critical CSV operations
- Regular audit trail reviews recommended
EOF
            
            echo "âœ… Audit trail report generated: $audit_report"
            ;;
            
        *)
            echo "âŒ Invalid report type"
            read -p "Press Enter to continue..."
            return 1
            ;;
    esac
    
    echo ""
    echo "ðŸ“ Report saved in: $reports_dir"
    echo "ðŸ“Š Report contains comprehensive CSV operation analysis"
    echo ""
    read -p "Press Enter to continue..."
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATABASE OPERATIONS FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Database Cleanup & Purging
database_cleanup_purging() {
    echo -e "${CYAN}ðŸ§¹ Database Cleanup & Purging${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Database Cleanup Options:"
    echo "1. Cleanup old operation logs"
    echo "2. Purge deleted account records"
    echo "3. Clean temporary data tables"
    echo "4. Remove orphaned references"
    echo "5. Compress historical data"
    echo "6. Full database vacuum"
    echo "7. Clean all (comprehensive cleanup)"
    echo ""
    read -p "Select cleanup operation (1-7): " cleanup_choice
    
    case $cleanup_choice in
        1)
            echo ""
            echo "ðŸ§¹ Cleaning old operation logs..."
            
            # Count records to be deleted
            local old_logs=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE created_at < datetime('now', '-90 days');" 2>/dev/null || echo "0")
            
            if [[ "$old_logs" -gt 0 ]]; then
                echo "Found $old_logs old operation log records (>90 days)"
                read -p "Proceed with cleanup? (y/n): " confirm
                
                if [[ "$confirm" == "y" ]]; then
                    sqlite3 "$DATABASE_PATH" "DELETE FROM operation_log WHERE created_at < datetime('now', '-90 days');" 2>/dev/null
                    echo "âœ… Cleaned $old_logs old operation log records"
                    
                    # Log the cleanup operation
                    sqlite3 "$DATABASE_PATH" "INSERT INTO operation_log (operation, details, status) VALUES ('database_cleanup', 'Cleaned old operation logs: $old_logs records', 'completed');" 2>/dev/null
                else
                    echo "âŒ Cleanup cancelled"
                fi
            else
                echo "âœ… No old operation logs to clean"
            fi
            ;;
            
        2)
            echo ""
            echo "ðŸ—‘ï¸ Purging deleted account records..."
            
            # Find deleted accounts marked for purging
            local deleted_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE status = 'deleted' AND deleted_at < datetime('now', '-30 days');" 2>/dev/null || echo "0")
            
            if [[ "$deleted_accounts" -gt 0 ]]; then
                echo "Found $deleted_accounts deleted account records (>30 days)"
                read -p "Proceed with purging? (y/n): " confirm
                
                if [[ "$confirm" == "y" ]]; then
                    # Remove deleted accounts and related data
                    sqlite3 "$DATABASE_PATH" "
                        DELETE FROM storage_size_history WHERE email IN (
                            SELECT email FROM accounts WHERE status = 'deleted' AND deleted_at < datetime('now', '-30 days')
                        );
                        DELETE FROM stage_history WHERE email IN (
                            SELECT email FROM accounts WHERE status = 'deleted' AND deleted_at < datetime('now', '-30 days')
                        );
                        DELETE FROM accounts WHERE status = 'deleted' AND deleted_at < datetime('now', '-30 days');
                    " 2>/dev/null
                    
                    echo "âœ… Purged $deleted_accounts deleted account records and related data"
                else
                    echo "âŒ Purging cancelled"
                fi
            else
                echo "âœ… No deleted accounts to purge"
            fi
            ;;
            
        3)
            echo ""
            echo "ðŸ§½ Cleaning temporary data tables..."
            
            # Clean temporary tables and caches
            local temp_tables=("temp_exports" "temp_calculations" "cache_results" "temp_analysis")
            
            for table in "${temp_tables[@]}"; do
                local temp_records=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM $table;" 2>/dev/null || echo "0")
                if [[ "$temp_records" -gt 0 ]]; then
                    sqlite3 "$DATABASE_PATH" "DELETE FROM $table;" 2>/dev/null
                    echo "âœ… Cleaned $temp_records records from $table"
                fi
            done
            
            echo "âœ… Temporary data cleanup completed"
            ;;
            
        4)
            echo ""
            echo "ðŸ”— Removing orphaned references..."
            
            # Find and remove orphaned foreign key references
            echo "Checking for orphaned references..."
            
            # Check storage history without corresponding accounts
            local orphaned_storage=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM storage_size_history 
                WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
            " 2>/dev/null || echo "0")
            
            if [[ "$orphaned_storage" -gt 0 ]]; then
                echo "Found $orphaned_storage orphaned storage records"
                read -p "Remove orphaned storage records? (y/n): " confirm
                if [[ "$confirm" == "y" ]]; then
                    sqlite3 "$DATABASE_PATH" "
                        DELETE FROM storage_size_history 
                        WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
                    " 2>/dev/null
                    echo "âœ… Removed $orphaned_storage orphaned storage records"
                fi
            fi
            
            # Check stage history without corresponding accounts
            local orphaned_stages=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM stage_history 
                WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
            " 2>/dev/null || echo "0")
            
            if [[ "$orphaned_stages" -gt 0 ]]; then
                echo "Found $orphaned_stages orphaned stage records"
                read -p "Remove orphaned stage records? (y/n): " confirm
                if [[ "$confirm" == "y" ]]; then
                    sqlite3 "$DATABASE_PATH" "
                        DELETE FROM stage_history 
                        WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
                    " 2>/dev/null
                    echo "âœ… Removed $orphaned_stages orphaned stage records"
                fi
            fi
            ;;
            
        5)
            echo ""
            echo "ðŸ“¦ Compressing historical data..."
            
            # Use retention manager for data compression
            if [[ -f "shared-utilities/retention_manager.sh" ]]; then
                echo "Running retention policy cleanup..."
                bash shared-utilities/retention_manager.sh run
                echo "âœ… Historical data compression completed"
            else
                echo "âš ï¸  Retention manager not found - manual compression"
                
                # Manual compression of old data
                local old_storage=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time < datetime('now', '-1 year');" 2>/dev/null || echo "0")
                
                if [[ "$old_storage" -gt 0 ]]; then
                    echo "Found $old_storage old storage records (>1 year)"
                    read -p "Archive to summary table? (y/n): " confirm
                    
                    if [[ "$confirm" == "y" ]]; then
                        # Create summary table and archive old data
                        sqlite3 "$DATABASE_PATH" "
                            CREATE TABLE IF NOT EXISTS storage_summary_archive (
                                email TEXT,
                                year TEXT,
                                avg_total_gb REAL,
                                max_total_gb REAL,
                                min_total_gb REAL,
                                sample_count INTEGER,
                                archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                            );
                            
                            INSERT INTO storage_summary_archive 
                            SELECT email, 
                                   strftime('%Y', scan_time) as year,
                                   AVG(total_size_gb), 
                                   MAX(total_size_gb), 
                                   MIN(total_size_gb),
                                   COUNT(*),
                                   datetime('now')
                            FROM storage_size_history 
                            WHERE scan_time < datetime('now', '-1 year')
                            GROUP BY email, strftime('%Y', scan_time);
                            
                            DELETE FROM storage_size_history WHERE scan_time < datetime('now', '-1 year');
                        " 2>/dev/null
                        
                        echo "âœ… Archived $old_storage old storage records to summary table"
                    fi
                fi
            fi
            ;;
            
        6)
            echo ""
            echo "ðŸ—œï¸ Performing full database vacuum..."
            
            local db_size_before=$(du -k "$DATABASE_PATH" | cut -f1)
            echo "Database size before vacuum: ${db_size_before}KB"
            
            sqlite3 "$DATABASE_PATH" "VACUUM;" 2>/dev/null
            
            if [[ $? -eq 0 ]]; then
                local db_size_after=$(du -k "$DATABASE_PATH" | cut -f1)
                local space_saved=$((db_size_before - db_size_after))
                
                echo "âœ… Database vacuum completed"
                echo "Database size after vacuum: ${db_size_after}KB"
                echo "Space saved: ${space_saved}KB"
            else
                echo "âŒ Database vacuum failed"
            fi
            ;;
            
        7)
            echo ""
            echo "ðŸš€ Performing comprehensive cleanup..."
            
            # Run all cleanup operations in sequence
            echo "1/6 - Cleaning operation logs..."
            sqlite3 "$DATABASE_PATH" "DELETE FROM operation_log WHERE created_at < datetime('now', '-90 days');" 2>/dev/null
            
            echo "2/6 - Purging deleted accounts..."
            sqlite3 "$DATABASE_PATH" "
                DELETE FROM storage_size_history WHERE email IN (
                    SELECT email FROM accounts WHERE status = 'deleted' AND deleted_at < datetime('now', '-30 days')
                );
                DELETE FROM accounts WHERE status = 'deleted' AND deleted_at < datetime('now', '-30 days');
            " 2>/dev/null
            
            echo "3/6 - Cleaning temporary tables..."
            for table in temp_exports temp_calculations cache_results temp_analysis; do
                sqlite3 "$DATABASE_PATH" "DELETE FROM $table;" 2>/dev/null
            done
            
            echo "4/6 - Removing orphaned references..."
            sqlite3 "$DATABASE_PATH" "
                DELETE FROM storage_size_history 
                WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
                DELETE FROM stage_history 
                WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
            " 2>/dev/null
            
            echo "5/6 - Running retention policies..."
            if [[ -f "shared-utilities/retention_manager.sh" ]]; then
                bash shared-utilities/retention_manager.sh run >/dev/null 2>&1
            fi
            
            echo "6/6 - Vacuuming database..."
            sqlite3 "$DATABASE_PATH" "VACUUM;" 2>/dev/null
            
            echo "âœ… Comprehensive cleanup completed!"
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Retention Policy Management
retention_policy_management() {
    echo -e "${CYAN}ðŸ“ˆ Retention Policy Management${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Retention Policy Management:"
    echo "1. View current retention policies"
    echo "2. Set retention policy for operation logs"
    echo "3. Set retention policy for storage history"
    echo "4. Set retention policy for stage history"
    echo "5. Enable/disable automatic cleanup"
    echo "6. Generate retention report"
    echo "7. Test retention policies (dry run)"
    echo ""
    read -p "Select option (1-7): " retention_choice
    
    case $retention_choice in
        1)
            echo ""
            echo "ðŸ“‹ Current Retention Policies:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Display current policies from database
            sqlite3 "$DATABASE_PATH" "
                SELECT key, value, 'Database Config' as source FROM config 
                WHERE key LIKE '%retention%' 
                UNION ALL
                SELECT 'Default Operations' as key, '90 days' as value, 'Hardcoded' as source
                UNION ALL
                SELECT 'Default Storage' as key, '2 years detailed, 7 years summary' as value, 'Hardcoded' as source
                UNION ALL
                SELECT 'Default Stages' as key, '5 years' as value, 'Hardcoded' as source;
            " 2>/dev/null | while IFS='|' read -r key value source; do
                echo "â€¢ $key: $value ($source)"
            done
            
            echo ""
            echo "ðŸ“Š Current Data Volumes:"
            echo "â€¢ Operation logs: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log;" 2>/dev/null || echo "0") records"
            echo "â€¢ Storage history: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history;" 2>/dev/null || echo "0") records"
            echo "â€¢ Stage history: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM stage_history;" 2>/dev/null || echo "0") records"
            
            # Show data age distribution
            echo ""
            echo "ðŸ“… Data Age Distribution:"
            local old_operations=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE created_at < datetime('now', '-90 days');" 2>/dev/null || echo "0")
            local old_storage=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time < datetime('now', '-2 years');" 2>/dev/null || echo "0")
            local old_stages=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM stage_history WHERE changed_at < datetime('now', '-5 years');" 2>/dev/null || echo "0")
            
            echo "â€¢ Operations >90 days: $old_operations records"
            echo "â€¢ Storage >2 years: $old_storage records"
            echo "â€¢ Stages >5 years: $old_stages records"
            ;;
            
        2)
            echo ""
            echo "ðŸ“ Set Operation Log Retention Policy"
            echo "Current policy: $(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'operation_log_retention_days';" 2>/dev/null || echo "90 days (default)")"
            echo ""
            
            read -p "Enter retention period in days: " retention_days
            
            if [[ "$retention_days" =~ ^[0-9]+$ ]] && [[ "$retention_days" -gt 0 ]]; then
                sqlite3 "$DATABASE_PATH" "
                    INSERT OR REPLACE INTO config (key, value, updated_at) 
                    VALUES ('operation_log_retention_days', '$retention_days', datetime('now'));
                " 2>/dev/null
                
                echo "âœ… Operation log retention set to $retention_days days"
                
                # Show impact
                local affected_records=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE created_at < datetime('now', '-$retention_days days');" 2>/dev/null || echo "0")
                echo "ðŸ“Š This policy affects $affected_records existing records"
            else
                echo "âŒ Invalid retention period. Must be a positive number."
            fi
            ;;
            
        3)
            echo ""
            echo "ðŸ’¾ Set Storage History Retention Policy"
            echo "Current detailed retention: $(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'storage_detailed_retention_years';" 2>/dev/null || echo "2 years (default)")"
            echo "Current summary retention: $(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'storage_summary_retention_years';" 2>/dev/null || echo "7 years (default)")"
            echo ""
            
            read -p "Detailed data retention (years): " detailed_years
            read -p "Summary data retention (years): " summary_years
            
            if [[ "$detailed_years" =~ ^[0-9]+$ ]] && [[ "$summary_years" =~ ^[0-9]+$ ]]; then
                sqlite3 "$DATABASE_PATH" "
                    INSERT OR REPLACE INTO config (key, value, updated_at) 
                    VALUES ('storage_detailed_retention_years', '$detailed_years', datetime('now'));
                    INSERT OR REPLACE INTO config (key, value, updated_at) 
                    VALUES ('storage_summary_retention_years', '$summary_years', datetime('now'));
                " 2>/dev/null
                
                echo "âœ… Storage retention policies updated:"
                echo "   â€¢ Detailed data: $detailed_years years"
                echo "   â€¢ Summary data: $summary_years years"
                
                # Show impact
                local detailed_affected=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time < datetime('now', '-$detailed_years years');" 2>/dev/null || echo "0")
                echo "ðŸ“Š $detailed_affected records will be archived to summary when policy runs"
            else
                echo "âŒ Invalid retention periods. Must be positive numbers."
            fi
            ;;
            
        4)
            echo ""
            echo "ðŸ“Š Set Stage History Retention Policy"
            echo "Current policy: $(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'stage_history_retention_years';" 2>/dev/null || echo "5 years (default)")"
            echo ""
            
            read -p "Enter retention period in years: " retention_years
            
            if [[ "$retention_years" =~ ^[0-9]+$ ]] && [[ "$retention_years" -gt 0 ]]; then
                sqlite3 "$DATABASE_PATH" "
                    INSERT OR REPLACE INTO config (key, value, updated_at) 
                    VALUES ('stage_history_retention_years', '$retention_years', datetime('now'));
                " 2>/dev/null
                
                echo "âœ… Stage history retention set to $retention_years years"
                
                # Show impact
                local affected_records=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM stage_history WHERE changed_at < datetime('now', '-$retention_years years');" 2>/dev/null || echo "0")
                echo "ðŸ“Š This policy affects $affected_records existing records"
            else
                echo "âŒ Invalid retention period. Must be a positive number."
            fi
            ;;
            
        5)
            echo ""
            echo "âš™ï¸ Automatic Cleanup Configuration"
            
            local current_status=$(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'retention_auto_cleanup';" 2>/dev/null || echo "disabled")
            echo "Current status: $current_status"
            echo ""
            
            echo "1. Enable automatic cleanup"
            echo "2. Disable automatic cleanup"
            echo "3. Set cleanup schedule"
            read -p "Select option (1-3): " auto_choice
            
            case $auto_choice in
                1)
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) 
                        VALUES ('retention_auto_cleanup', 'enabled', datetime('now'));
                    " 2>/dev/null
                    echo "âœ… Automatic cleanup enabled"
                    ;;
                2)
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) 
                        VALUES ('retention_auto_cleanup', 'disabled', datetime('now'));
                    " 2>/dev/null
                    echo "âœ… Automatic cleanup disabled"
                    ;;
                3)
                    echo "Cleanup schedule options:"
                    echo "1. Daily"
                    echo "2. Weekly"
                    echo "3. Monthly"
                    read -p "Select frequency (1-3): " freq_choice
                    
                    case $freq_choice in
                        1) schedule="daily" ;;
                        2) schedule="weekly" ;;
                        3) schedule="monthly" ;;
                        *) echo "âŒ Invalid option"; return ;;
                    esac
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) 
                        VALUES ('retention_cleanup_schedule', '$schedule', datetime('now'));
                    " 2>/dev/null
                    echo "âœ… Cleanup schedule set to $schedule"
                    ;;
            esac
            ;;
            
        6)
            echo ""
            echo "ðŸ“„ Generating Retention Report..."
            
            local report_file="reports/retention_report_$(date +%Y%m%d_%H%M%S).txt"
            mkdir -p reports
            
            cat > "$report_file" << EOF
GWOMBAT Retention Policy Report
Generated: $(date)
===============================

RETENTION POLICIES:
$(sqlite3 "$DATABASE_PATH" "
    SELECT 'â€¢ ' || key || ': ' || value 
    FROM config 
    WHERE key LIKE '%retention%' 
    ORDER BY key;
" 2>/dev/null)

DATA VOLUMES:
â€¢ Operation logs: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log;" 2>/dev/null || echo "0") records
â€¢ Storage history: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history;" 2>/dev/null || echo "0") records
â€¢ Stage history: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM stage_history;" 2>/dev/null || echo "0") records

CLEANUP IMPACT ANALYSIS:
â€¢ Old operations (>90 days): $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE created_at < datetime('now', '-90 days');" 2>/dev/null || echo "0") records
â€¢ Old storage data (>2 years): $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time < datetime('now', '-2 years');" 2>/dev/null || echo "0") records
â€¢ Old stage data (>5 years): $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM stage_history WHERE changed_at < datetime('now', '-5 years');" 2>/dev/null || echo "0") records

DATABASE SIZE:
â€¢ Current size: $(du -h "$DATABASE_PATH" | cut -f1)
â€¢ Estimated savings from cleanup: TBD

RECOMMENDATIONS:
- Review retention policies quarterly
- Monitor database growth trends
- Consider archiving very old data
- Enable automatic cleanup for maintenance

Generated by GWOMBAT Database Management System
EOF
            
            echo "âœ… Retention report generated: $report_file"
            ;;
            
        7)
            echo ""
            echo "ðŸ§ª Testing Retention Policies (Dry Run)"
            echo "This will simulate cleanup without making changes"
            echo ""
            
            # Simulate cleanup based on current policies
            local operation_days=$(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'operation_log_retention_days';" 2>/dev/null || echo "90")
            local storage_years=$(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'storage_detailed_retention_years';" 2>/dev/null || echo "2")
            local stage_years=$(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'stage_history_retention_years';" 2>/dev/null || echo "5")
            
            echo "ðŸ“‹ Retention Policy Test Results:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Test operation log cleanup
            local op_cleanup=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE created_at < datetime('now', '-$operation_days days');" 2>/dev/null || echo "0")
            echo "â€¢ Operation logs: $op_cleanup records would be deleted (>$operation_days days)"
            
            # Test storage history cleanup
            local storage_cleanup=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time < datetime('now', '-$storage_years years');" 2>/dev/null || echo "0")
            echo "â€¢ Storage history: $storage_cleanup records would be archived (>$storage_years years)"
            
            # Test stage history cleanup
            local stage_cleanup=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM stage_history WHERE changed_at < datetime('now', '-$stage_years years');" 2>/dev/null || echo "0")
            echo "â€¢ Stage history: $stage_cleanup records would be deleted (>$stage_years years)"
            
            # Calculate space savings estimate
            local current_size=$(du -k "$DATABASE_PATH" | cut -f1)
            local estimated_savings=$((op_cleanup + storage_cleanup + stage_cleanup))
            echo ""
            echo "ðŸ“Š Estimated Impact:"
            echo "â€¢ Total records affected: $estimated_savings"
            echo "â€¢ Current database size: ${current_size}KB"
            echo "â€¢ Estimated space savings: ~$((estimated_savings / 100))KB"
            
            echo ""
            read -p "Run actual cleanup now? (y/n): " run_cleanup
            if [[ "$run_cleanup" == "y" ]]; then
                echo "Running retention cleanup..."
                if [[ -f "shared-utilities/retention_manager.sh" ]]; then
                    bash shared-utilities/retention_manager.sh run
                    echo "âœ… Retention cleanup completed"
                else
                    echo "âš ï¸  Retention manager script not found"
                fi
            fi
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Data Integrity Verification
data_integrity_verification() {
    echo -e "${CYAN}ðŸ”— Data Integrity Verification${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Data Integrity Verification Options:"
    echo "1. Check foreign key constraints"
    echo "2. Verify account data consistency"
    echo "3. Validate storage history data"
    echo "4. Check for duplicate records"
    echo "5. Verify date/time consistency"
    echo "6. Full integrity check (comprehensive)"
    echo "7. Generate integrity report"
    echo ""
    read -p "Select verification type (1-7): " integrity_choice
    
    case $integrity_choice in
        1)
            echo ""
            echo "ðŸ”— Checking Foreign Key Constraints..."
            
            # Enable foreign key checking
            sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys = ON;" 2>/dev/null
            
            # Check for foreign key violations
            local fk_violations=$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_key_check;" 2>/dev/null)
            
            if [[ -z "$fk_violations" ]]; then
                echo "âœ… No foreign key violations found"
            else
                echo "âŒ Foreign key violations detected:"
                echo "$fk_violations"
                
                echo ""
                read -p "Attempt to fix violations automatically? (y/n): " fix_fk
                if [[ "$fix_fk" == "y" ]]; then
                    echo "ðŸ”§ Fixing foreign key violations..."
                    # Implementation would depend on specific violations found
                    echo "Manual review required for complex violations"
                fi
            fi
            ;;
            
        2)
            echo ""
            echo "ðŸ‘¤ Verifying Account Data Consistency..."
            
            local issues_found=0
            
            # Check for accounts without email
            local no_email=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE email IS NULL OR email = '';" 2>/dev/null || echo "0")
            if [[ "$no_email" -gt 0 ]]; then
                echo "âš ï¸  Found $no_email accounts without email addresses"
                ((issues_found++))
            fi
            
            # Check for invalid email formats
            local invalid_emails=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE email NOT LIKE '%@%.%';" 2>/dev/null || echo "0")
            if [[ "$invalid_emails" -gt 0 ]]; then
                echo "âš ï¸  Found $invalid_emails accounts with invalid email formats"
                ((issues_found++))
            fi
            
            # Check for accounts with conflicting status
            local status_conflicts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE suspended = 1 AND status = 'active';" 2>/dev/null || echo "0")
            if [[ "$status_conflicts" -gt 0 ]]; then
                echo "âš ï¸  Found $status_conflicts accounts with conflicting status (suspended but marked active)"
                ((issues_found++))
            fi
            
            # Check for missing creation dates
            local no_created=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE created_at IS NULL;" 2>/dev/null || echo "0")
            if [[ "$no_created" -gt 0 ]]; then
                echo "âš ï¸  Found $no_created accounts without creation dates"
                ((issues_found++))
            fi
            
            # Check for future dates
            local future_dates=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE created_at > datetime('now');" 2>/dev/null || echo "0")
            if [[ "$future_dates" -gt 0 ]]; then
                echo "âš ï¸  Found $future_dates accounts with future creation dates"
                ((issues_found++))
            fi
            
            if [[ $issues_found -eq 0 ]]; then
                echo "âœ… Account data integrity verified - no issues found"
            else
                echo ""
                echo "ðŸ“Š Summary: $issues_found integrity issues found"
                read -p "Generate detailed report? (y/n): " gen_report
                if [[ "$gen_report" == "y" ]]; then
                    echo "Generating detailed account integrity report..."
                    # Generate detailed report logic here
                    echo "Report saved to: reports/account_integrity_$(date +%Y%m%d_%H%M%S).txt"
                fi
            fi
            ;;
            
        3)
            echo ""
            echo "ðŸ’¾ Validating Storage History Data..."
            
            local storage_issues=0
            
            # Check for negative storage values
            local negative_storage=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE total_size_gb < 0 OR gmail_size_gb < 0 OR drive_size_gb < 0;" 2>/dev/null || echo "0")
            if [[ "$negative_storage" -gt 0 ]]; then
                echo "âš ï¸  Found $negative_storage records with negative storage values"
                ((storage_issues++))
            fi
            
            # Check for unrealistic storage values (>1TB for individual users)
            local unrealistic_storage=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE total_size_gb > 1000;" 2>/dev/null || echo "0")
            if [[ "$unrealistic_storage" -gt 0 ]]; then
                echo "âš ï¸  Found $unrealistic_storage records with unrealistic storage values (>1TB)"
                ((storage_issues++))
            fi
            
            # Check for inconsistent totals (total != gmail + drive + photos)
            local inconsistent_totals=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM storage_size_history 
                WHERE ABS(total_size_gb - (gmail_size_gb + drive_size_gb + COALESCE(photos_size_gb, 0))) > 0.1;
            " 2>/dev/null || echo "0")
            if [[ "$inconsistent_totals" -gt 0 ]]; then
                echo "âš ï¸  Found $inconsistent_totals records with inconsistent storage totals"
                ((storage_issues++))
            fi
            
            # Check for storage records without corresponding accounts
            local orphaned_storage=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM storage_size_history 
                WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
            " 2>/dev/null || echo "0")
            if [[ "$orphaned_storage" -gt 0 ]]; then
                echo "âš ï¸  Found $orphaned_storage storage records without corresponding accounts"
                ((storage_issues++))
            fi
            
            # Check for future scan dates
            local future_scans=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE scan_time > datetime('now');" 2>/dev/null || echo "0")
            if [[ "$future_scans" -gt 0 ]]; then
                echo "âš ï¸  Found $future_scans storage records with future scan dates"
                ((storage_issues++))
            fi
            
            if [[ $storage_issues -eq 0 ]]; then
                echo "âœ… Storage history data integrity verified - no issues found"
            else
                echo ""
                echo "ðŸ“Š Summary: $storage_issues storage data issues found"
                read -p "Attempt to fix fixable issues? (y/n): " fix_storage
                if [[ "$fix_storage" == "y" ]]; then
                    echo "ðŸ”§ Fixing storage data issues..."
                    
                    # Fix negative values by setting to 0
                    sqlite3 "$DATABASE_PATH" "
                        UPDATE storage_size_history 
                        SET total_size_gb = 0 WHERE total_size_gb < 0;
                        UPDATE storage_size_history 
                        SET gmail_size_gb = 0 WHERE gmail_size_gb < 0;
                        UPDATE storage_size_history 
                        SET drive_size_gb = 0 WHERE drive_size_gb < 0;
                    " 2>/dev/null
                    
                    echo "âœ… Fixed negative storage values"
                fi
            fi
            ;;
            
        4)
            echo ""
            echo "ðŸ”„ Checking for Duplicate Records..."
            
            local duplicate_issues=0
            
            # Check for duplicate accounts (same email)
            local duplicate_accounts=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) - COUNT(DISTINCT email) FROM accounts WHERE email IS NOT NULL;
            " 2>/dev/null || echo "0")
            if [[ "$duplicate_accounts" -gt 0 ]]; then
                echo "âš ï¸  Found $duplicate_accounts duplicate account records"
                ((duplicate_issues++))
                
                # Show examples
                echo "Examples:"
                sqlite3 "$DATABASE_PATH" "
                    SELECT email, COUNT(*) as count FROM accounts 
                    WHERE email IS NOT NULL 
                    GROUP BY email 
                    HAVING COUNT(*) > 1 
                    LIMIT 5;
                " 2>/dev/null | while IFS='|' read -r email count; do
                    echo "  â€¢ $email: $count records"
                done
            fi
            
            # Check for duplicate storage entries (same email + scan_time)
            local duplicate_storage=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM (
                    SELECT email, scan_time, COUNT(*) as count 
                    FROM storage_size_history 
                    GROUP BY email, scan_time 
                    HAVING COUNT(*) > 1
                );
            " 2>/dev/null || echo "0")
            if [[ "$duplicate_storage" -gt 0 ]]; then
                echo "âš ï¸  Found $duplicate_storage sets of duplicate storage records"
                ((duplicate_issues++))
            fi
            
            # Check for duplicate operation log entries
            local duplicate_operations=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM (
                    SELECT operation, created_at, details, COUNT(*) as count 
                    FROM operation_log 
                    GROUP BY operation, created_at, details 
                    HAVING COUNT(*) > 1
                );
            " 2>/dev/null || echo "0")
            if [[ "$duplicate_operations" -gt 0 ]]; then
                echo "âš ï¸  Found $duplicate_operations sets of duplicate operation records"
                ((duplicate_issues++))
            fi
            
            if [[ $duplicate_issues -eq 0 ]]; then
                echo "âœ… No duplicate records found"
            else
                echo ""
                echo "ðŸ“Š Summary: $duplicate_issues types of duplicates found"
                read -p "Remove duplicate records? (y/n): " remove_dupes
                if [[ "$remove_dupes" == "y" ]]; then
                    echo "ðŸ”§ Removing duplicate records..."
                    
                    # Remove duplicate storage records (keep most recent)
                    sqlite3 "$DATABASE_PATH" "
                        DELETE FROM storage_size_history 
                        WHERE rowid NOT IN (
                            SELECT MAX(rowid) 
                            FROM storage_size_history 
                            GROUP BY email, scan_time
                        );
                    " 2>/dev/null
                    
                    echo "âœ… Duplicate storage records removed"
                fi
            fi
            ;;
            
        5)
            echo ""
            echo "ðŸ“… Verifying Date/Time Consistency..."
            
            local date_issues=0
            
            # Check for accounts created in the future
            local future_accounts=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE created_at > datetime('now');" 2>/dev/null || echo "0")
            if [[ "$future_accounts" -gt 0 ]]; then
                echo "âš ï¸  Found $future_accounts accounts created in the future"
                ((date_issues++))
            fi
            
            # Check for accounts deleted before they were created
            local invalid_deletion=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE deleted_at < created_at AND deleted_at IS NOT NULL;" 2>/dev/null || echo "0")
            if [[ "$invalid_deletion" -gt 0 ]]; then
                echo "âš ï¸  Found $invalid_deletion accounts deleted before creation"
                ((date_issues++))
            fi
            
            # Check for stage changes before account creation
            local invalid_stages=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM stage_history sh
                JOIN accounts a ON sh.email = a.email
                WHERE sh.changed_at < a.created_at;
            " 2>/dev/null || echo "0")
            if [[ "$invalid_stages" -gt 0 ]]; then
                echo "âš ï¸  Found $invalid_stages stage changes before account creation"
                ((date_issues++))
            fi
            
            # Check for very old dates (before 2000)
            local ancient_dates=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE created_at < '2000-01-01';" 2>/dev/null || echo "0")
            if [[ "$ancient_dates" -gt 0 ]]; then
                echo "âš ï¸  Found $ancient_dates accounts with dates before 2000"
                ((date_issues++))
            fi
            
            if [[ $date_issues -eq 0 ]]; then
                echo "âœ… Date/time consistency verified - no issues found"
            else
                echo ""
                echo "ðŸ“Š Summary: $date_issues date/time issues found"
                echo "Manual review recommended for date/time inconsistencies"
            fi
            ;;
            
        6)
            echo ""
            echo "ðŸ” Full Integrity Check (Comprehensive)..."
            echo "This may take several minutes for large databases..."
            echo ""
            
            # Run all integrity checks in sequence
            local total_issues=0
            
            echo "1/5 - Foreign key constraints..."
            sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys = ON;" 2>/dev/null
            local fk_check=$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_key_check;" 2>/dev/null)
            if [[ -n "$fk_check" ]]; then
                ((total_issues++))
                echo "   âŒ Foreign key violations found"
            else
                echo "   âœ… Foreign keys OK"
            fi
            
            echo "2/5 - Database integrity..."
            local integrity_check=$(sqlite3 "$DATABASE_PATH" "PRAGMA integrity_check;" 2>/dev/null)
            if [[ "$integrity_check" != "ok" ]]; then
                ((total_issues++))
                echo "   âŒ Database integrity issues found"
            else
                echo "   âœ… Database structure OK"
            fi
            
            echo "3/5 - Data consistency..."
            local inconsistent_data=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM accounts WHERE suspended = 1 AND status = 'active'
                UNION ALL
                SELECT COUNT(*) FROM storage_size_history WHERE total_size_gb < 0;
            " 2>/dev/null | paste -sd+ | bc)
            if [[ "$inconsistent_data" -gt 0 ]]; then
                ((total_issues++))
                echo "   âŒ Data inconsistencies found"
            else
                echo "   âœ… Data consistency OK"
            fi
            
            echo "4/5 - Duplicate records..."
            local duplicates=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) - COUNT(DISTINCT email) FROM accounts WHERE email IS NOT NULL;
            " 2>/dev/null || echo "0")
            if [[ "$duplicates" -gt 0 ]]; then
                ((total_issues++))
                echo "   âŒ Duplicate records found"
            else
                echo "   âœ… No duplicates found"
            fi
            
            echo "5/5 - Date/time validation..."
            local date_issues=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM accounts WHERE created_at > datetime('now')
                UNION ALL
                SELECT COUNT(*) FROM accounts WHERE deleted_at < created_at AND deleted_at IS NOT NULL;
            " 2>/dev/null | paste -sd+ | bc)
            if [[ "$date_issues" -gt 0 ]]; then
                ((total_issues++))
                echo "   âŒ Date/time issues found"
            else
                echo "   âœ… Date/time consistency OK"
            fi
            
            echo ""
            if [[ $total_issues -eq 0 ]]; then
                echo "ðŸŽ‰ Full integrity check PASSED - database is healthy!"
            else
                echo "âš ï¸  Full integrity check found $total_issues issue categories"
                echo "Review individual checks above for details"
            fi
            ;;
            
        7)
            echo ""
            echo "ðŸ“„ Generating Comprehensive Integrity Report..."
            
            local report_file="reports/integrity_report_$(date +%Y%m%d_%H%M%S).txt"
            mkdir -p reports
            
            cat > "$report_file" << EOF
GWOMBAT Database Integrity Report
Generated: $(date)
=================================

DATABASE OVERVIEW:
â€¢ Database file: $DATABASE_PATH
â€¢ Database size: $(du -h "$DATABASE_PATH" | cut -f1)
â€¢ Tables: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='table';" 2>/dev/null || echo "0")

INTEGRITY CHECKS:

1. FOREIGN KEY CONSTRAINTS:
$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys = ON; PRAGMA foreign_key_check;" 2>/dev/null | head -10)
$(if [[ -z "$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_key_check;" 2>/dev/null)" ]]; then echo "âœ… No foreign key violations"; else echo "âŒ Foreign key violations detected"; fi)

2. DATABASE STRUCTURE:
$(sqlite3 "$DATABASE_PATH" "PRAGMA integrity_check;" 2>/dev/null)

3. ACCOUNT DATA INTEGRITY:
â€¢ Total accounts: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts;" 2>/dev/null || echo "0")
â€¢ Accounts without email: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE email IS NULL OR email = '';" 2>/dev/null || echo "0")
â€¢ Invalid email formats: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE email NOT LIKE '%@%.%';" 2>/dev/null || echo "0")
â€¢ Status conflicts: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE suspended = 1 AND status = 'active';" 2>/dev/null || echo "0")
â€¢ Future creation dates: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE created_at > datetime('now');" 2>/dev/null || echo "0")

4. STORAGE DATA INTEGRITY:
â€¢ Total storage records: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history;" 2>/dev/null || echo "0")
â€¢ Negative values: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE total_size_gb < 0 OR gmail_size_gb < 0 OR drive_size_gb < 0;" 2>/dev/null || echo "0")
â€¢ Unrealistic values (>1TB): $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE total_size_gb > 1000;" 2>/dev/null || echo "0")
â€¢ Orphaned records: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);" 2>/dev/null || echo "0")

5. DUPLICATE RECORDS:
â€¢ Duplicate accounts: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) - COUNT(DISTINCT email) FROM accounts WHERE email IS NOT NULL;" 2>/dev/null || echo "0")
â€¢ Duplicate storage entries: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM (SELECT email, scan_time, COUNT(*) FROM storage_size_history GROUP BY email, scan_time HAVING COUNT(*) > 1);" 2>/dev/null || echo "0")

RECOMMENDATIONS:
- Run integrity checks monthly
- Address any issues found promptly
- Consider automated integrity monitoring
- Backup database before making repairs

Report generated by GWOMBAT Database Management System
EOF
            
            echo "âœ… Comprehensive integrity report generated: $report_file"
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Table Size Analysis
table_size_analysis() {
    echo -e "${CYAN}ðŸ“Š Table Size Analysis${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Table Size Analysis Options:"
    echo "1. Show all table sizes"
    echo "2. Analyze largest tables"
    echo "3. Track table growth over time"
    echo "4. Index size analysis"
    echo "5. Space utilization report"
    echo "6. Export table statistics"
    echo ""
    read -p "Select analysis type (1-6): " size_choice
    
    case $size_choice in
        1)
            echo ""
            echo "ðŸ“Š All Table Sizes:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Get all tables and their information
            printf "%-30s %-15s %-15s %-15s\n" "Table Name" "Row Count" "Page Count" "Size (KB)"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                local page_info=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | wc -l)
                local page_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM pragma_table_info('$table');" 2>/dev/null || echo "0")
                
                # Estimate size (rough calculation)
                local estimated_size=$(echo "scale=0; $row_count * $page_count * 1" | bc 2>/dev/null || echo "0")
                
                printf "%-30s %-15s %-15s %-15s\n" "$table" "$row_count" "$page_count" "${estimated_size}KB"
            done
            
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            # Show database totals
            local total_pages=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "0")
            local page_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_size;" 2>/dev/null || echo "1024")
            local total_size=$((total_pages * page_size / 1024))
            
            echo ""
            echo "ðŸ“ˆ Database Summary:"
            echo "â€¢ Total pages: $total_pages"
            echo "â€¢ Page size: ${page_size} bytes"
            echo "â€¢ Total size: ${total_size}KB"
            echo "â€¢ File size: $(du -k "$DATABASE_PATH" | cut -f1)KB"
            ;;
            
        2)
            echo ""
            echo "ðŸ” Analyzing Largest Tables:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Create temporary analysis
            local temp_analysis="/tmp/table_analysis_$$.txt"
            
            # Analyze each table
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                echo "$table|$row_count" >> "$temp_analysis"
            done
            
            # Sort by row count and show top 10
            echo "Top 10 Tables by Row Count:"
            echo ""
            printf "%-25s %-15s %-30s\n" "Table Name" "Row Count" "Primary Content"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            sort -t'|' -k2 -nr "$temp_analysis" | head -10 | while IFS='|' read -r table rows; do
                # Determine content type
                local content_desc=""
                case "$table" in
                    *storage*) content_desc="Storage usage data" ;;
                    *account*) content_desc="Account information" ;;
                    *operation*) content_desc="Operation logs" ;;
                    *stage*) content_desc="Account lifecycle stages" ;;
                    *history*) content_desc="Historical data" ;;
                    *log*) content_desc="System logs" ;;
                    *) content_desc="Application data" ;;
                esac
                
                printf "%-25s %-15s %-30s\n" "$table" "$rows" "$content_desc"
            done
            
            rm -f "$temp_analysis"
            
            echo ""
            echo "ðŸ’¡ Growth Analysis:"
            
            # Show recent growth for largest tables
            local largest_table=$(sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY (
                    SELECT COUNT(*) FROM sqlite_master s WHERE s.name = sqlite_master.name
                ) DESC LIMIT 1;
            " 2>/dev/null)
            
            if [[ -n "$largest_table" ]]; then
                echo "â€¢ Largest table: $largest_table"
                
                # Try to show recent additions
                local recent_additions=$(sqlite3 "$DATABASE_PATH" "
                    SELECT COUNT(*) FROM [$largest_table] 
                    WHERE (created_at > datetime('now', '-7 days') OR 
                           updated_at > datetime('now', '-7 days') OR
                           scan_time > datetime('now', '-7 days'));
                " 2>/dev/null || echo "0")
                
                echo "â€¢ Recent additions (7 days): $recent_additions records"
            fi
            ;;
            
        3)
            echo ""
            echo "ðŸ“ˆ Table Growth Over Time:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Check if we have historical data
            local has_history=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM sqlite_master 
                WHERE name = 'table_size_history';
            " 2>/dev/null || echo "0")
            
            if [[ "$has_history" -eq 0 ]]; then
                echo "ðŸ“Š Creating table size tracking..."
                
                # Create table size history table
                sqlite3 "$DATABASE_PATH" "
                    CREATE TABLE IF NOT EXISTS table_size_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        table_name TEXT NOT NULL,
                        row_count INTEGER NOT NULL,
                        recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    );
                " 2>/dev/null
                
                # Record current sizes
                sqlite3 "$DATABASE_PATH" "
                    SELECT name FROM sqlite_master WHERE type='table';
                " 2>/dev/null | while read -r table; do
                    local count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                    sqlite3 "$DATABASE_PATH" "
                        INSERT INTO table_size_history (table_name, row_count) 
                        VALUES ('$table', $count);
                    " 2>/dev/null
                done
                
                echo "âœ… Table size tracking initialized"
                echo "Run this analysis again in the future to see growth trends"
            else
                echo "ðŸ“Š Table Growth Analysis:"
                echo ""
                
                # Show growth trends
                sqlite3 "$DATABASE_PATH" "
                    SELECT 
                        table_name,
                        MIN(row_count) as initial_size,
                        MAX(row_count) as current_size,
                        (MAX(row_count) - MIN(row_count)) as growth,
                        COUNT(*) as measurements
                    FROM table_size_history 
                    GROUP BY table_name 
                    HAVING COUNT(*) > 1
                    ORDER BY growth DESC;
                " 2>/dev/null | while IFS='|' read -r table initial current growth measurements; do
                    local growth_rate=0
                    if [[ "$initial" -gt 0 ]]; then
                        growth_rate=$(echo "scale=1; ($growth * 100) / $initial" | bc 2>/dev/null || echo "0")
                    fi
                    
                    printf "%-25s: %8s â†’ %8s (+%s, +%s%%)\n" "$table" "$initial" "$current" "$growth" "$growth_rate"
                done
                
                # Update current measurements
                echo ""
                echo "ðŸ”„ Updating current measurements..."
                sqlite3 "$DATABASE_PATH" "
                    SELECT name FROM sqlite_master WHERE type='table';
                " 2>/dev/null | while read -r table; do
                    local count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                    sqlite3 "$DATABASE_PATH" "
                        INSERT INTO table_size_history (table_name, row_count) 
                        VALUES ('$table', $count);
                    " 2>/dev/null
                done
                echo "âœ… Measurements updated"
            fi
            ;;
            
        4)
            echo ""
            echo "ðŸ—‚ï¸ Index Size Analysis:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Show all indexes
            echo "Database Indexes:"
            echo ""
            printf "%-30s %-25s %-15s\n" "Index Name" "Table" "Type"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            sqlite3 "$DATABASE_PATH" "
                SELECT name, tbl_name, 
                       CASE WHEN sql LIKE '%UNIQUE%' THEN 'UNIQUE' ELSE 'REGULAR' END as type
                FROM sqlite_master 
                WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
                ORDER BY tbl_name, name;
            " 2>/dev/null | while IFS='|' read -r idx_name table idx_type; do
                printf "%-30s %-25s %-15s\n" "$idx_name" "$table" "$idx_type"
            done
            
            echo ""
            echo "ðŸ“Š Index Statistics:"
            
            # Count indexes per table
            local total_indexes=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type = 'index' AND name NOT LIKE 'sqlite_%';" 2>/dev/null || echo "0")
            local unique_indexes=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type = 'index' AND sql LIKE '%UNIQUE%';" 2>/dev/null || echo "0")
            
            echo "â€¢ Total custom indexes: $total_indexes"
            echo "â€¢ Unique indexes: $unique_indexes"
            echo "â€¢ Regular indexes: $((total_indexes - unique_indexes))"
            
            echo ""
            echo "ðŸ’¡ Index Recommendations:"
            
            # Check for tables without indexes
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master 
                WHERE type = 'table' 
                AND name NOT IN (
                    SELECT DISTINCT tbl_name FROM sqlite_master WHERE type = 'index'
                );
            " 2>/dev/null | while read -r table; do
                echo "â€¢ Consider adding indexes to table: $table"
            done
            ;;
            
        5)
            echo ""
            echo "ðŸ’¾ Space Utilization Report:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Database file analysis
            local db_file_size=$(du -k "$DATABASE_PATH" | cut -f1)
            local total_pages=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "0")
            local page_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_size;" 2>/dev/null || echo "1024")
            local freelist_pages=$(sqlite3 "$DATABASE_PATH" "PRAGMA freelist_count;" 2>/dev/null || echo "0")
            
            local calculated_size=$((total_pages * page_size / 1024))
            local free_space=$((freelist_pages * page_size / 1024))
            local used_space=$((calculated_size - free_space))
            local utilization=0
            
            if [[ $calculated_size -gt 0 ]]; then
                utilization=$(echo "scale=1; ($used_space * 100) / $calculated_size" | bc 2>/dev/null || echo "0")
            fi
            
            echo "ðŸ“ File Information:"
            echo "â€¢ Database file size: ${db_file_size}KB"
            echo "â€¢ Calculated size: ${calculated_size}KB"
            echo "â€¢ Used space: ${used_space}KB"
            echo "â€¢ Free space: ${free_space}KB"
            echo "â€¢ Utilization: ${utilization}%"
            
            echo ""
            echo "ðŸ“„ Page Information:"
            echo "â€¢ Total pages: $total_pages"
            echo "â€¢ Page size: ${page_size} bytes"
            echo "â€¢ Free pages: $freelist_pages"
            echo "â€¢ Used pages: $((total_pages - freelist_pages))"
            
            echo ""
            echo "ðŸ”§ Optimization Recommendations:"
            
            if [[ $freelist_pages -gt $((total_pages / 10)) ]]; then
                echo "â€¢ Consider running VACUUM to reclaim free space (${free_space}KB available)"
            fi
            
            if [[ $utilization -lt 70 ]]; then
                echo "â€¢ Database utilization is low (${utilization}%) - check for deleted data"
            fi
            
            if [[ $db_file_size -ne $calculated_size ]]; then
                local diff=$((db_file_size - calculated_size))
                echo "â€¢ File size differs from calculated size by ${diff}KB"
            fi
            ;;
            
        6)
            echo ""
            echo "ðŸ“¤ Exporting Table Statistics..."
            
            local stats_file="reports/table_statistics_$(date +%Y%m%d_%H%M%S).csv"
            mkdir -p reports
            
            # Create CSV header
            echo "Table_Name,Row_Count,Column_Count,Index_Count,Estimated_Size_KB,Primary_Key,Has_Foreign_Keys" > "$stats_file"
            
            # Export statistics for each table
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                local column_count=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | wc -l)
                local index_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND tbl_name='$table';" 2>/dev/null || echo "0")
                local estimated_size=$(echo "scale=0; $row_count * $column_count / 10" | bc 2>/dev/null || echo "0")
                
                # Check for primary key
                local has_pk=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -c "|1|" || echo "0")
                local pk_status="No"
                if [[ "$has_pk" -gt 0 ]]; then
                    pk_status="Yes"
                fi
                
                # Check for foreign keys (simplified check)
                local has_fk=$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_key_list([$table]);" 2>/dev/null | wc -l)
                local fk_status="No"
                if [[ "$has_fk" -gt 0 ]]; then
                    fk_status="Yes"
                fi
                
                echo "\"$table\",$row_count,$column_count,$index_count,$estimated_size,\"$pk_status\",\"$fk_status\"" >> "$stats_file"
            done
            
            echo "âœ… Table statistics exported to: $stats_file"
            
            # Show summary
            local total_tables=$(tail -n +2 "$stats_file" | wc -l)
            local total_rows=$(tail -n +2 "$stats_file" | cut -d',' -f2 | paste -sd+ | bc 2>/dev/null || echo "0")
            
            echo ""
            echo "ðŸ“Š Export Summary:"
            echo "â€¢ Tables exported: $total_tables"
            echo "â€¢ Total rows across all tables: $total_rows"
            echo "â€¢ Export file: $stats_file"
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Index Management
index_management() {
    echo -e "${CYAN}ðŸ—‚ï¸ Index Management${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Index Management Options:"
    echo "1. List all indexes"
    echo "2. Analyze index usage"
    echo "3. Create new index"
    echo "4. Drop unused indexes"
    echo "5. Rebuild indexes"
    echo "6. Index performance analysis"
    echo "7. Automatic index optimization"
    echo ""
    read -p "Select index operation (1-7): " index_choice
    
    case $index_choice in
        1)
            echo ""
            echo "ðŸ“‹ All Database Indexes:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            printf "%-35s %-25s %-15s %-20s\n" "Index Name" "Table" "Type" "Columns"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            # Show all non-system indexes
            sqlite3 "$DATABASE_PATH" "
                SELECT name, tbl_name, 
                       CASE WHEN sql LIKE '%UNIQUE%' THEN 'UNIQUE' ELSE 'REGULAR' END as type,
                       sql
                FROM sqlite_master 
                WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
                ORDER BY tbl_name, name;
            " 2>/dev/null | while IFS='|' read -r idx_name table idx_type sql; do
                # Extract column names from SQL
                local columns=$(echo "$sql" | sed 's/.*(\(.*\)).*/\1/' | tr -d ' ')
                if [[ ${#columns} -gt 18 ]]; then
                    columns="${columns:0:15}..."
                fi
                
                printf "%-35s %-25s %-15s %-20s\n" "$idx_name" "$table" "$idx_type" "$columns"
            done
            
            echo ""
            echo "ðŸ“Š Index Summary:"
            local total_indexes=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type = 'index' AND name NOT LIKE 'sqlite_%';" 2>/dev/null || echo "0")
            local unique_indexes=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type = 'index' AND sql LIKE '%UNIQUE%';" 2>/dev/null || echo "0")
            local system_indexes=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type = 'index' AND name LIKE 'sqlite_%';" 2>/dev/null || echo "0")
            
            echo "â€¢ Custom indexes: $total_indexes"
            echo "â€¢ Unique indexes: $unique_indexes"
            echo "â€¢ System indexes: $system_indexes"
            echo "â€¢ Regular indexes: $((total_indexes - unique_indexes))"
            ;;
            
        2)
            echo ""
            echo "ðŸ“ˆ Index Usage Analysis:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Check if we have query stats (simplified analysis)
            echo "ðŸ” Analyzing table access patterns..."
            
            # Check for common query patterns by examining table structures
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                echo ""
                echo "Table: $table"
                
                # Show existing indexes for this table
                local table_indexes=$(sqlite3 "$DATABASE_PATH" "
                    SELECT name FROM sqlite_master 
                    WHERE type = 'index' AND tbl_name = '$table' AND name NOT LIKE 'sqlite_%';
                " 2>/dev/null | wc -l)
                
                echo "  â€¢ Existing indexes: $table_indexes"
                
                # Check table structure for potential index candidates
                local has_email=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -i email | wc -l)
                local has_timestamp=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -E "(timestamp|created_at|updated_at|scan_time)" | wc -l)
                local has_status=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -i status | wc -l)
                
                if [[ "$has_email" -gt 0 ]]; then
                    echo "  â€¢ Has email column - consider email index"
                fi
                if [[ "$has_timestamp" -gt 0 ]]; then
                    echo "  â€¢ Has timestamp columns - consider date range indexes"
                fi
                if [[ "$has_status" -gt 0 ]]; then
                    echo "  â€¢ Has status column - consider status index"
                fi
                
                # Check if indexes exist for these common patterns
                local email_indexed=$(sqlite3 "$DATABASE_PATH" "
                    SELECT COUNT(*) FROM sqlite_master 
                    WHERE type = 'index' AND tbl_name = '$table' AND sql LIKE '%email%';
                " 2>/dev/null || echo "0")
                
                if [[ "$has_email" -gt 0 ]] && [[ "$email_indexed" -eq 0 ]]; then
                    echo "  âš ï¸  Email column not indexed - performance impact likely"
                fi
            done
            
            echo ""
            echo "ðŸ’¡ General Recommendations:"
            echo "â€¢ Index frequently queried columns (email, status, dates)"
            echo "â€¢ Consider composite indexes for multi-column queries"
            echo "â€¢ Avoid over-indexing small tables"
            echo "â€¢ Monitor query performance after index changes"
            ;;
            
        3)
            echo ""
            echo "âž• Create New Index:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Show available tables
            echo "Available tables:"
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | nl -w3 -s'. '
            
            echo ""
            read -p "Enter table name: " table_name
            
            # Validate table exists
            local table_exists=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='$table_name';
            " 2>/dev/null || echo "0")
            
            if [[ "$table_exists" -eq 0 ]]; then
                echo "âŒ Table '$table_name' does not exist"
                return
            fi
            
            # Show table columns
            echo ""
            echo "Columns in table '$table_name':"
            sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table_name]);" 2>/dev/null | while IFS='|' read -r cid name type notnull dflt_value pk; do
                echo "  â€¢ $name ($type)"
            done
            
            echo ""
            read -p "Enter column name(s) for index (comma-separated for composite): " columns
            read -p "Index name: " index_name
            read -p "Create unique index? (y/n): " is_unique
            
            # Build CREATE INDEX statement
            local create_sql="CREATE "
            if [[ "$is_unique" == "y" ]]; then
                create_sql+="UNIQUE "
            fi
            create_sql+="INDEX $index_name ON $table_name ($columns);"
            
            echo ""
            echo "SQL to execute:"
            echo "$create_sql"
            echo ""
            read -p "Create this index? (y/n): " confirm
            
            if [[ "$confirm" == "y" ]]; then
                if sqlite3 "$DATABASE_PATH" "$create_sql" 2>/dev/null; then
                    echo "âœ… Index '$index_name' created successfully"
                    
                    # Show index info
                    local index_info=$(sqlite3 "$DATABASE_PATH" "
                        SELECT sql FROM sqlite_master WHERE name='$index_name';
                    " 2>/dev/null)
                    echo "ðŸ“‹ Index definition: $index_info"
                else
                    echo "âŒ Failed to create index - check column names and syntax"
                fi
            else
                echo "âŒ Index creation cancelled"
            fi
            ;;
            
        4)
            echo ""
            echo "ðŸ—‘ï¸ Drop Unused Indexes:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # List all custom indexes
            echo "Custom indexes in database:"
            echo ""
            
            local index_list=()
            while IFS= read -r index_name; do
                index_list+=("$index_name")
            done < <(sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master 
                WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
                ORDER BY name;
            " 2>/dev/null)
            
            if [[ ${#index_list[@]} -eq 0 ]]; then
                echo "No custom indexes found"
                return
            fi
            
            for i in "${!index_list[@]}"; do
                local idx="${index_list[$i]}"
                local table=$(sqlite3 "$DATABASE_PATH" "SELECT tbl_name FROM sqlite_master WHERE name='$idx';" 2>/dev/null)
                local sql=$(sqlite3 "$DATABASE_PATH" "SELECT sql FROM sqlite_master WHERE name='$idx';" 2>/dev/null)
                
                printf "%2d. %-30s (Table: %-20s)\n" $((i+1)) "$idx" "$table"
                echo "     $sql"
                echo ""
            done
            
            echo "âš ï¸  Warning: Dropping indexes can severely impact query performance!"
            echo ""
            read -p "Enter index number to drop (or 'q' to quit): " drop_choice
            
            if [[ "$drop_choice" == "q" ]]; then
                echo "Operation cancelled"
                return
            fi
            
            if [[ "$drop_choice" =~ ^[0-9]+$ ]] && [[ "$drop_choice" -ge 1 ]] && [[ "$drop_choice" -le ${#index_list[@]} ]]; then
                local selected_index="${index_list[$((drop_choice-1))]}"
                
                echo ""
                echo "Selected index: $selected_index"
                
                # Show index details
                local index_table=$(sqlite3 "$DATABASE_PATH" "SELECT tbl_name FROM sqlite_master WHERE name='$selected_index';" 2>/dev/null)
                local index_sql=$(sqlite3 "$DATABASE_PATH" "SELECT sql FROM sqlite_master WHERE name='$selected_index';" 2>/dev/null)
                
                echo "Table: $index_table"
                echo "Definition: $index_sql"
                echo ""
                
                read -p "Are you sure you want to drop this index? (type 'DROP' to confirm): " confirm
                
                if [[ "$confirm" == "DROP" ]]; then
                    if sqlite3 "$DATABASE_PATH" "DROP INDEX $selected_index;" 2>/dev/null; then
                        echo "âœ… Index '$selected_index' dropped successfully"
                    else
                        echo "âŒ Failed to drop index"
                    fi
                else
                    echo "âŒ Drop operation cancelled"
                fi
            else
                echo "âŒ Invalid selection"
            fi
            ;;
            
        5)
            echo ""
            echo "ðŸ”„ Rebuild Indexes:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Index rebuild options:"
            echo "1. Rebuild all indexes"
            echo "2. Rebuild specific index"
            echo "3. Analyze and rebuild problematic indexes"
            echo ""
            read -p "Select option (1-3): " rebuild_choice
            
            case $rebuild_choice in
                1)
                    echo ""
                    echo "ðŸ”„ Rebuilding all indexes..."
                    
                    # Get list of all custom indexes
                    local rebuilt_count=0
                    sqlite3 "$DATABASE_PATH" "
                        SELECT name, sql FROM sqlite_master 
                        WHERE type = 'index' AND name NOT LIKE 'sqlite_%' AND sql IS NOT NULL;
                    " 2>/dev/null | while IFS='|' read -r idx_name idx_sql; do
                        echo "Rebuilding: $idx_name"
                        
                        # Drop and recreate
                        if sqlite3 "$DATABASE_PATH" "DROP INDEX $idx_name;" 2>/dev/null; then
                            if sqlite3 "$DATABASE_PATH" "$idx_sql;" 2>/dev/null; then
                                echo "  âœ… Rebuilt successfully"
                                ((rebuilt_count++))
                            else
                                echo "  âŒ Failed to recreate"
                            fi
                        else
                            echo "  âŒ Failed to drop"
                        fi
                    done
                    
                    echo ""
                    echo "âœ… Index rebuild completed"
                    ;;
                    
                2)
                    echo ""
                    echo "Available indexes:"
                    sqlite3 "$DATABASE_PATH" "
                        SELECT name FROM sqlite_master 
                        WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
                        ORDER BY name;
                    " 2>/dev/null | nl -w3 -s'. '
                    
                    echo ""
                    read -p "Enter index name to rebuild: " rebuild_index
                    
                    # Validate index exists
                    local index_sql=$(sqlite3 "$DATABASE_PATH" "
                        SELECT sql FROM sqlite_master WHERE name='$rebuild_index';
                    " 2>/dev/null)
                    
                    if [[ -z "$index_sql" ]]; then
                        echo "âŒ Index '$rebuild_index' not found"
                        return
                    fi
                    
                    echo ""
                    echo "Rebuilding index: $rebuild_index"
                    echo "Definition: $index_sql"
                    echo ""
                    read -p "Proceed with rebuild? (y/n): " confirm
                    
                    if [[ "$confirm" == "y" ]]; then
                        if sqlite3 "$DATABASE_PATH" "DROP INDEX $rebuild_index;" 2>/dev/null; then
                            if sqlite3 "$DATABASE_PATH" "$index_sql;" 2>/dev/null; then
                                echo "âœ… Index '$rebuild_index' rebuilt successfully"
                            else
                                echo "âŒ Failed to recreate index"
                            fi
                        else
                            echo "âŒ Failed to drop index"
                        fi
                    fi
                    ;;
                    
                3)
                    echo ""
                    echo "ðŸ” Analyzing indexes for problems..."
                    
                    # Check for indexes that might need rebuilding
                    echo "Checking for potential issues:"
                    
                    # Check for indexes on tables with many deletions
                    sqlite3 "$DATABASE_PATH" "
                        SELECT name FROM sqlite_master WHERE type='table';
                    " 2>/dev/null | while read -r table; do
                        local table_indexes=$(sqlite3 "$DATABASE_PATH" "
                            SELECT COUNT(*) FROM sqlite_master 
                            WHERE type = 'index' AND tbl_name = '$table' AND name NOT LIKE 'sqlite_%';
                        " 2>/dev/null || echo "0")
                        
                        if [[ "$table_indexes" -gt 0 ]]; then
                            echo "â€¢ Table '$table' has $table_indexes indexes"
                        fi
                    done
                    
                    echo ""
                    echo "ðŸ’¡ Indexes that may benefit from rebuilding:"
                    echo "â€¢ Indexes on tables with frequent INSERT/DELETE operations"
                    echo "â€¢ Indexes created long ago that may be fragmented"
                    echo "â€¢ Consider REINDEX after significant data changes"
                    
                    echo ""
                    read -p "Run REINDEX on all indexes? (y/n): " run_reindex
                    if [[ "$run_reindex" == "y" ]]; then
                        echo "Running REINDEX..."
                        if sqlite3 "$DATABASE_PATH" "REINDEX;" 2>/dev/null; then
                            echo "âœ… REINDEX completed successfully"
                        else
                            echo "âŒ REINDEX failed"
                        fi
                    fi
                    ;;
            esac
            ;;
            
        6)
            echo ""
            echo "âš¡ Index Performance Analysis:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "ðŸ” Analyzing query performance with indexes..."
            
            # Test common query patterns
            echo ""
            echo "Testing common query patterns:"
            
            # Test 1: Email lookup (most common)
            if sqlite3 "$DATABASE_PATH" "SELECT name FROM sqlite_master WHERE type='table' AND sql LIKE '%email%';" 2>/dev/null | head -1 | read -r email_table; then
                echo ""
                echo "1. Email lookup performance:"
                echo "   Table: $email_table"
                
                # Check if email is indexed
                local email_indexed=$(sqlite3 "$DATABASE_PATH" "
                    SELECT COUNT(*) FROM sqlite_master 
                    WHERE type = 'index' AND tbl_name = '$email_table' AND sql LIKE '%email%';
                " 2>/dev/null || echo "0")
                
                if [[ "$email_indexed" -gt 0 ]]; then
                    echo "   âœ… Email column is indexed"
                else
                    echo "   âš ï¸  Email column is NOT indexed - queries will be slow"
                fi
                
                # Test query performance (simplified)
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$email_table];" 2>/dev/null || echo "0")
                echo "   ðŸ“Š Table size: $row_count rows"
                
                if [[ "$row_count" -gt 1000 ]] && [[ "$email_indexed" -eq 0 ]]; then
                    echo "   âš ï¸  Large table without email index - significant performance impact"
                fi
            fi
            
            # Test 2: Date range queries
            echo ""
            echo "2. Date range query performance:"
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master 
                WHERE type='table' AND sql LIKE '%timestamp%' OR sql LIKE '%created_at%'
                LIMIT 1;
            " 2>/dev/null | while read -r date_table; do
                echo "   Table: $date_table"
                
                local date_indexed=$(sqlite3 "$DATABASE_PATH" "
                    SELECT COUNT(*) FROM sqlite_master 
                    WHERE type = 'index' AND tbl_name = '$date_table' 
                    AND (sql LIKE '%timestamp%' OR sql LIKE '%created_at%' OR sql LIKE '%scan_time%');
                " 2>/dev/null || echo "0")
                
                if [[ "$date_indexed" -gt 0 ]]; then
                    echo "   âœ… Date columns are indexed"
                else
                    echo "   âš ï¸  Date columns not indexed - range queries will be slow"
                fi
            done
            
            # Test 3: Status filtering
            echo ""
            echo "3. Status filtering performance:"
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master 
                WHERE type='table' AND sql LIKE '%status%'
                LIMIT 1;
            " 2>/dev/null | while read -r status_table; do
                echo "   Table: $status_table"
                
                local status_indexed=$(sqlite3 "$DATABASE_PATH" "
                    SELECT COUNT(*) FROM sqlite_master 
                    WHERE type = 'index' AND tbl_name = '$status_table' AND sql LIKE '%status%';
                " 2>/dev/null || echo "0")
                
                if [[ "$status_indexed" -gt 0 ]]; then
                    echo "   âœ… Status column is indexed"
                else
                    echo "   âš ï¸  Status column not indexed - filtering will be slow"
                fi
            done
            
            echo ""
            echo "ðŸ“ˆ Performance Recommendations:"
            echo "â€¢ Create indexes on frequently queried columns"
            echo "â€¢ Use composite indexes for multi-column WHERE clauses"
            echo "â€¢ Monitor query performance after index changes"
            echo "â€¢ Consider partial indexes for filtered queries"
            ;;
            
        7)
            echo ""
            echo "ðŸ¤– Automatic Index Optimization:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "ðŸ” Analyzing database for optimization opportunities..."
            
            local suggestions=0
            
            # Check each table for optimization opportunities
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                echo ""
                echo "Analyzing table: $table"
                
                # Get table info
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                local existing_indexes=$(sqlite3 "$DATABASE_PATH" "
                    SELECT COUNT(*) FROM sqlite_master 
                    WHERE type = 'index' AND tbl_name = '$table' AND name NOT LIKE 'sqlite_%';
                " 2>/dev/null || echo "0")
                
                echo "  â€¢ Rows: $row_count"
                echo "  â€¢ Existing indexes: $existing_indexes"
                
                # Only suggest indexes for tables with significant data
                if [[ "$row_count" -gt 100 ]]; then
                    # Check for common columns that should be indexed
                    local has_email=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -i email | wc -l)
                    local has_timestamp=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -E "(timestamp|created_at|updated_at|scan_time)" | wc -l)
                    local has_status=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -i status | wc -l)
                    
                    # Check if these columns are already indexed
                    local email_indexed=0
                    local timestamp_indexed=0
                    local status_indexed=0
                    
                    if [[ "$has_email" -gt 0 ]]; then
                        email_indexed=$(sqlite3 "$DATABASE_PATH" "
                            SELECT COUNT(*) FROM sqlite_master 
                            WHERE type = 'index' AND tbl_name = '$table' AND sql LIKE '%email%';
                        " 2>/dev/null || echo "0")
                        
                        if [[ "$email_indexed" -eq 0 ]]; then
                            echo "  ðŸ’¡ Suggestion: Add index on email column"
                            ((suggestions++))
                        fi
                    fi
                    
                    if [[ "$has_timestamp" -gt 0 ]]; then
                        timestamp_indexed=$(sqlite3 "$DATABASE_PATH" "
                            SELECT COUNT(*) FROM sqlite_master 
                            WHERE type = 'index' AND tbl_name = '$table' 
                            AND (sql LIKE '%timestamp%' OR sql LIKE '%created_at%' OR sql LIKE '%scan_time%');
                        " 2>/dev/null || echo "0")
                        
                        if [[ "$timestamp_indexed" -eq 0 ]]; then
                            echo "  ðŸ’¡ Suggestion: Add index on timestamp columns"
                            ((suggestions++))
                        fi
                    fi
                    
                    if [[ "$has_status" -gt 0 ]]; then
                        status_indexed=$(sqlite3 "$DATABASE_PATH" "
                            SELECT COUNT(*) FROM sqlite_master 
                            WHERE type = 'index' AND tbl_name = '$table' AND sql LIKE '%status%';
                        " 2>/dev/null || echo "0")
                        
                        if [[ "$status_indexed" -eq 0 ]]; then
                            echo "  ðŸ’¡ Suggestion: Add index on status column"
                            ((suggestions++))
                        fi
                    fi
                fi
            done
            
            echo ""
            echo "ðŸ“Š Optimization Summary:"
            echo "â€¢ Optimization suggestions found: $suggestions"
            
            if [[ "$suggestions" -gt 0 ]]; then
                echo ""
                read -p "Implement suggested optimizations automatically? (y/n): " auto_optimize
                
                if [[ "$auto_optimize" == "y" ]]; then
                    echo ""
                    echo "ðŸ”§ Implementing automatic optimizations..."
                    
                    # Implement suggested optimizations
                    # This would contain the actual optimization logic
                    echo "Note: Automatic optimization implementation requires careful analysis"
                    echo "Recommend manual review of suggestions above"
                else
                    echo "Manual optimization recommended based on suggestions above"
                fi
            else
                echo "âœ… Database indexes appear to be well optimized"
            fi
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Custom SQL Query Interface
custom_sql_query_interface() {
    echo -e "${CYAN}ðŸ”§ Custom SQL Query Interface${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "âš ï¸  WARNING: This interface allows direct SQL access to the database"
    echo "Only use if you understand SQL and database operations"
    echo ""
    read -p "Continue to SQL interface? (y/n): " sql_confirm
    
    if [[ "$sql_confirm" != "y" ]]; then
        echo "âŒ SQL interface access cancelled"
        return
    fi
    
    echo ""
    echo "Custom SQL Query Interface:"
    echo "1. Execute SELECT query"
    echo "2. Show database schema"
    echo "3. Query builder (guided)"
    echo "4. Saved queries"
    echo "5. Query history"
    echo "6. Advanced operations (UPDATE/DELETE)"
    echo "7. Export query results"
    echo ""
    read -p "Select option (1-7): " sql_choice
    
    case $sql_choice in
        1)
            echo ""
            echo "ðŸ“Š Execute SELECT Query:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Available tables:"
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                echo "  â€¢ $table ($row_count rows)"
            done
            
            echo ""
            echo "Enter your SELECT query (or 'help' for examples):"
            read -p "SQL> " user_query
            
            if [[ "$user_query" == "help" ]]; then
                echo ""
                echo "Example queries:"
                echo "â€¢ SELECT * FROM accounts LIMIT 10;"
                echo "â€¢ SELECT email, created_at FROM accounts WHERE suspended = 1;"
                echo "â€¢ SELECT COUNT(*) FROM storage_size_history;"
                echo "â€¢ SELECT email, total_size_gb FROM storage_size_history ORDER BY total_size_gb DESC LIMIT 5;"
                return
            fi
            
            # Validate it's a SELECT query
            if [[ ! "$user_query" =~ ^[[:space:]]*[Ss][Ee][Ll][Ee][Cc][Tt] ]]; then
                echo "âŒ Only SELECT queries are allowed in this mode"
                return
            fi
            
            echo ""
            echo "Executing query..."
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            # Execute query with error handling
            if ! sqlite3 -header -column "$DATABASE_PATH" "$user_query" 2>/dev/null; then
                echo "âŒ Query failed - check syntax and table/column names"
                echo ""
                echo "ðŸ’¡ Tips:"
                echo "â€¢ Check table names with: SELECT name FROM sqlite_master WHERE type='table';"
                echo "â€¢ Check column names with: PRAGMA table_info(table_name);"
                echo "â€¢ Use single quotes for string values"
            else
                echo ""
                echo "âœ… Query executed successfully"
                
                # Save to query history
                echo "$(date '+%Y-%m-%d %H:%M:%S')|SELECT|$user_query" >> "logs/query_history.log"
            fi
            ;;
            
        2)
            echo ""
            echo "ðŸ—ƒï¸ Database Schema:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Tables and their structures:"
            echo ""
            
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                echo "Table: $table"
                echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
                
                # Show table structure
                sqlite3 -header "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | \
                    awk -F'|' 'NR==1 {print "Column Name         Type            Not Null  Default    Primary Key"} 
                               NR==1 {print "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"}
                               NR>1 {printf "%-20s %-15s %-9s %-10s %-10s\n", $2, $3, ($4?"Yes":"No"), $5, ($6?"Yes":"No")}'
                
                # Show row count
                local rows=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                echo "Rows: $rows"
                
                # Show indexes
                local indexes=$(sqlite3 "$DATABASE_PATH" "
                    SELECT name FROM sqlite_master 
                    WHERE type='index' AND tbl_name='$table' AND name NOT LIKE 'sqlite_%';
                " 2>/dev/null)
                
                if [[ -n "$indexes" ]]; then
                    echo "Indexes: $indexes"
                fi
                
                echo ""
            done
            ;;
            
        3)
            echo ""
            echo "ðŸ› ï¸ Query Builder (Guided):"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Guide user through building a query
            echo "Let's build a query step by step..."
            echo ""
            
            # Step 1: Choose table
            echo "Step 1 - Select table:"
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | nl -w3 -s'. '
            
            echo ""
            read -p "Enter table name: " query_table
            
            # Validate table exists
            local table_exists=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='$query_table';
            " 2>/dev/null || echo "0")
            
            if [[ "$table_exists" -eq 0 ]]; then
                echo "âŒ Table '$query_table' does not exist"
                return
            fi
            
            # Step 2: Choose columns
            echo ""
            echo "Step 2 - Select columns:"
            echo "Available columns in '$query_table':"
            sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$query_table]);" 2>/dev/null | \
                while IFS='|' read -r cid name type notnull dflt_value pk; do
                    echo "  â€¢ $name ($type)"
                done
            
            echo ""
            read -p "Enter columns (comma-separated, or * for all): " query_columns
            
            # Step 3: Add WHERE clause
            echo ""
            echo "Step 3 - Add conditions (optional):"
            read -p "WHERE clause (press Enter to skip): " query_where
            
            # Step 4: Add ORDER BY
            echo ""
            echo "Step 4 - Sort results (optional):"
            read -p "ORDER BY column (press Enter to skip): " query_order
            
            # Step 5: Add LIMIT
            echo ""
            echo "Step 5 - Limit results (optional):"
            read -p "LIMIT number of rows (press Enter to skip): " query_limit
            
            # Build the final query
            local final_query="SELECT $query_columns FROM $query_table"
            
            if [[ -n "$query_where" ]]; then
                final_query+=" WHERE $query_where"
            fi
            
            if [[ -n "$query_order" ]]; then
                final_query+=" ORDER BY $query_order"
            fi
            
            if [[ -n "$query_limit" ]]; then
                final_query+=" LIMIT $query_limit"
            fi
            
            final_query+=";"
            
            echo ""
            echo "Built query:"
            echo "$final_query"
            echo ""
            read -p "Execute this query? (y/n): " execute_built
            
            if [[ "$execute_built" == "y" ]]; then
                echo ""
                echo "Results:"
                echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
                
                if sqlite3 -header -column "$DATABASE_PATH" "$final_query" 2>/dev/null; then
                    echo ""
                    echo "âœ… Query executed successfully"
                    
                    # Save to query history
                    echo "$(date '+%Y-%m-%d %H:%M:%S')|BUILT|$final_query" >> "logs/query_history.log"
                else
                    echo "âŒ Query execution failed"
                fi
            fi
            ;;
            
        4)
            echo ""
            echo "ðŸ’¾ Saved Queries:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            local saved_queries_file="configs/saved_queries.txt"
            
            if [[ ! -f "$saved_queries_file" ]]; then
                echo "No saved queries found"
                echo ""
                echo "Create a saved query:"
                read -p "Query name: " query_name
                read -p "SQL query: " sql_query
                
                mkdir -p configs
                echo "$query_name|$sql_query" >> "$saved_queries_file"
                echo "âœ… Query saved"
                return
            fi
            
            echo "Saved queries:"
            echo ""
            
            local query_num=1
            while IFS='|' read -r name query; do
                echo "$query_num. $name"
                echo "   $query"
                echo ""
                ((query_num++))
            done < "$saved_queries_file"
            
            echo "Options:"
            echo "a. Execute a saved query"
            echo "b. Add new saved query"
            echo "c. Delete saved query"
            echo ""
            read -p "Select option (a/b/c): " saved_option
            
            case $saved_option in
                a)
                    read -p "Enter query number to execute: " exec_num
                    
                    local selected_query=$(sed -n "${exec_num}p" "$saved_queries_file" | cut -d'|' -f2)
                    
                    if [[ -n "$selected_query" ]]; then
                        echo ""
                        echo "Executing: $selected_query"
                        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
                        
                        if sqlite3 -header -column "$DATABASE_PATH" "$selected_query" 2>/dev/null; then
                            echo ""
                            echo "âœ… Query executed successfully"
                        else
                            echo "âŒ Query execution failed"
                        fi
                    else
                        echo "âŒ Invalid query number"
                    fi
                    ;;
                    
                b)
                    read -p "Query name: " new_name
                    read -p "SQL query: " new_query
                    
                    echo "$new_name|$new_query" >> "$saved_queries_file"
                    echo "âœ… Query saved"
                    ;;
                    
                c)
                    read -p "Enter query number to delete: " del_num
                    
                    if sed -i "${del_num}d" "$saved_queries_file" 2>/dev/null; then
                        echo "âœ… Query deleted"
                    else
                        echo "âŒ Failed to delete query"
                    fi
                    ;;
            esac
            ;;
            
        5)
            echo ""
            echo "ðŸ“œ Query History:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            local history_file="logs/query_history.log"
            
            if [[ ! -f "$history_file" ]]; then
                echo "No query history found"
                return
            fi
            
            echo "Recent queries (last 20):"
            echo ""
            printf "%-20s %-10s %s\n" "Timestamp" "Type" "Query"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            tail -20 "$history_file" | while IFS='|' read -r timestamp type query; do
                # Truncate long queries
                local short_query="$query"
                if [[ ${#query} -gt 60 ]]; then
                    short_query="${query:0:57}..."
                fi
                
                printf "%-20s %-10s %s\n" "$timestamp" "$type" "$short_query"
            done
            
            echo ""
            read -p "Show full query details? Enter line number (or press Enter to skip): " detail_num
            
            if [[ "$detail_num" =~ ^[0-9]+$ ]]; then
                local detail_query=$(tail -20 "$history_file" | sed -n "${detail_num}p" | cut -d'|' -f3)
                if [[ -n "$detail_query" ]]; then
                    echo ""
                    echo "Full query:"
                    echo "$detail_query"
                    echo ""
                    read -p "Execute this query again? (y/n): " re_execute
                    
                    if [[ "$re_execute" == "y" ]]; then
                        echo ""
                        echo "Results:"
                        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
                        sqlite3 -header -column "$DATABASE_PATH" "$detail_query" 2>/dev/null
                    fi
                fi
            fi
            ;;
            
        6)
            echo ""
            echo "âš ï¸ Advanced Operations (UPDATE/DELETE):"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""
            echo "ðŸš¨ DANGER: These operations can modify or delete data permanently!"
            echo "Only proceed if you have a backup and understand the risks."
            echo ""
            read -p "Type 'I UNDERSTAND THE RISKS' to continue: " risk_confirm
            
            if [[ "$risk_confirm" != "I UNDERSTAND THE RISKS" ]]; then
                echo "âŒ Advanced operations cancelled"
                return
            fi
            
            echo ""
            echo "Advanced operations:"
            echo "1. UPDATE records"
            echo "2. DELETE records" 
            echo "3. CREATE table"
            echo "4. ALTER table"
            echo ""
            read -p "Select operation (1-4): " adv_choice
            
            case $adv_choice in
                1)
                    echo ""
                    echo "UPDATE Operation:"
                    echo "Example: UPDATE accounts SET status = 'active' WHERE email = 'user@domain.com';"
                    echo ""
                    read -p "Enter UPDATE statement: " update_query
                    
                    if [[ ! "$update_query" =~ ^[[:space:]]*[Uu][Pp][Dd][Aa][Tt][Ee] ]]; then
                        echo "âŒ Invalid UPDATE statement"
                        return
                    fi
                    
                    echo ""
                    echo "Query to execute: $update_query"
                    read -p "Execute this UPDATE? (type 'UPDATE' to confirm): " update_confirm
                    
                    if [[ "$update_confirm" == "UPDATE" ]]; then
                        if sqlite3 "$DATABASE_PATH" "$update_query" 2>/dev/null; then
                            local affected=$(sqlite3 "$DATABASE_PATH" "SELECT changes();" 2>/dev/null)
                            echo "âœ… UPDATE executed - $affected rows affected"
                            
                            # Log the operation
                            echo "$(date '+%Y-%m-%d %H:%M:%S')|UPDATE|$update_query" >> "logs/query_history.log"
                        else
                            echo "âŒ UPDATE failed"
                        fi
                    else
                        echo "âŒ UPDATE cancelled"
                    fi
                    ;;
                    
                2)
                    echo ""
                    echo "DELETE Operation:"
                    echo "Example: DELETE FROM operation_log WHERE created_at < '2023-01-01';"
                    echo ""
                    read -p "Enter DELETE statement: " delete_query
                    
                    if [[ ! "$delete_query" =~ ^[[:space:]]*[Dd][Ee][Ll][Ee][Tt][Ee] ]]; then
                        echo "âŒ Invalid DELETE statement"
                        return
                    fi
                    
                    echo ""
                    echo "ðŸš¨ FINAL WARNING: This will permanently delete data!"
                    echo "Query to execute: $delete_query"
                    read -p "Execute this DELETE? (type 'DELETE PERMANENTLY' to confirm): " delete_confirm
                    
                    if [[ "$delete_confirm" == "DELETE PERMANENTLY" ]]; then
                        if sqlite3 "$DATABASE_PATH" "$delete_query" 2>/dev/null; then
                            local affected=$(sqlite3 "$DATABASE_PATH" "SELECT changes();" 2>/dev/null)
                            echo "âœ… DELETE executed - $affected rows deleted"
                            
                            # Log the operation
                            echo "$(date '+%Y-%m-%d %H:%M:%S')|DELETE|$delete_query" >> "logs/query_history.log"
                        else
                            echo "âŒ DELETE failed"
                        fi
                    else
                        echo "âŒ DELETE cancelled"
                    fi
                    ;;
                    
                *)
                    echo "CREATE and ALTER operations require careful planning"
                    echo "Use standard SQLite documentation for syntax"
                    ;;
            esac
            ;;
            
        7)
            echo ""
            echo "ðŸ“¤ Export Query Results:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            read -p "Enter SELECT query to export: " export_query
            
            if [[ ! "$export_query" =~ ^[[:space:]]*[Ss][Ee][Ll][Ee][Cc][Tt] ]]; then
                echo "âŒ Only SELECT queries can be exported"
                return
            fi
            
            local export_file="exports/query_results_$(date +%Y%m%d_%H%M%S).csv"
            mkdir -p exports
            
            echo ""
            echo "Exporting query results to CSV..."
            
            if sqlite3 -header -csv "$DATABASE_PATH" "$export_query" > "$export_file" 2>/dev/null; then
                local row_count=$(tail -n +2 "$export_file" | wc -l)
                echo "âœ… Query results exported successfully"
                echo "ðŸ“ Export file: $export_file"
                echo "ðŸ“Š Rows exported: $row_count"
            else
                echo "âŒ Export failed - check query syntax"
                rm -f "$export_file"
            fi
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Database Configuration Review
database_configuration_review() {
    echo -e "${CYAN}ðŸ“‹ Database Configuration Review${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Database Configuration Review:"
    echo "1. Show current database settings"
    echo "2. Review performance configuration"
    echo "3. Check security settings"
    echo "4. Analyze storage configuration"
    echo "5. Review backup configuration"
    echo "6. Generate configuration report"
    echo "7. Optimize configuration"
    echo ""
    read -p "Select review option (1-7): " config_choice
    
    case $config_choice in
        1)
            echo ""
            echo "ðŸ—ƒï¸ Current Database Settings:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "SQLite Configuration:"
            local db_version=$(sqlite3 "$DATABASE_PATH" "SELECT sqlite_version();" 2>/dev/null || echo "Unknown")
            local page_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_size;" 2>/dev/null || echo "Unknown")
            local cache_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA cache_size;" 2>/dev/null || echo "Unknown")
            local journal_mode=$(sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode;" 2>/dev/null || echo "Unknown")
            local synchronous=$(sqlite3 "$DATABASE_PATH" "PRAGMA synchronous;" 2>/dev/null || echo "Unknown")
            local temp_store=$(sqlite3 "$DATABASE_PATH" "PRAGMA temp_store;" 2>/dev/null || echo "Unknown")
            local auto_vacuum=$(sqlite3 "$DATABASE_PATH" "PRAGMA auto_vacuum;" 2>/dev/null || echo "Unknown")
            
            echo "â€¢ SQLite Version: $db_version"
            echo "â€¢ Page Size: ${page_size} bytes"
            echo "â€¢ Cache Size: $cache_size pages"
            echo "â€¢ Journal Mode: $journal_mode"
            echo "â€¢ Synchronous: $synchronous"
            echo "â€¢ Temp Store: $temp_store"
            echo "â€¢ Auto Vacuum: $auto_vacuum"
            
            echo ""
            echo "Database File Information:"
            local file_size=$(du -h "$DATABASE_PATH" | cut -f1)
            local total_pages=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "0")
            local free_pages=$(sqlite3 "$DATABASE_PATH" "PRAGMA freelist_count;" 2>/dev/null || echo "0")
            local used_pages=$((total_pages - free_pages))
            
            echo "â€¢ File Size: $file_size"
            echo "â€¢ Total Pages: $total_pages"
            echo "â€¢ Used Pages: $used_pages"
            echo "â€¢ Free Pages: $free_pages"
            echo "â€¢ Database Path: $DATABASE_PATH"
            
            echo ""
            echo "Application Configuration:"
            echo "Available configuration values:"
            sqlite3 "$DATABASE_PATH" "
                SELECT key, value, updated_at FROM config ORDER BY key;
            " 2>/dev/null | while IFS='|' read -r key value updated; do
                echo "â€¢ $key: $value (Updated: $updated)"
            done
            ;;
            
        2)
            echo ""
            echo "âš¡ Performance Configuration Review:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Current Performance Settings:"
            local cache_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA cache_size;" 2>/dev/null || echo "Unknown")
            local page_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_size;" 2>/dev/null || echo "Unknown")
            local temp_store=$(sqlite3 "$DATABASE_PATH" "PRAGMA temp_store;" 2>/dev/null || echo "Unknown")
            local journal_mode=$(sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode;" 2>/dev/null || echo "Unknown")
            local synchronous=$(sqlite3 "$DATABASE_PATH" "PRAGMA synchronous;" 2>/dev/null || echo "Unknown")
            
            echo ""
            printf "%-20s %-15s %-20s\n" "Setting" "Current Value" "Recommendation"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            # Cache Size Analysis
            local cache_mb=$(echo "scale=1; $cache_size * $page_size / 1048576" | bc 2>/dev/null || echo "Unknown")
            if [[ "$cache_size" -lt 10000 ]]; then
                printf "%-20s %-15s %-20s\n" "Cache Size" "$cache_size pages" "Increase to 10000+"
            else
                printf "%-20s %-15s %-20s\n" "Cache Size" "$cache_size pages" "Good"
            fi
            
            # Page Size Analysis
            if [[ "$page_size" -eq 4096 ]]; then
                printf "%-20s %-15s %-20s\n" "Page Size" "${page_size} bytes" "Optimal"
            elif [[ "$page_size" -lt 4096 ]]; then
                printf "%-20s %-15s %-20s\n" "Page Size" "${page_size} bytes" "Consider 4096"
            else
                printf "%-20s %-15s %-20s\n" "Page Size" "${page_size} bytes" "Good"
            fi
            
            # Journal Mode Analysis
            case "$journal_mode" in
                "WAL") printf "%-20s %-15s %-20s\n" "Journal Mode" "$journal_mode" "Excellent" ;;
                "DELETE") printf "%-20s %-15s %-20s\n" "Journal Mode" "$journal_mode" "Consider WAL" ;;
                *) printf "%-20s %-15s %-20s\n" "Journal Mode" "$journal_mode" "Review needed" ;;
            esac
            
            # Synchronous Mode Analysis
            case "$synchronous" in
                "2"|"FULL") printf "%-20s %-15s %-20s\n" "Synchronous" "FULL" "Safe but slower" ;;
                "1"|"NORMAL") printf "%-20s %-15s %-20s\n" "Synchronous" "NORMAL" "Good balance" ;;
                "0"|"OFF") printf "%-20s %-15s %-20s\n" "Synchronous" "OFF" "Fast but risky" ;;
                *) printf "%-20s %-15s %-20s\n" "Synchronous" "$synchronous" "Unknown" ;;
            esac
            
            # Temp Store Analysis
            case "$temp_store" in
                "2"|"MEMORY") printf "%-20s %-15s %-20s\n" "Temp Store" "MEMORY" "Good for performance" ;;
                "1"|"FILE") printf "%-20s %-15s %-20s\n" "Temp Store" "FILE" "Consider MEMORY" ;;
                "0"|"DEFAULT") printf "%-20s %-15s %-20s\n" "Temp Store" "DEFAULT" "Consider MEMORY" ;;
                *) printf "%-20s %-15s %-20s\n" "Temp Store" "$temp_store" "Unknown" ;;
            esac
            
            echo ""
            echo "ðŸ’¡ Performance Recommendations:"
            
            # Check database size for specific recommendations
            local db_size_kb=$(du -k "$DATABASE_PATH" | cut -f1)
            if [[ "$db_size_kb" -gt 100000 ]]; then
                echo "â€¢ Large database detected (${db_size_kb}KB) - consider:"
                echo "  - Increase cache_size to 20000+ pages"
                echo "  - Use WAL journal mode for better concurrency"
                echo "  - Consider database partitioning for very large datasets"
            fi
            
            # Check for potential bottlenecks
            local total_records=0
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table';
            " 2>/dev/null | while read -r table; do
                local count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                total_records=$((total_records + count))
            done
            
            echo "â€¢ Regular maintenance: Run ANALYZE monthly"
            echo "â€¢ Monitor query performance with EXPLAIN QUERY PLAN"
            echo "â€¢ Consider VACUUM if free pages > 10% of total"
            ;;
            
        3)
            echo ""
            echo "ðŸ›¡ï¸ Security Settings Review:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Database Security Configuration:"
            
            # Check file permissions
            local file_perms=$(ls -l "$DATABASE_PATH" | awk '{print $1}' 2>/dev/null || echo "Unknown")
            local file_owner=$(ls -l "$DATABASE_PATH" | awk '{print $3}' 2>/dev/null || echo "Unknown")
            local file_group=$(ls -l "$DATABASE_PATH" | awk '{print $4}' 2>/dev/null || echo "Unknown")
            
            echo ""
            echo "File System Security:"
            echo "â€¢ Database Permissions: $file_perms"
            echo "â€¢ Owner: $file_owner"
            echo "â€¢ Group: $file_group"
            
            # Check for overly permissive settings
            if [[ "$file_perms" =~ .*r.*.* ]]; then
                echo "âš ï¸  Warning: Database file may be readable by others"
            fi
            
            # Foreign Key Enforcement
            local foreign_keys=$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys;" 2>/dev/null || echo "Unknown")
            echo ""
            echo "Data Integrity Security:"
            echo "â€¢ Foreign Key Enforcement: $foreign_keys"
            if [[ "$foreign_keys" != "1" ]]; then
                echo "âš ï¸  Recommendation: Enable foreign key constraints"
            fi
            
            # Check for security-related configuration
            echo ""
            echo "Application Security Settings:"
            sqlite3 "$DATABASE_PATH" "
                SELECT key, value FROM config WHERE key LIKE '%security%' OR key LIKE '%auth%' OR key LIKE '%access%';
            " 2>/dev/null | while IFS='|' read -r key value; do
                echo "â€¢ $key: $value"
            done
            
            # Database backup security
            echo ""
            echo "Backup Security:"
            if [[ -d "backups" ]]; then
                local backup_perms=$(ls -ld backups 2>/dev/null | awk '{print $1}' || echo "Unknown")
                echo "â€¢ Backup Directory Permissions: $backup_perms"
                local backup_count=$(ls backups/*.db 2>/dev/null | wc -l || echo "0")
                echo "â€¢ Number of Backups: $backup_count"
            else
                echo "â€¢ Backup Directory: Not found"
                echo "âš ï¸  Recommendation: Set up regular database backups"
            fi
            
            echo ""
            echo "ðŸ”’ Security Recommendations:"
            echo "â€¢ Ensure database file permissions are 600 (owner read/write only)"
            echo "â€¢ Enable foreign key constraints for data integrity"
            echo "â€¢ Regular backup rotation with secure storage"
            echo "â€¢ Monitor access patterns and unusual queries"
            echo "â€¢ Consider encryption for sensitive data"
            ;;
            
        4)
            echo ""
            echo "ðŸ’¾ Storage Configuration Analysis:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # File system analysis
            local db_dir=$(dirname "$DATABASE_PATH")
            local available_space=$(df -h "$db_dir" 2>/dev/null | tail -1 | awk '{print $4}' || echo "Unknown")
            local used_space=$(df -h "$db_dir" 2>/dev/null | tail -1 | awk '{print $3}' || echo "Unknown")
            local filesystem=$(df -T "$db_dir" 2>/dev/null | tail -1 | awk '{print $2}' || echo "Unknown")
            
            echo "File System Information:"
            echo "â€¢ Database Directory: $db_dir"
            echo "â€¢ File System Type: $filesystem"
            echo "â€¢ Available Space: $available_space"
            echo "â€¢ Used Space: $used_space"
            
            # Database storage analysis
            local db_size=$(du -h "$DATABASE_PATH" | cut -f1)
            local page_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_size;" 2>/dev/null || echo "0")
            local page_count=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "0")
            local free_pages=$(sqlite3 "$DATABASE_PATH" "PRAGMA freelist_count;" 2>/dev/null || echo "0")
            
            echo ""
            echo "Database Storage Details:"
            echo "â€¢ Database File Size: $db_size"
            echo "â€¢ Page Size: ${page_size} bytes"
            echo "â€¢ Total Pages: $page_count"
            echo "â€¢ Free Pages: $free_pages"
            local efficiency=$((100 - (free_pages * 100 / page_count)))
            echo "â€¢ Storage Efficiency: ${efficiency}%"
            
            # Storage usage by table
            echo ""
            echo "Storage Usage by Table:"
            printf "%-25s %-15s %-15s\n" "Table Name" "Row Count" "Est. Size"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                local col_count=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | wc -l)
                local est_size=$(echo "scale=0; $row_count * $col_count * 50 / 1024" | bc 2>/dev/null || echo "0")
                printf "%-25s %-15s %-15s\n" "$table" "$row_count" "${est_size}KB"
            done
            
            echo ""
            echo "ðŸ“Š Storage Recommendations:"
            
            if [[ $free_pages -gt $((page_count / 10)) ]]; then
                echo "â€¢ High fragmentation detected ($free_pages free pages)"
                echo "  Recommendation: Run VACUUM to reclaim space"
            fi
            
            if [[ "$page_size" -lt 4096 ]]; then
                echo "â€¢ Small page size may impact performance"
                echo "  Recommendation: Consider 4KB pages for new databases"
            fi
            
            # Check available disk space
            local available_mb=$(df -m "$db_dir" 2>/dev/null | tail -1 | awk '{print $4}' || echo "0")
            local db_size_mb=$(du -m "$DATABASE_PATH" | cut -f1)
            
            if [[ "$available_mb" -lt $((db_size_mb * 3)) ]]; then
                echo "â€¢ Low disk space warning"
                echo "  Available: ${available_mb}MB, Database: ${db_size_mb}MB"
                echo "  Recommendation: Ensure at least 3x database size free"
            fi
            ;;
            
        5)
            echo ""
            echo "ðŸ’¾ Backup Configuration Review:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Backup System Status:"
            
            # Check backup directory
            if [[ -d "backups" ]]; then
                echo "âœ… Backup directory exists: backups/"
                
                # List recent backups
                echo ""
                echo "Recent Backups:"
                ls -lt backups/*.db 2>/dev/null | head -5 | while read -r line; do
                    echo "  $line"
                done
                
                # Backup statistics
                local backup_count=$(ls backups/*.db 2>/dev/null | wc -l || echo "0")
                local total_backup_size=$(du -sh backups 2>/dev/null | cut -f1 || echo "0")
                echo ""
                echo "Backup Statistics:"
                echo "â€¢ Total Backups: $backup_count"
                echo "â€¢ Total Backup Size: $total_backup_size"
                
                # Check backup age
                local latest_backup=$(ls -t backups/*.db 2>/dev/null | head -1)
                if [[ -n "$latest_backup" ]]; then
                    local backup_age=$(find "$latest_backup" -mtime +1 2>/dev/null | wc -l)
                    if [[ "$backup_age" -gt 0 ]]; then
                        echo "âš ï¸  Latest backup is more than 24 hours old"
                    else
                        echo "âœ… Recent backup found (within 24 hours)"
                    fi
                fi
            else
                echo "âŒ Backup directory not found"
                echo "âš ï¸  No backup system configured"
            fi
            
            # Check for backup configuration
            echo ""
            echo "Backup Configuration:"
            local backup_enabled=$(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'backup_enabled';" 2>/dev/null || echo "not configured")
            local backup_frequency=$(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'backup_frequency';" 2>/dev/null || echo "not configured")
            local backup_retention=$(sqlite3 "$DATABASE_PATH" "SELECT value FROM config WHERE key = 'backup_retention_days';" 2>/dev/null || echo "not configured")
            
            echo "â€¢ Backup Enabled: $backup_enabled"
            echo "â€¢ Backup Frequency: $backup_frequency"
            echo "â€¢ Retention Policy: $backup_retention"
            
            # Check backup script
            if [[ -f "shared-utilities/backup_manager.sh" ]]; then
                echo "âœ… Backup script found: shared-utilities/backup_manager.sh"
            else
                echo "âŒ Backup script not found"
            fi
            
            echo ""
            echo "ðŸ’¾ Backup Recommendations:"
            
            if [[ ! -d "backups" ]]; then
                echo "â€¢ Create backup directory and implement regular backups"
            fi
            
            if [[ "$backup_enabled" != "true" ]]; then
                echo "â€¢ Enable automatic backups in configuration"
            fi
            
            if [[ "$backup_frequency" == "not configured" ]]; then
                echo "â€¢ Set backup frequency (recommended: daily)"
            fi
            
            if [[ "$backup_retention" == "not configured" ]]; then
                echo "â€¢ Set backup retention policy (recommended: 30 days)"
            fi
            
            echo "â€¢ Test backup restoration procedure regularly"
            echo "â€¢ Consider off-site backup storage for disaster recovery"
            echo "â€¢ Monitor backup integrity with checksum verification"
            ;;
            
        6)
            echo ""
            echo "ðŸ“„ Generating Configuration Report..."
            
            local config_report="reports/database_config_$(date +%Y%m%d_%H%M%S).txt"
            mkdir -p reports
            
            cat > "$config_report" << EOF
GWOMBAT Database Configuration Report
Generated: $(date)
====================================

DATABASE OVERVIEW:
â€¢ Database Path: $DATABASE_PATH
â€¢ SQLite Version: $(sqlite3 "$DATABASE_PATH" "SELECT sqlite_version();" 2>/dev/null || echo "Unknown")
â€¢ File Size: $(du -h "$DATABASE_PATH" | cut -f1)
â€¢ Total Tables: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='table';" 2>/dev/null || echo "0")

PERFORMANCE SETTINGS:
â€¢ Page Size: $(sqlite3 "$DATABASE_PATH" "PRAGMA page_size;" 2>/dev/null || echo "Unknown") bytes
â€¢ Cache Size: $(sqlite3 "$DATABASE_PATH" "PRAGMA cache_size;" 2>/dev/null || echo "Unknown") pages
â€¢ Journal Mode: $(sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode;" 2>/dev/null || echo "Unknown")
â€¢ Synchronous: $(sqlite3 "$DATABASE_PATH" "PRAGMA synchronous;" 2>/dev/null || echo "Unknown")
â€¢ Temp Store: $(sqlite3 "$DATABASE_PATH" "PRAGMA temp_store;" 2>/dev/null || echo "Unknown")
â€¢ Auto Vacuum: $(sqlite3 "$DATABASE_PATH" "PRAGMA auto_vacuum;" 2>/dev/null || echo "Unknown")

STORAGE ANALYSIS:
â€¢ Total Pages: $(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "0")
â€¢ Free Pages: $(sqlite3 "$DATABASE_PATH" "PRAGMA freelist_count;" 2>/dev/null || echo "0")
â€¢ Storage Efficiency: $(echo "scale=1; ($(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "1") - $(sqlite3 "$DATABASE_PATH" "PRAGMA freelist_count;" 2>/dev/null || echo "0")) * 100 / $(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "1")" | bc 2>/dev/null || echo "Unknown")%

SECURITY SETTINGS:
â€¢ File Permissions: $(ls -l "$DATABASE_PATH" | awk '{print $1}' 2>/dev/null || echo "Unknown")
â€¢ Foreign Keys: $(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys;" 2>/dev/null || echo "Unknown")
â€¢ Owner: $(ls -l "$DATABASE_PATH" | awk '{print $3}' 2>/dev/null || echo "Unknown")

APPLICATION CONFIGURATION:
$(sqlite3 "$DATABASE_PATH" "SELECT key || ': ' || value FROM config ORDER BY key;" 2>/dev/null || echo "No application configuration found")

BACKUP STATUS:
â€¢ Backup Directory: $(if [[ -d "backups" ]]; then echo "Present"; else echo "Missing"; fi)
â€¢ Backup Count: $(ls backups/*.db 2>/dev/null | wc -l || echo "0")
â€¢ Latest Backup: $(ls -t backups/*.db 2>/dev/null | head -1 | xargs ls -l 2>/dev/null | awk '{print $6" "$7" "$8}' || echo "None")

INDEX SUMMARY:
â€¢ Total Indexes: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%';" 2>/dev/null || echo "0")
â€¢ Unique Indexes: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND sql LIKE '%UNIQUE%';" 2>/dev/null || echo "0")

TABLE STATISTICS:
$(sqlite3 "$DATABASE_PATH" "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;" 2>/dev/null | while read table; do
    rows=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
    echo "â€¢ $table: $rows rows"
done)

RECOMMENDATIONS:
- Review performance settings based on database size and usage patterns
- Ensure regular backup schedule is implemented and tested
- Monitor storage efficiency and run VACUUM when needed
- Keep SQLite version updated for security and performance improvements
- Review and optimize indexes based on query patterns

Report generated by GWOMBAT Database Management System
EOF
            
            echo "âœ… Configuration report generated: $config_report"
            ;;
            
        7)
            echo ""
            echo "ðŸ”§ Database Configuration Optimization:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "ðŸ” Analyzing current configuration for optimization opportunities..."
            echo ""
            
            # Get current settings
            local cache_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA cache_size;" 2>/dev/null || echo "0")
            local journal_mode=$(sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode;" 2>/dev/null || echo "delete")
            local synchronous=$(sqlite3 "$DATABASE_PATH" "PRAGMA synchronous;" 2>/dev/null || echo "2")
            local temp_store=$(sqlite3 "$DATABASE_PATH" "PRAGMA temp_store;" 2>/dev/null || echo "0")
            local page_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA page_size;" 2>/dev/null || echo "1024")
            
            echo "Current Configuration:"
            echo "â€¢ Cache Size: $cache_size pages"
            echo "â€¢ Journal Mode: $journal_mode"
            echo "â€¢ Synchronous: $synchronous"
            echo "â€¢ Temp Store: $temp_store"
            echo "â€¢ Page Size: $page_size bytes"
            
            echo ""
            echo "ðŸŽ¯ Optimization Recommendations:"
            
            local optimizations=()
            
            # Cache size optimization
            if [[ "$cache_size" -lt 10000 ]]; then
                optimizations+=("Increase cache size to 20000 pages")
                echo "â€¢ Cache size is low - recommend increasing to 20000 pages"
            fi
            
            # Journal mode optimization
            if [[ "$journal_mode" != "wal" ]]; then
                optimizations+=("Change journal mode to WAL")
                echo "â€¢ Journal mode is not WAL - recommend WAL for better performance"
            fi
            
            # Synchronous optimization
            if [[ "$synchronous" == "2" ]]; then
                optimizations+=("Change synchronous to NORMAL")
                echo "â€¢ Synchronous is FULL - consider NORMAL for better performance"
            fi
            
            # Temp store optimization
            if [[ "$temp_store" != "2" ]]; then
                optimizations+=("Change temp store to MEMORY")
                echo "â€¢ Temp store is not MEMORY - recommend MEMORY for performance"
            fi
            
            echo ""
            if [[ ${#optimizations[@]} -gt 0 ]]; then
                read -p "Apply recommended optimizations? (y/n): " apply_optimizations
                
                if [[ "$apply_optimizations" == "y" ]]; then
                    echo ""
                    echo "ðŸ”§ Applying optimizations..."
                    
                    # Apply cache size optimization
                    if [[ "$cache_size" -lt 10000 ]]; then
                        sqlite3 "$DATABASE_PATH" "PRAGMA cache_size = 20000;" 2>/dev/null
                        echo "âœ… Cache size increased to 20000 pages"
                    fi
                    
                    # Apply journal mode optimization
                    if [[ "$journal_mode" != "wal" ]]; then
                        sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode = WAL;" 2>/dev/null
                        echo "âœ… Journal mode changed to WAL"
                    fi
                    
                    # Apply synchronous optimization
                    if [[ "$synchronous" == "2" ]]; then
                        sqlite3 "$DATABASE_PATH" "PRAGMA synchronous = NORMAL;" 2>/dev/null
                        echo "âœ… Synchronous mode changed to NORMAL"
                    fi
                    
                    # Apply temp store optimization
                    if [[ "$temp_store" != "2" ]]; then
                        sqlite3 "$DATABASE_PATH" "PRAGMA temp_store = MEMORY;" 2>/dev/null
                        echo "âœ… Temp store changed to MEMORY"
                    fi
                    
                    echo ""
                    echo "ðŸŽ‰ Optimization completed!"
                    echo "Note: Some optimizations (like WAL mode) persist across sessions"
                    echo "Others (like cache_size) may need to be set each time"
                    
                    # Suggest adding to startup script
                    echo ""
                    echo "ðŸ’¡ Recommendation: Add these PRAGMA statements to your application startup:"
                    echo "   PRAGMA cache_size = 20000;"
                    echo "   PRAGMA temp_store = MEMORY;"
                else
                    echo "âŒ Optimization cancelled"
                fi
            else
                echo "âœ… Database configuration is already well optimized!"
            fi
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Error Log Analysis
error_log_analysis() {
    echo -e "${CYAN}ðŸš¨ Error Log Analysis${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Error Log Analysis Options:"
    echo "1. View recent database errors"
    echo "2. Analyze error patterns"
    echo "3. Check SQLite error logs"
    echo "4. Application error analysis"
    echo "5. Generate error report"
    echo "6. Clear old error logs"
    echo "7. Configure error monitoring"
    echo ""
    read -p "Select analysis option (1-7): " error_choice
    
    case $error_choice in
        1)
            echo ""
            echo "ðŸ“Š Recent Database Errors:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Check for error logs in operation_log table
            local recent_errors=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM operation_log 
                WHERE status = 'error' AND created_at > datetime('now', '-7 days');
            " 2>/dev/null || echo "0")
            
            echo "Recent errors (last 7 days): $recent_errors"
            echo ""
            
            if [[ "$recent_errors" -gt 0 ]]; then
                echo "Latest Database Errors:"
                sqlite3 "$DATABASE_PATH" "
                    SELECT created_at, operation, details 
                    FROM operation_log 
                    WHERE status = 'error' 
                    ORDER BY created_at DESC 
                    LIMIT 10;
                " 2>/dev/null | while IFS='|' read -r timestamp operation details; do
                    echo "[$timestamp] $operation: $details"
                done
            else
                echo "âœ… No recent database errors found"
            fi
            
            # Check for application log files
            echo ""
            echo "Application Log Files:"
            if [[ -d "logs" ]]; then
                find logs -name "*.log" -mtime -7 2>/dev/null | while read -r logfile; do
                    local error_count=$(grep -i error "$logfile" 2>/dev/null | wc -l || echo "0")
                    if [[ "$error_count" -gt 0 ]]; then
                        echo "â€¢ $(basename "$logfile"): $error_count errors"
                        
                        # Show sample errors
                        echo "  Recent errors:"
                        grep -i error "$logfile" 2>/dev/null | tail -3 | while read -r line; do
                            echo "    $line"
                        done
                    fi
                done
            else
                echo "No logs directory found"
            fi
            ;;
            
        2)
            echo ""
            echo "ðŸ“ˆ Error Pattern Analysis:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Analyzing error patterns in database..."
            
            # Error frequency by operation
            echo ""
            echo "Errors by Operation Type:"
            sqlite3 "$DATABASE_PATH" "
                SELECT operation, COUNT(*) as error_count 
                FROM operation_log 
                WHERE status = 'error' AND created_at > datetime('now', '-30 days')
                GROUP BY operation 
                ORDER BY error_count DESC;
            " 2>/dev/null | while IFS='|' read -r operation count; do
                echo "â€¢ $operation: $count errors"
            done
            
            # Error trends over time
            echo ""
            echo "Error Trends (Last 7 days):"
            sqlite3 "$DATABASE_PATH" "
                SELECT date(created_at) as error_date, COUNT(*) as daily_errors
                FROM operation_log 
                WHERE status = 'error' AND created_at > datetime('now', '-7 days')
                GROUP BY date(created_at) 
                ORDER BY error_date DESC;
            " 2>/dev/null | while IFS='|' read -r date count; do
                echo "â€¢ $date: $count errors"
            done
            
            # Common error messages
            echo ""
            echo "Most Common Error Messages:"
            sqlite3 "$DATABASE_PATH" "
                SELECT details, COUNT(*) as occurrence_count
                FROM operation_log 
                WHERE status = 'error' AND created_at > datetime('now', '-30 days')
                GROUP BY details 
                ORDER BY occurrence_count DESC 
                LIMIT 10;
            " 2>/dev/null | while IFS='|' read -r message count; do
                echo "â€¢ ($count times) $message"
            done
            
            # Check for recurring patterns
            echo ""
            echo "ðŸ” Pattern Analysis:"
            
            local total_operations=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            local total_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            
            if [[ "$total_operations" -gt 0 ]]; then
                local error_rate=$(echo "scale=2; ($total_errors * 100) / $total_operations" | bc 2>/dev/null || echo "0")
                echo "â€¢ Overall error rate: ${error_rate}% ($total_errors errors in $total_operations operations)"
                
                if [[ $(echo "$error_rate > 5" | bc 2>/dev/null || echo "0") -eq 1 ]]; then
                    echo "âš ï¸  High error rate detected - investigation recommended"
                elif [[ $(echo "$error_rate > 1" | bc 2>/dev/null || echo "0") -eq 1 ]]; then
                    echo "âš ï¸  Moderate error rate - monitor closely"
                else
                    echo "âœ… Error rate is within acceptable range"
                fi
            fi
            
            # Check for error spikes
            local max_daily_errors=$(sqlite3 "$DATABASE_PATH" "
                SELECT MAX(daily_errors) FROM (
                    SELECT COUNT(*) as daily_errors
                    FROM operation_log 
                    WHERE status = 'error' AND created_at > datetime('now', '-7 days')
                    GROUP BY date(created_at)
                );
            " 2>/dev/null || echo "0")
            
            if [[ "$max_daily_errors" -gt 10 ]]; then
                echo "âš ï¸  Error spike detected: $max_daily_errors errors in a single day"
            fi
            ;;
            
        3)
            echo ""
            echo "ðŸ—ƒï¸ SQLite Error Log Check:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Checking SQLite integrity and errors..."
            
            # Run integrity check
            echo ""
            echo "Database Integrity Check:"
            local integrity_result=$(sqlite3 "$DATABASE_PATH" "PRAGMA integrity_check;" 2>/dev/null)
            
            if [[ "$integrity_result" == "ok" ]]; then
                echo "âœ… Database integrity check passed"
            else
                echo "âŒ Database integrity issues detected:"
                echo "$integrity_result"
            fi
            
            # Check for foreign key violations
            echo ""
            echo "Foreign Key Constraint Check:"
            sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys = ON;" 2>/dev/null
            local fk_violations=$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_key_check;" 2>/dev/null)
            
            if [[ -z "$fk_violations" ]]; then
                echo "âœ… No foreign key violations found"
            else
                echo "âŒ Foreign key violations detected:"
                echo "$fk_violations"
            fi
            
            # Check for SQLite errors in system logs
            echo ""
            echo "System SQLite Errors:"
            if command -v journalctl >/dev/null 2>&1; then
                local sqlite_errors=$(journalctl --since "1 week ago" | grep -i sqlite | grep -i error | wc -l 2>/dev/null || echo "0")
                echo "â€¢ System log SQLite errors (last week): $sqlite_errors"
                
                if [[ "$sqlite_errors" -gt 0 ]]; then
                    echo "Recent SQLite system errors:"
                    journalctl --since "1 week ago" | grep -i sqlite | grep -i error | tail -5
                fi
            else
                echo "â€¢ journalctl not available - cannot check system logs"
            fi
            
            # Check database file for corruption indicators
            echo ""
            echo "Database File Analysis:"
            if command -v file >/dev/null 2>&1; then
                local file_type=$(file "$DATABASE_PATH" 2>/dev/null)
                if [[ "$file_type" == *"SQLite"* ]]; then
                    echo "âœ… Database file format is valid SQLite"
                else
                    echo "âŒ Database file format issue: $file_type"
                fi
            fi
            
            # Check for unusual database size changes
            local current_size=$(du -k "$DATABASE_PATH" | cut -f1)
            echo "â€¢ Current database size: ${current_size}KB"
            
            # Try to detect if database is locked
            if timeout 5 sqlite3 "$DATABASE_PATH" "SELECT 1;" >/dev/null 2>&1; then
                echo "âœ… Database is accessible and not locked"
            else
                echo "âŒ Database may be locked or inaccessible"
            fi
            ;;
            
        4)
            echo ""
            echo "ðŸ“± Application Error Analysis:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Analyzing application-specific errors..."
            
            # Check for application error patterns in operation_log
            echo ""
            echo "Application Error Categories:"
            
            local gam_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%gam%' OR details LIKE '%gam%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            local db_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%database%' OR details LIKE '%sqlite%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            local storage_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%storage%' OR details LIKE '%storage%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            local auth_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%auth%' OR details LIKE '%auth%' OR details LIKE '%permission%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            
            echo "â€¢ GAM-related errors: $gam_errors"
            echo "â€¢ Database errors: $db_errors"
            echo "â€¢ Storage errors: $storage_errors"
            echo "â€¢ Authentication errors: $auth_errors"
            
            # Check for specific GWOMBAT error patterns
            echo ""
            echo "GWOMBAT-Specific Error Analysis:"
            
            # Check for account lifecycle errors
            local lifecycle_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%stage%' OR operation LIKE '%lifecycle%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            echo "â€¢ Account lifecycle errors: $lifecycle_errors"
            
            # Check for export errors
            local export_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND operation LIKE '%export%' AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            echo "â€¢ Export operation errors: $export_errors"
            
            # Check for configuration errors
            local config_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND operation LIKE '%config%' AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            echo "â€¢ Configuration errors: $config_errors"
            
            # Check application log files for additional errors
            echo ""
            echo "Application Log File Analysis:"
            if [[ -d "logs" ]]; then
                find logs -name "*.log" -mtime -7 2>/dev/null | while read -r logfile; do
                    local filename=$(basename "$logfile")
                    echo ""
                    echo "Log file: $filename"
                    
                    # Count different types of errors
                    local critical_errors=$(grep -i "critical\|fatal" "$logfile" 2>/dev/null | wc -l || echo "0")
                    local warning_errors=$(grep -i "warning\|warn" "$logfile" 2>/dev/null | wc -l || echo "0")
                    local error_errors=$(grep -i "error" "$logfile" 2>/dev/null | wc -l || echo "0")
                    
                    echo "  â€¢ Critical/Fatal: $critical_errors"
                    echo "  â€¢ Warnings: $warning_errors"
                    echo "  â€¢ Errors: $error_errors"
                    
                    # Show recent critical errors
                    if [[ "$critical_errors" -gt 0 ]]; then
                        echo "  Recent critical errors:"
                        grep -i "critical\|fatal" "$logfile" 2>/dev/null | tail -2 | while read -r line; do
                            echo "    $line"
                        done
                    fi
                done
            else
                echo "No logs directory found for additional analysis"
            fi
            ;;
            
        5)
            echo ""
            echo "ðŸ“„ Generating Error Analysis Report..."
            
            local error_report="reports/error_analysis_$(date +%Y%m%d_%H%M%S).txt"
            mkdir -p reports
            
            cat > "$error_report" << EOF
GWOMBAT Error Analysis Report
Generated: $(date)
=============================

SUMMARY:
â€¢ Analysis Period: Last 30 days
â€¢ Total Operations: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
â€¢ Total Errors: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
â€¢ Error Rate: $(echo "scale=2; ($(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0") * 100) / $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE created_at > datetime('now', '-30 days');" 2>/dev/null || echo "1")" | bc 2>/dev/null || echo "0")%

ERROR BREAKDOWN BY OPERATION:
$(sqlite3 "$DATABASE_PATH" "
    SELECT operation || ': ' || COUNT(*) || ' errors'
    FROM operation_log 
    WHERE status = 'error' AND created_at > datetime('now', '-30 days')
    GROUP BY operation 
    ORDER BY COUNT(*) DESC;
" 2>/dev/null)

DAILY ERROR TRENDS:
$(sqlite3 "$DATABASE_PATH" "
    SELECT date(created_at) || ': ' || COUNT(*) || ' errors'
    FROM operation_log 
    WHERE status = 'error' AND created_at > datetime('now', '-7 days')
    GROUP BY date(created_at) 
    ORDER BY date(created_at) DESC;
" 2>/dev/null)

MOST COMMON ERROR MESSAGES:
$(sqlite3 "$DATABASE_PATH" "
    SELECT '(' || COUNT(*) || 'x) ' || details
    FROM operation_log 
    WHERE status = 'error' AND created_at > datetime('now', '-30 days')
    GROUP BY details 
    ORDER BY COUNT(*) DESC 
    LIMIT 10;
" 2>/dev/null)

DATABASE INTEGRITY:
â€¢ Integrity Check: $(sqlite3 "$DATABASE_PATH" "PRAGMA integrity_check;" 2>/dev/null)
â€¢ Foreign Key Violations: $(if [[ -z "$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys = ON; PRAGMA foreign_key_check;" 2>/dev/null)" ]]; then echo "None"; else echo "Found"; fi)

ERROR CATEGORIES:
â€¢ GAM-related: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%gam%' OR details LIKE '%gam%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0") errors
â€¢ Database-related: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%database%' OR details LIKE '%sqlite%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0") errors
â€¢ Storage-related: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%storage%' OR details LIKE '%storage%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0") errors
â€¢ Authentication: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND (operation LIKE '%auth%' OR details LIKE '%auth%' OR details LIKE '%permission%') AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0") errors

RECOMMENDATIONS:
- Monitor error patterns for recurring issues
- Address high-frequency error sources first
- Implement better error handling for common failure points
- Regular database integrity checks
- Consider automated error alerting for critical issues

LOG FILES ANALYZED:
$(find logs -name "*.log" -mtime -7 2>/dev/null | while read logfile; do
    errors=$(grep -i error "$logfile" 2>/dev/null | wc -l || echo "0")
    echo "â€¢ $(basename "$logfile"): $errors errors"
done)

Generated by GWOMBAT Database Management System
EOF
            
            echo "âœ… Error analysis report generated: $error_report"
            
            # Show summary
            local total_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND created_at > datetime('now', '-30 days');" 2>/dev/null || echo "0")
            echo ""
            echo "ðŸ“Š Error Analysis Summary:"
            echo "â€¢ Total errors analyzed: $total_errors"
            echo "â€¢ Report saved to: $error_report"
            ;;
            
        6)
            echo ""
            echo "ðŸ§¹ Clear Old Error Logs:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Error log cleanup options:"
            echo "1. Clear database error logs older than 90 days"
            echo "2. Clear application log files older than 30 days"
            echo "3. Archive old error logs"
            echo "4. Clear all error logs (DANGEROUS)"
            echo ""
            read -p "Select cleanup option (1-4): " cleanup_choice
            
            case $cleanup_choice in
                1)
                    echo ""
                    echo "Cleaning database error logs older than 90 days..."
                    
                    local old_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error' AND created_at < datetime('now', '-90 days');" 2>/dev/null || echo "0")
                    
                    if [[ "$old_errors" -gt 0 ]]; then
                        echo "Found $old_errors old error records"
                        read -p "Proceed with cleanup? (y/n): " confirm
                        
                        if [[ "$confirm" == "y" ]]; then
                            sqlite3 "$DATABASE_PATH" "DELETE FROM operation_log WHERE status = 'error' AND created_at < datetime('now', '-90 days');" 2>/dev/null
                            echo "âœ… Cleaned $old_errors old error records"
                        else
                            echo "âŒ Cleanup cancelled"
                        fi
                    else
                        echo "âœ… No old error records to clean"
                    fi
                    ;;
                    
                2)
                    echo ""
                    echo "Cleaning application log files older than 30 days..."
                    
                    if [[ -d "logs" ]]; then
                        local old_logs=$(find logs -name "*.log" -mtime +30 2>/dev/null | wc -l || echo "0")
                        
                        if [[ "$old_logs" -gt 0 ]]; then
                            echo "Found $old_logs old log files"
                            find logs -name "*.log" -mtime +30 2>/dev/null | while read -r logfile; do
                                echo "  â€¢ $(basename "$logfile")"
                            done
                            
                            read -p "Proceed with cleanup? (y/n): " confirm
                            
                            if [[ "$confirm" == "y" ]]; then
                                find logs -name "*.log" -mtime +30 -delete 2>/dev/null
                                echo "âœ… Cleaned $old_logs old log files"
                            else
                                echo "âŒ Cleanup cancelled"
                            fi
                        else
                            echo "âœ… No old log files to clean"
                        fi
                    else
                        echo "No logs directory found"
                    fi
                    ;;
                    
                3)
                    echo ""
                    echo "Archiving old error logs..."
                    
                    local archive_dir="logs/archive_$(date +%Y%m%d)"
                    mkdir -p "$archive_dir"
                    
                    # Archive database errors
                    local archived_errors="$archive_dir/database_errors.csv"
                    sqlite3 -header -csv "$DATABASE_PATH" "
                        SELECT created_at, operation, details 
                        FROM operation_log 
                        WHERE status = 'error' AND created_at < datetime('now', '-30 days')
                        ORDER BY created_at;
                    " > "$archived_errors" 2>/dev/null
                    
                    local error_count=$(tail -n +2 "$archived_errors" | wc -l)
                    echo "âœ… Archived $error_count database errors to: $archived_errors"
                    
                    # Archive old log files
                    if [[ -d "logs" ]]; then
                        find logs -name "*.log" -mtime +30 2>/dev/null | while read -r logfile; do
                            mv "$logfile" "$archive_dir/"
                            echo "âœ… Archived: $(basename "$logfile")"
                        done
                    fi
                    
                    echo "Archive directory: $archive_dir"
                    ;;
                    
                4)
                    echo ""
                    echo "ðŸš¨ DANGER: Clear ALL Error Logs"
                    echo "This will permanently delete all error logs!"
                    echo ""
                    read -p "Type 'DELETE ALL ERRORS' to confirm: " danger_confirm
                    
                    if [[ "$danger_confirm" == "DELETE ALL ERRORS" ]]; then
                        # Clear database errors
                        local total_errors=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM operation_log WHERE status = 'error';" 2>/dev/null || echo "0")
                        sqlite3 "$DATABASE_PATH" "DELETE FROM operation_log WHERE status = 'error';" 2>/dev/null
                        
                        # Clear log files
                        if [[ -d "logs" ]]; then
                            find logs -name "*.log" -delete 2>/dev/null
                        fi
                        
                        echo "âœ… Cleared all error logs ($total_errors database errors)"
                    else
                        echo "âŒ Dangerous cleanup cancelled"
                    fi
                    ;;
            esac
            ;;
            
        7)
            echo ""
            echo "âš™ï¸ Configure Error Monitoring:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Error monitoring configuration:"
            echo "1. Set error alerting thresholds"
            echo "2. Configure error log retention"
            echo "3. Enable/disable error categories"
            echo "4. Set up error notifications"
            echo "5. Configure automated error reports"
            echo ""
            read -p "Select configuration option (1-5): " monitor_choice
            
            case $monitor_choice in
                1)
                    echo ""
                    echo "Setting Error Alerting Thresholds:"
                    
                    read -p "Daily error threshold (alert if exceeded): " daily_threshold
                    read -p "Error rate threshold % (alert if exceeded): " rate_threshold
                    read -p "Critical error immediate alert (y/n): " critical_alert
                    
                    # Save configuration to database
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('error_daily_threshold', '$daily_threshold', datetime('now')),
                        ('error_rate_threshold', '$rate_threshold', datetime('now')),
                        ('error_critical_alert', '$critical_alert', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Error thresholds configured:"
                    echo "   â€¢ Daily threshold: $daily_threshold errors"
                    echo "   â€¢ Rate threshold: $rate_threshold%"
                    echo "   â€¢ Critical alerts: $critical_alert"
                    ;;
                    
                2)
                    echo ""
                    echo "Configure Error Log Retention:"
                    
                    read -p "Database error retention (days): " db_retention
                    read -p "Application log retention (days): " app_retention
                    read -p "Archive before deletion (y/n): " archive_before
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('error_db_retention_days', '$db_retention', datetime('now')),
                        ('error_app_retention_days', '$app_retention', datetime('now')),
                        ('error_archive_before_delete', '$archive_before', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Error retention configured:"
                    echo "   â€¢ Database errors: $db_retention days"
                    echo "   â€¢ Application logs: $app_retention days"
                    echo "   â€¢ Archive before deletion: $archive_before"
                    ;;
                    
                3)
                    echo ""
                    echo "Configure Error Categories:"
                    
                    echo "Enable monitoring for:"
                    read -p "GAM errors (y/n): " gam_monitoring
                    read -p "Database errors (y/n): " db_monitoring
                    read -p "Storage errors (y/n): " storage_monitoring
                    read -p "Authentication errors (y/n): " auth_monitoring
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('monitor_gam_errors', '$gam_monitoring', datetime('now')),
                        ('monitor_db_errors', '$db_monitoring', datetime('now')),
                        ('monitor_storage_errors', '$storage_monitoring', datetime('now')),
                        ('monitor_auth_errors', '$auth_monitoring', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Error category monitoring configured"
                    ;;
                    
                4)
                    echo ""
                    echo "Set Up Error Notifications:"
                    
                    read -p "Email address for notifications: " notify_email
                    read -p "Notification frequency (immediate/hourly/daily): " notify_freq
                    read -p "Include error details in notifications (y/n): " include_details
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('error_notify_email', '$notify_email', datetime('now')),
                        ('error_notify_frequency', '$notify_freq', datetime('now')),
                        ('error_notify_include_details', '$include_details', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Error notifications configured:"
                    echo "   â€¢ Email: $notify_email"
                    echo "   â€¢ Frequency: $notify_freq"
                    echo "   â€¢ Include details: $include_details"
                    echo ""
                    echo "Note: Implement notification delivery mechanism separately"
                    ;;
                    
                5)
                    echo ""
                    echo "Configure Automated Error Reports:"
                    
                    read -p "Generate daily error reports (y/n): " daily_reports
                    read -p "Generate weekly error reports (y/n): " weekly_reports
                    read -p "Report delivery method (email/file): " report_method
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('error_daily_reports', '$daily_reports', datetime('now')),
                        ('error_weekly_reports', '$weekly_reports', datetime('now')),
                        ('error_report_method', '$report_method', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Automated error reports configured:"
                    echo "   â€¢ Daily reports: $daily_reports"
                    echo "   â€¢ Weekly reports: $weekly_reports"
                    echo "   â€¢ Delivery method: $report_method"
                    ;;
            esac
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Database Performance Tuning
database_performance_tuning() {
    echo -e "${CYAN}âš¡ Database Performance Tuning${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Performance Tuning Options:"
    echo "1. Analyze current performance"
    echo "2. Optimize SQLite settings"
    echo "3. Query performance analysis"
    echo "4. Index optimization"
    echo "5. Database maintenance"
    echo "6. Performance monitoring setup"
    echo "7. Generate performance report"
    echo ""
    read -p "Select tuning option (1-7): " perf_choice
    
    case $perf_choice in
        1)
            echo ""
            echo "ðŸ“Š Current Performance Analysis:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Database Performance Metrics:"
            
            # Database size and structure
            local db_size_mb=$(echo "scale=2; $(du -k "$DATABASE_PATH" | cut -f1) / 1024" | bc 2>/dev/null || echo "0")
            local total_tables=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='table';" 2>/dev/null || echo "0")
            local total_indexes=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%';" 2>/dev/null || echo "0")
            
            echo "â€¢ Database Size: ${db_size_mb}MB"
            echo "â€¢ Total Tables: $total_tables"
            echo "â€¢ Custom Indexes: $total_indexes"
            
            # Record counts for major tables
            echo ""
            echo "Table Sizes and Performance Impact:"
            printf "%-25s %-15s %-15s %-20s\n" "Table Name" "Row Count" "Est. Size" "Performance Notes"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                local col_count=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | wc -l)
                local est_size_kb=$(echo "scale=0; $row_count * $col_count * 30 / 1024" | bc 2>/dev/null || echo "0")
                
                local performance_notes=""
                if [[ "$row_count" -gt 100000 ]]; then
                    performance_notes="Large - needs indexes"
                elif [[ "$row_count" -gt 10000 ]]; then
                    performance_notes="Medium - monitor"
                else
                    performance_notes="Small - good"
                fi
                
                printf "%-25s %-15s %-15s %-20s\n" "$table" "$row_count" "${est_size_kb}KB" "$performance_notes"
            done
            
            # Current SQLite settings
            echo ""
            echo "Current SQLite Performance Settings:"
            local cache_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA cache_size;" 2>/dev/null || echo "Unknown")
            local journal_mode=$(sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode;" 2>/dev/null || echo "Unknown")
            local synchronous=$(sqlite3 "$DATABASE_PATH" "PRAGMA synchronous;" 2>/dev/null || echo "Unknown")
            local temp_store=$(sqlite3 "$DATABASE_PATH" "PRAGMA temp_store;" 2>/dev/null || echo "Unknown")
            
            echo "â€¢ Cache Size: $cache_size pages"
            echo "â€¢ Journal Mode: $journal_mode"
            echo "â€¢ Synchronous: $synchronous"
            echo "â€¢ Temp Store: $temp_store"
            
            # Performance score calculation
            local performance_score=100
            
            if [[ "$cache_size" -lt 10000 ]]; then
                performance_score=$((performance_score - 20))
            fi
            
            if [[ "$journal_mode" != "wal" ]]; then
                performance_score=$((performance_score - 15))
            fi
            
            if [[ "$total_indexes" -lt $((total_tables / 2)) ]]; then
                performance_score=$((performance_score - 25))
            fi
            
            echo ""
            echo "ðŸ“ˆ Performance Score: ${performance_score}/100"
            
            if [[ $performance_score -ge 80 ]]; then
                echo "âœ… Excellent performance configuration"
            elif [[ $performance_score -ge 60 ]]; then
                echo "âš ï¸  Good performance, some optimizations possible"
            else
                echo "âŒ Performance issues detected, optimization recommended"
            fi
            ;;
            
        2)
            echo ""
            echo "ðŸ”§ SQLite Settings Optimization:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Current vs Recommended Settings:"
            printf "%-20s %-20s %-20s %-15s\n" "Setting" "Current" "Recommended" "Action"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            # Cache Size
            local cache_size=$(sqlite3 "$DATABASE_PATH" "PRAGMA cache_size;" 2>/dev/null || echo "0")
            local recommended_cache="20000"
            local cache_action="No change"
            if [[ "$cache_size" -lt 10000 ]]; then
                cache_action="Increase"
            fi
            printf "%-20s %-20s %-20s %-15s\n" "Cache Size" "$cache_size pages" "$recommended_cache pages" "$cache_action"
            
            # Journal Mode
            local journal_mode=$(sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode;" 2>/dev/null || echo "delete")
            local recommended_journal="WAL"
            local journal_action="No change"
            if [[ "$journal_mode" != "wal" ]]; then
                journal_action="Change to WAL"
            fi
            printf "%-20s %-20s %-20s %-15s\n" "Journal Mode" "$journal_mode" "$recommended_journal" "$journal_action"
            
            # Synchronous
            local synchronous=$(sqlite3 "$DATABASE_PATH" "PRAGMA synchronous;" 2>/dev/null || echo "2")
            local recommended_sync="NORMAL"
            local sync_action="No change"
            if [[ "$synchronous" == "2" ]]; then
                sync_action="Optimize"
            fi
            printf "%-20s %-20s %-20s %-15s\n" "Synchronous" "$synchronous" "$recommended_sync" "$sync_action"
            
            # Temp Store
            local temp_store=$(sqlite3 "$DATABASE_PATH" "PRAGMA temp_store;" 2>/dev/null || echo "0")
            local recommended_temp="MEMORY"
            local temp_action="No change"
            if [[ "$temp_store" != "2" ]]; then
                temp_action="Change to MEM"
            fi
            printf "%-20s %-20s %-20s %-15s\n" "Temp Store" "$temp_store" "$recommended_temp" "$temp_action"
            
            echo ""
            read -p "Apply recommended optimizations? (y/n): " apply_opts
            
            if [[ "$apply_opts" == "y" ]]; then
                echo ""
                echo "ðŸ”§ Applying SQLite optimizations..."
                
                # Apply optimizations
                if [[ "$cache_size" -lt 10000 ]]; then
                    sqlite3 "$DATABASE_PATH" "PRAGMA cache_size = 20000;" 2>/dev/null
                    echo "âœ… Cache size increased to 20000 pages"
                fi
                
                if [[ "$journal_mode" != "wal" ]]; then
                    sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode = WAL;" 2>/dev/null
                    echo "âœ… Journal mode changed to WAL"
                fi
                
                if [[ "$synchronous" == "2" ]]; then
                    sqlite3 "$DATABASE_PATH" "PRAGMA synchronous = NORMAL;" 2>/dev/null
                    echo "âœ… Synchronous mode optimized to NORMAL"
                fi
                
                if [[ "$temp_store" != "2" ]]; then
                    sqlite3 "$DATABASE_PATH" "PRAGMA temp_store = MEMORY;" 2>/dev/null
                    echo "âœ… Temp store changed to MEMORY"
                fi
                
                echo ""
                echo "ðŸŽ‰ SQLite optimization completed!"
                echo "Note: Some settings persist, others may need to be set each session"
            else
                echo "âŒ Optimization cancelled"
            fi
            ;;
            
        3)
            echo ""
            echo "ðŸ” Query Performance Analysis:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Analyzing common query patterns..."
            
            # Test common query types
            echo ""
            echo "Query Performance Tests:"
            
            # Test 1: Full table scan
            echo "1. Full table scan performance:"
            local largest_table=$(sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' 
                ORDER BY (SELECT COUNT(*) FROM sqlite_master s WHERE s.name = sqlite_master.name) DESC 
                LIMIT 1;
            " 2>/dev/null)
            
            if [[ -n "$largest_table" ]]; then
                echo "   Testing table: $largest_table"
                local scan_time=$(time (sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$largest_table];" >/dev/null) 2>&1 | grep real | awk '{print $2}')
                echo "   Full scan time: $scan_time"
            fi
            
            # Test 2: Index usage
            echo ""
            echo "2. Index usage analysis:"
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' AND sql LIKE '%email%';
            " 2>/dev/null | head -1 | while read -r email_table; do
                if [[ -n "$email_table" ]]; then
                    echo "   Testing email lookup on: $email_table"
                    
                    # Check if email is indexed
                    local email_indexed=$(sqlite3 "$DATABASE_PATH" "
                        SELECT COUNT(*) FROM sqlite_master 
                        WHERE type = 'index' AND tbl_name = '$email_table' AND sql LIKE '%email%';
                    " 2>/dev/null || echo "0")
                    
                    if [[ "$email_indexed" -gt 0 ]]; then
                        echo "   âœ… Email column is indexed - queries should be fast"
                    else
                        echo "   âš ï¸  Email column not indexed - performance impact"
                        
                        # Suggest creating index
                        echo "   ðŸ’¡ Suggestion: CREATE INDEX idx_${email_table}_email ON $email_table(email);"
                    fi
                fi
            done
            
            # Test 3: Join performance
            echo ""
            echo "3. Join performance analysis:"
            local tables_with_relationships=$(sqlite3 "$DATABASE_PATH" "
                SELECT DISTINCT tbl_name FROM sqlite_master 
                WHERE type = 'index' AND sql LIKE '%email%'
                LIMIT 2;
            " 2>/dev/null)
            
            if [[ $(echo "$tables_with_relationships" | wc -l) -ge 2 ]]; then
                echo "   Multiple tables with email columns found"
                echo "   Join operations should perform well with existing indexes"
            else
                echo "   Limited relationship data for join testing"
            fi
            
            # Query plan analysis
            echo ""
            echo "4. Query plan analysis:"
            echo "   Sample query plans for common operations:"
            
            # Example query plan
            if [[ -n "$largest_table" ]]; then
                echo "   Query: SELECT * FROM $largest_table LIMIT 10"
                sqlite3 "$DATABASE_PATH" "EXPLAIN QUERY PLAN SELECT * FROM [$largest_table] LIMIT 10;" 2>/dev/null | while read -r line; do
                    echo "     $line"
                done
            fi
            
            echo ""
            echo "ðŸ’¡ Query Optimization Recommendations:"
            echo "â€¢ Add indexes on frequently queried columns"
            echo "â€¢ Use LIMIT clauses for large result sets"
            echo "â€¢ Avoid SELECT * in production queries"
            echo "â€¢ Use EXPLAIN QUERY PLAN to analyze slow queries"
            echo "â€¢ Consider composite indexes for multi-column WHERE clauses"
            ;;
            
        4)
            echo ""
            echo "ðŸ—‚ï¸ Index Optimization:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Analyzing index usage and optimization opportunities..."
            
            # Current index summary
            local total_indexes=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%';" 2>/dev/null || echo "0")
            local total_tables=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='table';" 2>/dev/null || echo "0")
            
            echo ""
            echo "Index Overview:"
            echo "â€¢ Total custom indexes: $total_indexes"
            echo "â€¢ Total tables: $total_tables"
            echo "â€¢ Average indexes per table: $(echo "scale=1; $total_indexes / $total_tables" | bc 2>/dev/null || echo "0")"
            
            # Tables without indexes
            echo ""
            echo "Tables without indexes:"
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master 
                WHERE type = 'table' 
                AND name NOT IN (
                    SELECT DISTINCT tbl_name FROM sqlite_master 
                    WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
                )
                ORDER BY name;
            " 2>/dev/null | while read -r table; do
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                if [[ "$row_count" -gt 100 ]]; then
                    echo "â€¢ $table ($row_count rows) - consider adding indexes"
                else
                    echo "â€¢ $table ($row_count rows) - small table, indexes optional"
                fi
            done
            
            # Index suggestions
            echo ""
            echo "Index Recommendations:"
            
            sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;
            " 2>/dev/null | while read -r table; do
                local row_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
                
                if [[ "$row_count" -gt 1000 ]]; then
                    echo ""
                    echo "Table: $table ($row_count rows)"
                    
                    # Check for common columns that should be indexed
                    local has_email=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -i email | wc -l)
                    local has_timestamp=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -E "(timestamp|created_at|updated_at|scan_time)" | wc -l)
                    local has_status=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -i status | wc -l)
                    
                    # Check existing indexes
                    local existing_indexes=$(sqlite3 "$DATABASE_PATH" "
                        SELECT COUNT(*) FROM sqlite_master 
                        WHERE type = 'index' AND tbl_name = '$table' AND name NOT LIKE 'sqlite_%';
                    " 2>/dev/null || echo "0")
                    
                    echo "  Current indexes: $existing_indexes"
                    
                    # Email index recommendation
                    if [[ "$has_email" -gt 0 ]]; then
                        local email_indexed=$(sqlite3 "$DATABASE_PATH" "
                            SELECT COUNT(*) FROM sqlite_master 
                            WHERE type = 'index' AND tbl_name = '$table' AND sql LIKE '%email%';
                        " 2>/dev/null || echo "0")
                        
                        if [[ "$email_indexed" -eq 0 ]]; then
                            echo "  ðŸ’¡ CREATE INDEX idx_${table}_email ON $table(email);"
                        fi
                    fi
                    
                    # Timestamp index recommendation
                    if [[ "$has_timestamp" -gt 0 ]]; then
                        local timestamp_indexed=$(sqlite3 "$DATABASE_PATH" "
                            SELECT COUNT(*) FROM sqlite_master 
                            WHERE type = 'index' AND tbl_name = '$table' 
                            AND (sql LIKE '%timestamp%' OR sql LIKE '%created_at%' OR sql LIKE '%scan_time%');
                        " 2>/dev/null || echo "0")
                        
                        if [[ "$timestamp_indexed" -eq 0 ]]; then
                            # Get the actual timestamp column name
                            local timestamp_col=$(sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -E "(timestamp|created_at|updated_at|scan_time)" | head -1 | cut -d'|' -f2)
                            if [[ -n "$timestamp_col" ]]; then
                                echo "  ðŸ’¡ CREATE INDEX idx_${table}_${timestamp_col} ON $table($timestamp_col);"
                            fi
                        fi
                    fi
                    
                    # Status index recommendation
                    if [[ "$has_status" -gt 0 ]]; then
                        local status_indexed=$(sqlite3 "$DATABASE_PATH" "
                            SELECT COUNT(*) FROM sqlite_master 
                            WHERE type = 'index' AND tbl_name = '$table' AND sql LIKE '%status%';
                        " 2>/dev/null || echo "0")
                        
                        if [[ "$status_indexed" -eq 0 ]]; then
                            echo "  ðŸ’¡ CREATE INDEX idx_${table}_status ON $table(status);"
                        fi
                    fi
                fi
            done
            
            echo ""
            read -p "Create recommended indexes automatically? (y/n): " create_indexes
            
            if [[ "$create_indexes" == "y" ]]; then
                echo ""
                echo "ðŸ”§ Creating recommended indexes..."
                
                # Implementation would create the suggested indexes
                echo "Note: Index creation implementation would go here"
                echo "Manual review of suggestions above is recommended"
            else
                echo "Index creation skipped - review suggestions above"
            fi
            ;;
            
        5)
            echo ""
            echo "ðŸ§¹ Database Maintenance:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Database maintenance operations:"
            echo "1. Run ANALYZE (update statistics)"
            echo "2. Run VACUUM (reclaim space)"
            echo "3. REINDEX (rebuild indexes)"
            echo "4. Comprehensive maintenance"
            echo ""
            read -p "Select maintenance operation (1-4): " maint_choice
            
            case $maint_choice in
                1)
                    echo ""
                    echo "ðŸ” Running ANALYZE..."
                    echo "This updates SQLite's internal statistics for query optimization"
                    
                    local start_time=$(date +%s)
                    sqlite3 "$DATABASE_PATH" "ANALYZE;" 2>/dev/null
                    local end_time=$(date +%s)
                    local duration=$((end_time - start_time))
                    
                    echo "âœ… ANALYZE completed in ${duration} seconds"
                    echo "Query planner statistics have been updated"
                    ;;
                    
                2)
                    echo ""
                    echo "ðŸ—œï¸ Running VACUUM..."
                    
                    local db_size_before=$(du -k "$DATABASE_PATH" | cut -f1)
                    echo "Database size before VACUUM: ${db_size_before}KB"
                    
                    local start_time=$(date +%s)
                    sqlite3 "$DATABASE_PATH" "VACUUM;" 2>/dev/null
                    local end_time=$(date +%s)
                    local duration=$((end_time - start_time))
                    
                    local db_size_after=$(du -k "$DATABASE_PATH" | cut -f1)
                    local space_saved=$((db_size_before - db_size_after))
                    
                    echo "âœ… VACUUM completed in ${duration} seconds"
                    echo "Database size after VACUUM: ${db_size_after}KB"
                    echo "Space reclaimed: ${space_saved}KB"
                    ;;
                    
                3)
                    echo ""
                    echo "ðŸ”„ Running REINDEX..."
                    echo "This rebuilds all indexes for optimal performance"
                    
                    local start_time=$(date +%s)
                    sqlite3 "$DATABASE_PATH" "REINDEX;" 2>/dev/null
                    local end_time=$(date +%s)
                    local duration=$((end_time - start_time))
                    
                    echo "âœ… REINDEX completed in ${duration} seconds"
                    echo "All indexes have been rebuilt"
                    ;;
                    
                4)
                    echo ""
                    echo "ðŸš€ Comprehensive Database Maintenance..."
                    echo "Running ANALYZE, VACUUM, and REINDEX in sequence"
                    echo ""
                    
                    local total_start=$(date +%s)
                    
                    echo "1/3 - Running ANALYZE..."
                    sqlite3 "$DATABASE_PATH" "ANALYZE;" 2>/dev/null
                    echo "âœ… ANALYZE completed"
                    
                    echo "2/3 - Running VACUUM..."
                    local db_size_before=$(du -k "$DATABASE_PATH" | cut -f1)
                    sqlite3 "$DATABASE_PATH" "VACUUM;" 2>/dev/null
                    local db_size_after=$(du -k "$DATABASE_PATH" | cut -f1)
                    local space_saved=$((db_size_before - db_size_after))
                    echo "âœ… VACUUM completed - ${space_saved}KB reclaimed"
                    
                    echo "3/3 - Running REINDEX..."
                    sqlite3 "$DATABASE_PATH" "REINDEX;" 2>/dev/null
                    echo "âœ… REINDEX completed"
                    
                    local total_end=$(date +%s)
                    local total_duration=$((total_end - total_start))
                    
                    echo ""
                    echo "ðŸŽ‰ Comprehensive maintenance completed!"
                    echo "Total time: ${total_duration} seconds"
                    echo "Database is now optimized for performance"
                    ;;
            esac
            ;;
            
        6)
            echo ""
            echo "ðŸ“Š Performance Monitoring Setup:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Configure performance monitoring:"
            echo "1. Enable query performance logging"
            echo "2. Set up performance alerts"
            echo "3. Configure automated maintenance"
            echo "4. Enable performance metrics collection"
            echo ""
            read -p "Select monitoring option (1-4): " monitor_choice
            
            case $monitor_choice in
                1)
                    read -p "Enable query performance logging? (y/n): " enable_logging
                    read -p "Log slow queries only? (y/n): " slow_only
                    read -p "Slow query threshold (seconds): " slow_threshold
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('perf_query_logging', '$enable_logging', datetime('now')),
                        ('perf_slow_queries_only', '$slow_only', datetime('now')),
                        ('perf_slow_threshold', '$slow_threshold', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Query performance logging configured"
                    ;;
                    
                2)
                    read -p "Alert on slow queries? (y/n): " alert_slow
                    read -p "Alert on database size growth? (y/n): " alert_growth
                    read -p "Performance check frequency (hours): " check_freq
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('perf_alert_slow_queries', '$alert_slow', datetime('now')),
                        ('perf_alert_db_growth', '$alert_growth', datetime('now')),
                        ('perf_check_frequency', '$check_freq', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Performance alerts configured"
                    ;;
                    
                3)
                    read -p "Auto-run ANALYZE weekly? (y/n): " auto_analyze
                    read -p "Auto-run VACUUM monthly? (y/n): " auto_vacuum
                    read -p "Auto-clean logs daily? (y/n): " auto_clean
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('perf_auto_analyze', '$auto_analyze', datetime('now')),
                        ('perf_auto_vacuum', '$auto_vacuum', datetime('now')),
                        ('perf_auto_clean_logs', '$auto_clean', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Automated maintenance configured"
                    ;;
                    
                4)
                    read -p "Collect query execution times? (y/n): " collect_times
                    read -p "Track table growth rates? (y/n): " track_growth
                    read -p "Monitor index usage? (y/n): " monitor_indexes
                    
                    sqlite3 "$DATABASE_PATH" "
                        INSERT OR REPLACE INTO config (key, value, updated_at) VALUES
                        ('perf_collect_times', '$collect_times', datetime('now')),
                        ('perf_track_growth', '$track_growth', datetime('now')),
                        ('perf_monitor_indexes', '$monitor_indexes', datetime('now'));
                    " 2>/dev/null
                    
                    echo "âœ… Performance metrics collection configured"
                    ;;
            esac
            ;;
            
        7)
            echo ""
            echo "ðŸ“„ Generating Performance Report..."
            
            local perf_report="reports/performance_tuning_$(date +%Y%m%d_%H%M%S).txt"
            mkdir -p reports
            
            cat > "$perf_report" << EOF
GWOMBAT Database Performance Tuning Report
Generated: $(date)
==========================================

DATABASE OVERVIEW:
â€¢ Database Size: $(echo "scale=2; $(du -k "$DATABASE_PATH" | cut -f1) / 1024" | bc 2>/dev/null || echo "0")MB
â€¢ Total Tables: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='table';" 2>/dev/null || echo "0")
â€¢ Custom Indexes: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%';" 2>/dev/null || echo "0")

CURRENT SQLITE SETTINGS:
â€¢ Cache Size: $(sqlite3 "$DATABASE_PATH" "PRAGMA cache_size;" 2>/dev/null || echo "Unknown") pages
â€¢ Journal Mode: $(sqlite3 "$DATABASE_PATH" "PRAGMA journal_mode;" 2>/dev/null || echo "Unknown")
â€¢ Synchronous: $(sqlite3 "$DATABASE_PATH" "PRAGMA synchronous;" 2>/dev/null || echo "Unknown")
â€¢ Temp Store: $(sqlite3 "$DATABASE_PATH" "PRAGMA temp_store;" 2>/dev/null || echo "Unknown")
â€¢ Page Size: $(sqlite3 "$DATABASE_PATH" "PRAGMA page_size;" 2>/dev/null || echo "Unknown") bytes

STORAGE ANALYSIS:
â€¢ Total Pages: $(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "0")
â€¢ Free Pages: $(sqlite3 "$DATABASE_PATH" "PRAGMA freelist_count;" 2>/dev/null || echo "0")
â€¢ Storage Efficiency: $(echo "scale=1; ($(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "1") - $(sqlite3 "$DATABASE_PATH" "PRAGMA freelist_count;" 2>/dev/null || echo "0")) * 100 / $(sqlite3 "$DATABASE_PATH" "PRAGMA page_count;" 2>/dev/null || echo "1")" | bc 2>/dev/null || echo "Unknown")%

TABLE PERFORMANCE ANALYSIS:
$(sqlite3 "$DATABASE_PATH" "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;" 2>/dev/null | while read table; do
    rows=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM [$table];" 2>/dev/null || echo "0")
    indexes=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND tbl_name='$table' AND name NOT LIKE 'sqlite_%';" 2>/dev/null || echo "0")
    
    if [ "$rows" -gt 10000 ]; then
        status="Large - needs monitoring"
    elif [ "$rows" -gt 1000 ]; then
        status="Medium - good size"
    else
        status="Small - efficient"
    fi
    
    echo "â€¢ $table: $rows rows, $indexes indexes ($status)"
done)

INDEX ANALYSIS:
$(sqlite3 "$DATABASE_PATH" "
    SELECT tbl_name || ': ' || COUNT(*) || ' indexes'
    FROM sqlite_master 
    WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
    GROUP BY tbl_name 
    ORDER BY tbl_name;
" 2>/dev/null)

TABLES WITHOUT INDEXES:
$(sqlite3 "$DATABASE_PATH" "
    SELECT name || ' (' || 'needs indexes)'
    FROM sqlite_master 
    WHERE type = 'table' 
    AND name NOT IN (
        SELECT DISTINCT tbl_name FROM sqlite_master 
        WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
    )
    ORDER BY name;
" 2>/dev/null)

PERFORMANCE RECOMMENDATIONS:
- Ensure cache_size is at least 20000 pages for better performance
- Use WAL journal mode for better concurrency
- Add indexes on frequently queried columns (email, timestamps, status)
- Run ANALYZE monthly to update query planner statistics
- Run VACUUM when free pages exceed 10% of total pages
- Monitor query performance and optimize slow queries
- Consider partitioning very large tables

OPTIMIZATION PRIORITIES:
1. Add missing indexes on large tables
2. Optimize SQLite PRAGMA settings
3. Implement regular maintenance schedule
4. Monitor and tune slow queries
5. Set up performance monitoring

Report generated by GWOMBAT Database Management System
EOF
            
            echo "âœ… Performance tuning report generated: $perf_report"
            
            echo ""
            echo "ðŸ“Š Performance Tuning Summary:"
            echo "â€¢ Report contains comprehensive performance analysis"
            echo "â€¢ Follow recommendations for optimal database performance"
            echo "â€¢ Report saved to: $perf_report"
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Database Security Audit
database_security_audit() {
    echo -e "${CYAN}ðŸ›¡ï¸ Database Security Audit${NC}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    
    echo "Security Audit Options:"
    echo "1. File system security check"
    echo "2. Database access control audit"
    echo "3. Data integrity verification"
    echo "4. Configuration security review"
    echo "5. Backup security assessment"
    echo "6. SQL injection vulnerability scan"
    echo "7. Generate security report"
    echo ""
    read -p "Select audit option (1-7): " security_choice
    
    case $security_choice in
        1)
            echo ""
            echo "ðŸ“ File System Security Check:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Database File Security Analysis:"
            
            # File permissions
            local file_perms=$(ls -l "$DATABASE_PATH" | awk '{print $1}' 2>/dev/null || echo "Unknown")
            local file_owner=$(ls -l "$DATABASE_PATH" | awk '{print $3}' 2>/dev/null || echo "Unknown")
            local file_group=$(ls -l "$DATABASE_PATH" | awk '{print $4}' 2>/dev/null || echo "Unknown")
            
            echo "â€¢ Database file: $DATABASE_PATH"
            echo "â€¢ Permissions: $file_perms"
            echo "â€¢ Owner: $file_owner"
            echo "â€¢ Group: $file_group"
            
            # Permission analysis
            echo ""
            echo "Permission Security Analysis:"
            
            if [[ "$file_perms" =~ ^-rw------- ]]; then
                echo "âœ… Excellent: Owner read/write only (600)"
            elif [[ "$file_perms" =~ ^-rw-rw---- ]]; then
                echo "âš ï¸  Warning: Group readable (660) - consider restricting"
            elif [[ "$file_perms" =~ ^-rw-r--r-- ]]; then
                echo "âŒ Critical: World readable (644) - SECURITY RISK!"
            else
                echo "âš ï¸  Unusual permissions: $file_perms - review required"
            fi
            
            # Directory permissions
            local db_dir=$(dirname "$DATABASE_PATH")
            local dir_perms=$(ls -ld "$db_dir" | awk '{print $1}' 2>/dev/null || echo "Unknown")
            
            echo ""
            echo "Directory Security:"
            echo "â€¢ Directory: $db_dir"
            echo "â€¢ Permissions: $dir_perms"
            
            if [[ "$dir_perms" =~ ^drwx------ ]]; then
                echo "âœ… Good: Directory restricted to owner"
            elif [[ "$dir_perms" =~ ^drwx.*r.* ]]; then
                echo "âš ï¸  Warning: Directory readable by others"
            fi
            
            # Check for backup file security
            echo ""
            echo "Backup File Security:"
            if [[ -d "backups" ]]; then
                local backup_perms=$(ls -ld backups | awk '{print $1}' 2>/dev/null || echo "Unknown")
                echo "â€¢ Backup directory permissions: $backup_perms"
                
                local backup_files=$(ls backups/*.db 2>/dev/null | wc -l || echo "0")
                echo "â€¢ Number of backup files: $backup_files"
                
                if [[ "$backup_files" -gt 0 ]]; then
                    echo "â€¢ Backup file permissions:"
                    ls -l backups/*.db 2>/dev/null | head -3 | while read -r line; do
                        echo "  $line"
                    done
                fi
            else
                echo "â€¢ No backup directory found"
            fi
            
            # Check for temporary files
            echo ""
            echo "Temporary File Security:"
            local temp_files=$(find /tmp -name "*gwombat*" -o -name "*sqlite*" 2>/dev/null | wc -l || echo "0")
            echo "â€¢ Temporary files found: $temp_files"
            
            if [[ "$temp_files" -gt 0 ]]; then
                echo "âš ï¸  Warning: Temporary files detected - review and clean"
                find /tmp -name "*gwombat*" -o -name "*sqlite*" 2>/dev/null | head -5
            else
                echo "âœ… No temporary database files found"
            fi
            ;;
            
        2)
            echo ""
            echo "ðŸ” Database Access Control Audit:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Access Control Analysis:"
            
            # Check for authentication mechanisms
            echo "â€¢ Authentication: File system based (SQLite)"
            echo "â€¢ Access method: Direct file access"
            
            # Foreign key enforcement
            local foreign_keys=$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys;" 2>/dev/null || echo "Unknown")
            echo "â€¢ Foreign key enforcement: $foreign_keys"
            
            if [[ "$foreign_keys" == "1" ]]; then
                echo "âœ… Foreign key constraints enabled"
            else
                echo "âš ï¸  Foreign key constraints disabled - data integrity risk"
            fi
            
            # Check for user/role tables
            echo ""
            echo "Application-Level Access Control:"
            
            local user_tables=$(sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master 
                WHERE type='table' AND (name LIKE '%user%' OR name LIKE '%auth%' OR name LIKE '%permission%');
            " 2>/dev/null | wc -l || echo "0")
            
            if [[ "$user_tables" -gt 0 ]]; then
                echo "â€¢ User/auth tables found: $user_tables"
                sqlite3 "$DATABASE_PATH" "
                    SELECT name FROM sqlite_master 
                    WHERE type='table' AND (name LIKE '%user%' OR name LIKE '%auth%' OR name LIKE '%permission%');
                " 2>/dev/null | while read -r table; do
                    echo "  â€¢ $table"
                done
            else
                echo "â€¢ No dedicated user/auth tables found"
                echo "â€¢ Access control managed externally"
            fi
            
            # Check for sensitive data exposure
            echo ""
            echo "Sensitive Data Exposure Check:"
            
            # Look for potential password/token columns
            local sensitive_columns=$(sqlite3 "$DATABASE_PATH" "
                SELECT name FROM sqlite_master WHERE type='table';
            " 2>/dev/null | while read -r table; do
                sqlite3 "$DATABASE_PATH" "PRAGMA table_info([$table]);" 2>/dev/null | grep -i -E "(password|token|secret|key|auth)" | while IFS='|' read -r cid name type notnull dflt pk; do
                    echo "$table.$name"
                done
            done)
            
            if [[ -n "$sensitive_columns" ]]; then
                echo "âš ï¸  Potentially sensitive columns found:"
                echo "$sensitive_columns" | while read -r column; do
                    echo "  â€¢ $column"
                done
                echo "  Recommendation: Ensure sensitive data is encrypted"
            else
                echo "âœ… No obvious sensitive data columns detected"
            fi
            
            # Check for admin privileges
            echo ""
            echo "Administrative Access:"
            local admin_accounts=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM accounts WHERE admin = 1;
            " 2>/dev/null || echo "0")
            
            echo "â€¢ Admin accounts in database: $admin_accounts"
            
            if [[ "$admin_accounts" -gt 0 ]] && [[ "$admin_accounts" -lt 10 ]]; then
                echo "âœ… Reasonable number of admin accounts"
            elif [[ "$admin_accounts" -gt 10 ]]; then
                echo "âš ï¸  High number of admin accounts - review required"
            fi
            ;;
            
        3)
            echo ""
            echo "ðŸ”— Data Integrity Verification:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Running comprehensive data integrity checks..."
            
            # Database integrity check
            echo ""
            echo "1. SQLite Integrity Check:"
            local integrity_result=$(sqlite3 "$DATABASE_PATH" "PRAGMA integrity_check;" 2>/dev/null)
            
            if [[ "$integrity_result" == "ok" ]]; then
                echo "âœ… Database structure integrity: PASSED"
            else
                echo "âŒ Database structure integrity: FAILED"
                echo "   Details: $integrity_result"
            fi
            
            # Foreign key check
            echo ""
            echo "2. Foreign Key Integrity:"
            sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys = ON;" 2>/dev/null
            local fk_violations=$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_key_check;" 2>/dev/null)
            
            if [[ -z "$fk_violations" ]]; then
                echo "âœ… Foreign key integrity: PASSED"
            else
                echo "âŒ Foreign key violations detected:"
                echo "$fk_violations"
            fi
            
            # Data consistency checks
            echo ""
            echo "3. Data Consistency Verification:"
            
            # Check for orphaned records
            local orphaned_storage=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM storage_size_history 
                WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
            " 2>/dev/null || echo "0")
            
            local orphaned_stages=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM stage_history 
                WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);
            " 2>/dev/null || echo "0")
            
            echo "â€¢ Orphaned storage records: $orphaned_storage"
            echo "â€¢ Orphaned stage records: $orphaned_stages"
            
            if [[ "$orphaned_storage" -eq 0 ]] && [[ "$orphaned_stages" -eq 0 ]]; then
                echo "âœ… No orphaned records found"
            else
                echo "âš ï¸  Orphaned records detected - cleanup recommended"
            fi
            
            # Check for data anomalies
            echo ""
            echo "4. Data Anomaly Detection:"
            
            # Check for accounts with invalid email formats
            local invalid_emails=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM accounts 
                WHERE email IS NOT NULL AND email NOT LIKE '%@%.%';
            " 2>/dev/null || echo "0")
            
            # Check for negative storage values
            local negative_storage=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM storage_size_history 
                WHERE total_size_gb < 0 OR gmail_size_gb < 0 OR drive_size_gb < 0;
            " 2>/dev/null || echo "0")
            
            # Check for future dates
            local future_dates=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) FROM accounts WHERE created_at > datetime('now');
            " 2>/dev/null || echo "0")
            
            echo "â€¢ Invalid email formats: $invalid_emails"
            echo "â€¢ Negative storage values: $negative_storage"
            echo "â€¢ Future creation dates: $future_dates"
            
            local total_anomalies=$((invalid_emails + negative_storage + future_dates))
            
            if [[ $total_anomalies -eq 0 ]]; then
                echo "âœ… No data anomalies detected"
            else
                echo "âš ï¸  $total_anomalies data anomalies detected - investigation recommended"
            fi
            
            # Check for duplicate records
            echo ""
            echo "5. Duplicate Record Detection:"
            
            local duplicate_accounts=$(sqlite3 "$DATABASE_PATH" "
                SELECT COUNT(*) - COUNT(DISTINCT email) FROM accounts WHERE email IS NOT NULL;
            " 2>/dev/null || echo "0")
            
            echo "â€¢ Duplicate account records: $duplicate_accounts"
            
            if [[ "$duplicate_accounts" -eq 0 ]]; then
                echo "âœ… No duplicate accounts found"
            else
                echo "âš ï¸  Duplicate accounts detected - data quality issue"
            fi
            ;;
            
        4)
            echo ""
            echo "âš™ï¸ Configuration Security Review:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Security Configuration Analysis:"
            
            # SQLite security settings
            echo ""
            echo "1. SQLite Security Settings:"
            
            local secure_delete=$(sqlite3 "$DATABASE_PATH" "PRAGMA secure_delete;" 2>/dev/null || echo "Unknown")
            local foreign_keys=$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys;" 2>/dev/null || echo "Unknown")
            local case_sensitive_like=$(sqlite3 "$DATABASE_PATH" "PRAGMA case_sensitive_like;" 2>/dev/null || echo "Unknown")
            
            echo "â€¢ Secure delete: $secure_delete"
            echo "â€¢ Foreign keys: $foreign_keys"
            echo "â€¢ Case sensitive LIKE: $case_sensitive_like"
            
            # Security recommendations
            if [[ "$secure_delete" == "1" ]]; then
                echo "âœ… Secure delete enabled - deleted data is overwritten"
            else
                echo "âš ï¸  Secure delete disabled - deleted data may be recoverable"
            fi
            
            if [[ "$foreign_keys" == "1" ]]; then
                echo "âœ… Foreign key constraints enabled"
            else
                echo "âš ï¸  Foreign key constraints disabled"
            fi
            
            # Application configuration security
            echo ""
            echo "2. Application Configuration Security:"
            
            # Check for sensitive configuration
            local config_count=$(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM config;" 2>/dev/null || echo "0")
            echo "â€¢ Configuration entries: $config_count"
            
            if [[ "$config_count" -gt 0 ]]; then
                echo "â€¢ Configuration keys:"
                sqlite3 "$DATABASE_PATH" "SELECT key FROM config ORDER BY key;" 2>/dev/null | while read -r key; do
                    # Check for potentially sensitive keys
                    if echo "$key" | grep -qi -E "(password|secret|key|token)"; then
                        echo "  âš ï¸  $key (potentially sensitive)"
                    else
                        echo "  â€¢ $key"
                    fi
                done
            fi
            
            # Check environment security
            echo ""
            echo "3. Environment Security:"
            
            # Check if .env files exist and their permissions
            if [[ -f ".env" ]]; then
                local env_perms=$(ls -l .env | awk '{print $1}' 2>/dev/null || echo "Unknown")
                echo "â€¢ .env file permissions: $env_perms"
                
                if [[ "$env_perms" =~ ^-rw------- ]]; then
                    echo "âœ… .env file properly secured (600)"
                else
                    echo "âŒ .env file permissions too permissive - SECURITY RISK!"
                fi
            else
                echo "â€¢ No .env file found"
            fi
            
            # Check for hardcoded secrets in config
            echo ""
            echo "4. Hardcoded Secrets Check:"
            
            local potential_secrets=$(sqlite3 "$DATABASE_PATH" "
                SELECT key, value FROM config 
                WHERE value LIKE '%password%' OR value LIKE '%secret%' OR value LIKE '%token%';
            " 2>/dev/null | wc -l || echo "0")
            
            if [[ "$potential_secrets" -gt 0 ]]; then
                echo "âš ï¸  $potential_secrets potentially sensitive configuration values found"
                echo "   Recommendation: Use environment variables for secrets"
            else
                echo "âœ… No obvious hardcoded secrets in configuration"
            fi
            ;;
            
        5)
            echo ""
            echo "ðŸ’¾ Backup Security Assessment:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "Backup Security Analysis:"
            
            # Backup directory security
            if [[ -d "backups" ]]; then
                echo ""
                echo "1. Backup Directory Security:"
                
                local backup_perms=$(ls -ld backups | awk '{print $1}' 2>/dev/null || echo "Unknown")
                local backup_owner=$(ls -ld backups | awk '{print $3}' 2>/dev/null || echo "Unknown")
                
                echo "â€¢ Directory: backups/"
                echo "â€¢ Permissions: $backup_perms"
                echo "â€¢ Owner: $backup_owner"
                
                if [[ "$backup_perms" =~ ^drwx------ ]]; then
                    echo "âœ… Backup directory properly secured"
                else
                    echo "âš ï¸  Backup directory permissions may be too permissive"
                fi
                
                # Check backup file permissions
                echo ""
                echo "2. Backup File Security:"
                
                local backup_files=($(ls backups/*.db 2>/dev/null | head -5))
                
                if [[ ${#backup_files[@]} -gt 0 ]]; then
                    echo "â€¢ Sample backup file permissions:"
                    for backup_file in "${backup_files[@]}"; do
                        local file_perms=$(ls -l "$backup_file" | awk '{print $1}' 2>/dev/null || echo "Unknown")
                        echo "  $(basename "$backup_file"): $file_perms"
                        
                        if [[ ! "$file_perms" =~ ^-rw------- ]]; then
                            echo "    âš ï¸  Consider restricting permissions to 600"
                        fi
                    done
                else
                    echo "â€¢ No backup files found"
                fi
                
                # Check backup age and rotation
                echo ""
                echo "3. Backup Rotation Security:"
                
                local total_backups=$(ls backups/*.db 2>/dev/null | wc -l || echo "0")
                local old_backups=$(find backups -name "*.db" -mtime +30 2>/dev/null | wc -l || echo "0")
                
                echo "â€¢ Total backups: $total_backups"
                echo "â€¢ Backups older than 30 days: $old_backups"
                
                if [[ "$old_backups" -gt 10 ]]; then
                    echo "âš ï¸  Many old backups - consider implementing rotation policy"
                else
                    echo "âœ… Backup retention appears reasonable"
                fi
                
            else
                echo "âŒ No backup directory found - MAJOR SECURITY RISK!"
                echo "   Recommendation: Implement backup strategy immediately"
            fi
            
            # Backup encryption check
            echo ""
            echo "4. Backup Encryption Assessment:"
            
            if [[ -d "backups" ]] && [[ $(ls backups/*.db 2>/dev/null | wc -l) -gt 0 ]]; then
                # Check if backups are encrypted (basic check)
                local sample_backup=$(ls backups/*.db 2>/dev/null | head -1)
                if [[ -n "$sample_backup" ]]; then
                    local file_type=$(file "$sample_backup" 2>/dev/null)
                    
                    if [[ "$file_type" == *"SQLite"* ]]; then
                        echo "âš ï¸  Backups appear to be unencrypted SQLite files"
                        echo "   Recommendation: Consider encrypting backups for enhanced security"
                    else
                        echo "â€¢ Backup file type: $file_type"
                        echo "âœ… Backups may be encrypted or compressed"
                    fi
                fi
            fi
            ;;
            
        6)
            echo ""
            echo "ðŸ’‰ SQL Injection Vulnerability Scan:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            echo "SQL Injection Security Analysis:"
            
            # Note: This is a basic assessment since we're dealing with SQLite and shell scripts
            echo ""
            echo "1. Application Architecture Assessment:"
            echo "â€¢ Database: SQLite (lower injection risk than networked databases)"
            echo "â€¢ Interface: Shell scripts with SQLite CLI"
            echo "â€¢ User input: Command-line based"
            
            echo ""
            echo "2. Input Validation Check:"
            
            # Check if there are any direct string concatenations in SQL
            echo "â€¢ Scanning for potential SQL injection patterns..."
            
            # Look for common SQL injection patterns in shell scripts
            local sql_concat_patterns=$(grep -n "SELECT.*\$" gwombat.sh 2>/dev/null | wc -l || echo "0")
            local where_concat_patterns=$(grep -n "WHERE.*\$" gwombat.sh 2>/dev/null | wc -l || echo "0")
            
            echo "â€¢ Direct variable concatenations in SELECT: $sql_concat_patterns"
            echo "â€¢ Direct variable concatenations in WHERE: $where_concat_patterns"
            
            if [[ "$sql_concat_patterns" -gt 0 ]] || [[ "$where_concat_patterns" -gt 0 ]]; then
                echo "âš ï¸  Potential SQL injection vulnerabilities detected"
                echo "   Recommendation: Use parameterized queries or proper escaping"
                
                # Show some examples (first few)
                echo ""
                echo "   Examples found:"
                grep -n "SELECT.*\$" gwombat.sh 2>/dev/null | head -3 | while read -r line; do
                    echo "   $line"
                done
            else
                echo "âœ… No obvious SQL injection patterns found"
            fi
            
            echo ""
            echo "3. User Input Sources:"
            
            # Check for read -p patterns (user input)
            local user_inputs=$(grep -n "read -p" gwombat.sh 2>/dev/null | wc -l || echo "0")
            echo "â€¢ User input prompts found: $user_inputs"
            
            if [[ "$user_inputs" -gt 0 ]]; then
                echo "   Recommendation: Validate and sanitize all user inputs"
                
                # Check if any user inputs go directly into SQL
                echo ""
                echo "   Sample user input patterns:"
                grep -n "read -p" gwombat.sh 2>/dev/null | head -3 | while read -r line; do
                    echo "   $line"
                done
            fi
            
            echo ""
            echo "4. SQL Query Security Assessment:"
            
            # Count different types of SQL operations
            local select_count=$(grep -c "SELECT" gwombat.sh 2>/dev/null || echo "0")
            local insert_count=$(grep -c "INSERT" gwombat.sh 2>/dev/null || echo "0")
            local update_count=$(grep -c "UPDATE" gwombat.sh 2>/dev/null || echo "0")
            local delete_count=$(grep -c "DELETE" gwombat.sh 2>/dev/null || echo "0")
            
            echo "â€¢ SELECT operations: $select_count"
            echo "â€¢ INSERT operations: $insert_count"
            echo "â€¢ UPDATE operations: $update_count"
            echo "â€¢ DELETE operations: $delete_count"
            
            # Check for potentially dangerous operations
            local drop_count=$(grep -c "DROP" gwombat.sh 2>/dev/null || echo "0")
            local truncate_count=$(grep -c "TRUNCATE" gwombat.sh 2>/dev/null || echo "0")
            
            if [[ "$drop_count" -gt 0 ]] || [[ "$truncate_count" -gt 0 ]]; then
                echo "âš ï¸  Potentially dangerous operations found:"
                echo "   â€¢ DROP statements: $drop_count"
                echo "   â€¢ TRUNCATE statements: $truncate_count"
                echo "   Recommendation: Ensure these are protected and validated"
            else
                echo "âœ… No dangerous SQL operations found"
            fi
            
            echo ""
            echo "5. Security Recommendations:"
            echo "â€¢ Use SQLite prepared statements when possible"
            echo "â€¢ Validate and sanitize all user inputs"
            echo "â€¢ Implement input length limits"
            echo "â€¢ Use allowlists for acceptable input characters"
            echo "â€¢ Log and monitor all database operations"
            echo "â€¢ Restrict database file permissions"
            ;;
            
        7)
            echo ""
            echo "ðŸ“„ Generating Security Audit Report..."
            
            local security_report="reports/security_audit_$(date +%Y%m%d_%H%M%S).txt"
            mkdir -p reports
            
            cat > "$security_report" << EOF
GWOMBAT Database Security Audit Report
Generated: $(date)
======================================

EXECUTIVE SUMMARY:
This security audit assesses the database security posture of the GWOMBAT system,
including file system security, access controls, data integrity, and potential vulnerabilities.

DATABASE FILE SECURITY:
â€¢ Database Path: $DATABASE_PATH
â€¢ File Permissions: $(ls -l "$DATABASE_PATH" | awk '{print $1}' 2>/dev/null || echo "Unknown")
â€¢ Owner: $(ls -l "$DATABASE_PATH" | awk '{print $3}' 2>/dev/null || echo "Unknown")
â€¢ Group: $(ls -l "$DATABASE_PATH" | awk '{print $4}' 2>/dev/null || echo "Unknown")

ACCESS CONTROL ASSESSMENT:
â€¢ Authentication Method: File system based (SQLite)
â€¢ Foreign Key Enforcement: $(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys;" 2>/dev/null || echo "Unknown")
â€¢ Admin Accounts: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM accounts WHERE admin = 1;" 2>/dev/null || echo "0")

DATA INTEGRITY STATUS:
â€¢ Database Integrity: $(sqlite3 "$DATABASE_PATH" "PRAGMA integrity_check;" 2>/dev/null)
â€¢ Foreign Key Violations: $(if [[ -z "$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys = ON; PRAGMA foreign_key_check;" 2>/dev/null)" ]]; then echo "None"; else echo "Detected"; fi)
â€¢ Orphaned Records: $(echo $(($(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM storage_size_history WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);" 2>/dev/null || echo "0") + $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM stage_history WHERE email NOT IN (SELECT email FROM accounts WHERE email IS NOT NULL);" 2>/dev/null || echo "0"))))

CONFIGURATION SECURITY:
â€¢ Secure Delete: $(sqlite3 "$DATABASE_PATH" "PRAGMA secure_delete;" 2>/dev/null || echo "Unknown")
â€¢ Configuration Entries: $(sqlite3 "$DATABASE_PATH" "SELECT COUNT(*) FROM config;" 2>/dev/null || echo "0")
â€¢ .env File Security: $(if [[ -f ".env" ]]; then ls -l .env | awk '{print $1}'; else echo "No .env file"; fi)

BACKUP SECURITY:
â€¢ Backup Directory: $(if [[ -d "backups" ]]; then echo "Present"; else echo "Missing"; fi)
â€¢ Backup Files: $(ls backups/*.db 2>/dev/null | wc -l || echo "0")
â€¢ Backup Permissions: $(if [[ -d "backups" ]]; then ls -ld backups | awk '{print $1}'; else echo "N/A"; fi)

POTENTIAL VULNERABILITIES:
â€¢ SQL Injection Risk: $(if [[ $(grep -c "SELECT.*\$" gwombat.sh 2>/dev/null || echo "0") -gt 0 ]]; then echo "Medium - Variable concatenation detected"; else echo "Low - No obvious patterns"; fi)
â€¢ User Input Validation: $(grep -c "read -p" gwombat.sh 2>/dev/null || echo "0") user input points require validation

SECURITY RECOMMENDATIONS:

HIGH PRIORITY:
$(if [[ ! "$(ls -l "$DATABASE_PATH" | awk '{print $1}')" =~ ^-rw------- ]]; then echo "â€¢ Restrict database file permissions to 600 (owner read/write only)"; fi)
$(if [[ "$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys;" 2>/dev/null)" != "1" ]]; then echo "â€¢ Enable foreign key constraints for data integrity"; fi)
$(if [[ ! -d "backups" ]]; then echo "â€¢ Implement database backup strategy immediately"; fi)

MEDIUM PRIORITY:
â€¢ Review and validate all user input handling
â€¢ Implement input sanitization for SQL operations
â€¢ Enable secure delete pragma for sensitive data removal
â€¢ Set up regular security monitoring and alerting

LOW PRIORITY:
â€¢ Consider database encryption for sensitive environments
â€¢ Implement audit logging for all database operations
â€¢ Review and minimize admin account privileges
â€¢ Set up automated security scanning

COMPLIANCE NOTES:
â€¢ File system access controls in place
â€¢ Data integrity mechanisms available
â€¢ Backup capabilities present (if backup directory exists)
â€¢ No network exposure (SQLite file-based)

NEXT STEPS:
1. Address high priority recommendations immediately
2. Implement comprehensive backup strategy if missing
3. Review and secure file system permissions
4. Establish regular security audit schedule
5. Consider implementing additional access controls for sensitive environments

Report generated by GWOMBAT Database Management System
Security Audit Module
EOF
            
            echo "âœ… Security audit report generated: $security_report"
            
            # Generate summary score
            local security_score=100
            
            # Deduct points for security issues
            if [[ ! "$(ls -l "$DATABASE_PATH" | awk '{print $1}')" =~ ^-rw------- ]]; then
                security_score=$((security_score - 25))
            fi
            
            if [[ "$(sqlite3 "$DATABASE_PATH" "PRAGMA foreign_keys;" 2>/dev/null)" != "1" ]]; then
                security_score=$((security_score - 15))
            fi
            
            if [[ ! -d "backups" ]]; then
                security_score=$((security_score - 30))
            fi
            
            if [[ $(grep -c "SELECT.*\$" gwombat.sh 2>/dev/null || echo "0") -gt 0 ]]; then
                security_score=$((security_score - 20))
            fi
            
            echo ""
            echo "ðŸ›¡ï¸ Security Score: ${security_score}/100"
            
            if [[ $security_score -ge 80 ]]; then
                echo "âœ… Good security posture"
            elif [[ $security_score -ge 60 ]]; then
                echo "âš ï¸  Moderate security - improvements needed"
            else
                echo "âŒ Security concerns detected - immediate action required"
            fi
            ;;
            
        *)
            echo "âŒ Invalid option"
            ;;
    esac
    
    echo ""
    read -p "Press Enter to continue..."
}

# Check if required directories exist
if [[ ! -d "$SCRIPTPATH" ]]; then
    echo -e "${RED}Error: Script path $SCRIPTPATH does not exist.${NC}"
    exit 1
fi

if [[ ! -d "$SHARED_UTILITIES_PATH" ]]; then
    echo -e "${RED}Error: Shared utilities path $SHARED_UTILITIES_PATH does not exist.${NC}"
    exit 1
fi

# Run the main function
main
# Backup Operations Function Dispatcher
backup_operations_function_dispatcher() {
    local function_name="$1"
    
    case "$function_name" in
        # Remote Storage Configuration (1)
        "configure_remote_storage")
            echo -e "${CYAN}Remote Storage Configuration: $function_name${NC}"
            echo "This feature will provide comprehensive cloud storage backend configuration."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ rclone configuration for multiple cloud providers"
            echo "â€¢ Encryption settings and security configuration"
            echo "â€¢ Storage quota management and monitoring"
            echo "â€¢ Connection testing and validation"
            read -p "Press Enter to continue..."
            ;;
        
        # Backup Policy Management (2-4)
        "create_backup_policy"|"manage_backup_policies"|"schedule_backups")
            echo -e "${CYAN}Backup Policy Management: $function_name${NC}"
            echo "This feature will provide comprehensive backup policy configuration."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Define backup frequencies and retention policies"
            echo "â€¢ Configure what data to include/exclude"
            echo "â€¢ Set up automated scheduling with cron integration"
            echo "â€¢ Policy templates for common scenarios"
            read -p "Press Enter to continue..."
            ;;
        
        # Backup Analysis & Execution (5-6)
        "analyze_backup_needs"|"execute_backup_now")
            echo -e "${CYAN}Backup Analysis & Execution: $function_name${NC}"
            echo "This feature will provide backup analysis and immediate execution."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Analyze data growth and storage requirements"
            echo "â€¢ Immediate backup execution with progress tracking"
            echo "â€¢ Resource usage monitoring during backups"
            echo "â€¢ Incremental and full backup options"
            read -p "Press Enter to continue..."
            ;;
        
        # Restore Operations (7-8)
        "restore_from_backup"|"selective_restore")
            echo -e "${CYAN}Restore Operations: $function_name${NC}"
            echo "This feature will provide comprehensive data restoration capabilities."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Full system restore from backup archives"
            echo "â€¢ Selective file and account restoration"
            echo "â€¢ Point-in-time recovery options"
            echo "â€¢ Restore verification and validation"
            read -p "Press Enter to continue..."
            ;;
        
        # Verification & Integrity (9-10)
        "verify_backup_integrity"|"test_restore_process")
            echo -e "${CYAN}Verification & Integrity: $function_name${NC}"
            echo "This feature will provide backup verification and testing capabilities."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Backup completeness and integrity checking"
            echo "â€¢ Automated restore testing procedures"
            echo "â€¢ Data consistency validation"
            echo "â€¢ Backup health monitoring and alerting"
            read -p "Press Enter to continue..."
            ;;
        
        # Monitoring & Reports (11-15)
        "backup_status_dashboard"|"backup_history_report"|"storage_usage_analysis"|"backup_alerts_config"|"export_backup_inventory")
            echo -e "${CYAN}Monitoring & Reports: $function_name${NC}"
            echo "This feature will provide comprehensive backup monitoring and reporting."
            echo ""
            echo "Capabilities will include:"
            echo "â€¢ Real-time backup system status dashboard"
            echo "â€¢ Historical backup operation reporting"
            echo "â€¢ Storage consumption analysis and forecasting"
            echo "â€¢ Alert configuration for backup events"
            echo "â€¢ Complete backup inventory documentation"
            read -p "Press Enter to continue..."
            ;;
        
        *)
            echo -e "${RED}Unknown backup operations function: $function_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Backup & Recovery Menu - SQLite-driven implementation
# Gmail backup, Drive backup, and system recovery operations interface
# Uses database-driven menu items from backup_operations_main section
backup_operations_main_menu() {
    render_menu "system_administration"
}

# Improved input handling function
# Usage: get_user_input "prompt" "valid_options" "max_attempts"
get_user_input() {
    local prompt="$1"
    local valid_options="$2"
    local max_attempts="${3:-3}"
    local attempt=0
    local user_input
    
    while true; do
        read -p "$prompt" user_input
        
        # Check if input is valid
        if [[ "$valid_options" == *"$user_input"* ]] || [[ "$user_input" =~ ^[0-9]+$ ]]; then
            echo "$user_input"
            return 0
        fi
        
        ((attempt++))
        
        if [[ $attempt -lt $max_attempts ]]; then
            # Show error and reprompt immediately
            echo -ne "\r${RED}Invalid option. Please try again: ${NC}"
        else
            # After max attempts, require Enter to continue
            echo ""
            echo -e "${RED}Invalid option. Please select from valid options.${NC}"
            read -p "Press Enter to continue..."
            return 1
        fi
    done
}

# Enhanced menu choice handler with improved error handling
handle_menu_choice() {
    local prompt="$1"
    local max_numeric="$2"
    local valid_letters="$3"
    local max_attempts="${4:-3}"
    
    local valid_options="$valid_letters"
    for ((i=1; i<=max_numeric; i++)); do
        valid_options="$valid_options $i"
    done
    
    get_user_input "$prompt" "$valid_options" "$max_attempts"
}

# Main Menu Function Dispatcher - Routes section names to their respective menu functions
main_menu_function_dispatcher() {
    local section_name="$1"
    
    case "$section_name" in
        "user_group_management")
            user_group_management_menu
            ;;
        "file_drive_operations")
            file_drive_operations_menu
            ;;
        "analysis_discovery")
            analysis_discovery_menu
            ;;
        "account_list_management")
            list_management_menu
            ;;
        "dashboard_statistics")
            dashboard_menu
            ;;
        "reports_monitoring")
            reports_and_cleanup_menu
            ;;
        "system_administration")
            system_administration_menu
            ;;
        "scuba_compliance")
            scuba_compliance_menu
            ;;
        "configuration_management")
            if [[ -x "$SHARED_UTILITIES_PATH/config_manager.sh" ]]; then
                source "$SHARED_UTILITIES_PATH/config_manager.sh"
                show_config_menu
            else
                configuration_menu
            fi
            ;;
        *)
            echo -e "${RED}Unknown section: $section_name${NC}"
            read -p "Press Enter to continue..."
            ;;
    esac
}

# Check if hierarchical menu system should be used
USE_HIERARCHICAL_MENUS="${USE_HIERARCHICAL_MENUS:-true}"

# Call main function to start the application
load_configuration

if [[ "$USE_HIERARCHICAL_MENUS" == "true" ]]; then
    # Check for enhanced version first
    if [[ -f "shared-utilities/enhanced_hierarchical_menu.sh" ]]; then
        echo -e "${BLUE}Starting GWOMBAT with Enhanced Hierarchical Menu System...${NC}"
        source shared-utilities/enhanced_hierarchical_menu.sh
        init_hierarchical_menu
    elif [[ -f "shared-utilities/hierarchical_menu_system.sh" ]]; then
        echo -e "${BLUE}Starting GWOMBAT with Hierarchical Menu System...${NC}"
        source shared-utilities/hierarchical_menu_system.sh
        init_hierarchical_menu
    else
        echo -e "${YELLOW}Hierarchical menu system not found. Using original system.${NC}"
        main
    fi
else
    # Use original system
    main
fi

